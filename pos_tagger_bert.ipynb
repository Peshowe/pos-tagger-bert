{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "2b1b350c-5a3c-434c-bcb2-0672625d154d"
    }
   },
   "source": [
    "# NATURAL LANGUAGE PROCESSING WITH TRANSFORMERS\n",
    "# Α Pos Tagger trained on UD treebank with fine-tuning a BERT model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras BERT implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbpresent": {
     "id": "f61bc4e3-579a-44c9-a7c6-4dc1f9fbf7cb"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyconll\n",
      "  Downloading https://files.pythonhosted.org/packages/2c/6e/c325d0db05ac1b8d45645de903e4ba691d419e861c915c3d4ebfcaf8ac25/pyconll-2.2.1-py3-none-any.whl\n",
      "Requirement already satisfied: requests>=2.21 in c:\\users\\pnedelev\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from pyconll) (2.21.0)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\pnedelev\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from requests>=2.21->pyconll) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pnedelev\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from requests>=2.21->pyconll) (2019.9.11)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in c:\\users\\pnedelev\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from requests>=2.21->pyconll) (1.24.1)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\pnedelev\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from requests>=2.21->pyconll) (3.0.4)\n",
      "Installing collected packages: pyconll\n",
      "Successfully installed pyconll-2.2.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 19.3.1; however, version 20.0.2 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydot in c:\\users\\pnedelev\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (1.4.1)\n",
      "Requirement already satisfied: pyparsing>=2.1.4 in c:\\users\\pnedelev\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from pydot) (2.3.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 19.3.1; however, version 20.0.2 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n",
      "ERROR: Could not find a version that satisfies the requirement graphiz (from versions: none)\n",
      "ERROR: No matching distribution found for graphiz\n",
      "WARNING: You are using pip version 19.3.1; however, version 20.0.2 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bert-tensorflow in c:\\users\\pnedelev\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (1.0.1)\n",
      "Requirement already satisfied: six in c:\\users\\pnedelev\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from bert-tensorflow) (1.12.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 19.3.1; however, version 20.0.2 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "# First install some extra packages\n",
    "! pip install pyconll\n",
    "! pip install pydot\n",
    "! pip install graphiz\n",
    "! pip install bert-tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbpresent": {
     "id": "f4a0e2d1-af80-44ba-a384-fab4d5d6c703"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "c:\\users\\pnedelev\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.metrics.classification module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(2)\n",
    "import pyconll, keras, pickle, os, random, nltk, datetime, warnings, gc, urllib.request, zipfile, collections\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import recall_score, precision_score, classification_report, accuracy_score, confusion_matrix, f1_score\n",
    "from sklearn.metrics.classification import UndefinedMetricWarning\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Embedding, Dense, Input, concatenate, Layer, Lambda, Dropout, Activation\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, Callback, TensorBoard\n",
    "from keras import backend as K\n",
    "from keras.models import load_model\n",
    "from keras.utils import plot_model\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from bert.tokenization import FullTokenizer\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "from IPython.display import Image \n",
    "warnings.filterwarnings(action='ignore', category=UndefinedMetricWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "0f518892-45cc-4978-97e8-02c1fce60407"
    }
   },
   "source": [
    "# Various plot functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbpresent": {
     "id": "ffc21443-9fd6-4cd3-86e1-a1a518cb6aca"
    }
   },
   "outputs": [],
   "source": [
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
    "                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes, scoring='accuracy')\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "nbpresent": {
     "id": "aa581c97-05ee-4e34-9f9d-884ce9ba4b66"
    }
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(f1,\n",
    "                          cm,\n",
    "                          target_names,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=None,\n",
    "                          normalize=True,\n",
    "                          i=1):\n",
    "    \"\"\"\n",
    "    given a sklearn confusion matrix (cm), make a nice plot\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    cm:           confusion matrix from sklearn.metrics.confusion_matrix\n",
    "\n",
    "    target_names: given classification classes such as [0, 1, 2]\n",
    "                  the class names, for example: ['high', 'medium', 'low']\n",
    "\n",
    "    title:        the text to display at the top of the matrix\n",
    "\n",
    "    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n",
    "                  see http://matplotlib.org/examples/color/colormaps_reference.html\n",
    "                  plt.get_cmap('jet') or plt.cm.Blues\n",
    "\n",
    "    normalize:    If False, plot the raw numbers\n",
    "                  If True, plot the proportions\n",
    "\n",
    "    Usage\n",
    "    -----\n",
    "    plot_confusion_matrix(cm           = cm,                  # confusion matrix created by\n",
    "                                                              # sklearn.metrics.confusion_matrix\n",
    "                          normalize    = True,                # show proportions\n",
    "                          target_names = y_labels_vals,       # list of names of the classes\n",
    "                          title        = best_estimator_name) # title of graph\n",
    "\n",
    "    Citiation\n",
    "    ---------\n",
    "    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import itertools\n",
    "\n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}; f1-score={:0.4f}'.format(accuracy, misclass, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "nbpresent": {
     "id": "7626f765-a0ca-4230-b9b4-5aea4c185273"
    }
   },
   "outputs": [],
   "source": [
    "def plot_acc():\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "nbpresent": {
     "id": "4afdbba0-cb73-4bdc-97f7-3cbdd2d19079"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_loss():\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "1585e7f9-881f-46fe-a1e8-ca970e292167"
    }
   },
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "nbpresent": {
     "id": "75d32121-1074-4cd6-8355-599afeff56db"
    }
   },
   "outputs": [],
   "source": [
    "UD_ENGLISH_TRAIN = 'en_partut-ud-train.conllu'\n",
    "UD_ENGLISH_DEV = 'en_partut-ud-dev.conllu'\n",
    "UD_ENGLISH_TEST = 'en_partut-ud-test.conllu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "nbpresent": {
     "id": "2eb0b136-1052-4d48-8a66-48b5ebc96ff8"
    }
   },
   "outputs": [],
   "source": [
    "def download_files():\n",
    "    print('Downloading English treebank...')\n",
    "    urllib.request.urlretrieve('http://archive.aueb.gr:8085/files/en_partut-ud-dev.conllu', 'en_partut-ud-dev.conllu')\n",
    "#     urllib.request.urlretrieve('http://archive.aueb.gr:8085/files/en_partut-ud-test.conllu', 'en_partut-ud-test.conllu')\n",
    "#     urllib.request.urlretrieve('http://archive.aueb.gr:8085/files/en_partut-ud-train.conllu', 'en_partut-ud-train.conllu')\n",
    "    print('Treebank downloaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "nbpresent": {
     "id": "4e617ca6-4cce-42a0-9396-28b9d704df73"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading English treebank...\n",
      "Treebank downloaded.\n"
     ]
    }
   ],
   "source": [
    "download_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "43c9097b-37be-4f3d-b3d3-4a54a94c1174"
    }
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "nbpresent": {
     "id": "016836bb-abb2-4235-ab21-df90a6e1e44e"
    }
   },
   "outputs": [],
   "source": [
    "def read_conllu(path):\n",
    "    data = pyconll.load_from_file(path)\n",
    "    tagged_sentences=[]\n",
    "    t=0\n",
    "    for sentence in data:\n",
    "        tagged_sentence=[]\n",
    "        for token in sentence:\n",
    "            if token.upos and token.form:\n",
    "                t+=1\n",
    "                tagged_sentence.append((token.form.lower(), token.upos))\n",
    "        tagged_sentences.append(tagged_sentence)\n",
    "    return tagged_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "11bff856-f685-49ef-a4e6-b5f2a768a9ad"
    }
   },
   "source": [
    "Load train, development and test set in the appropriate tagged format, tuple (word, pos-tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "nbpresent": {
     "id": "fc49598b-75d3-45e9-9fa3-a7b604234760"
    }
   },
   "outputs": [],
   "source": [
    "# train_sentences = read_conllu(UD_ENGLISH_TRAIN)\n",
    "val_sentences = read_conllu(UD_ENGLISH_DEV)\n",
    "# test_sentences = read_conllu(UD_ENGLISH_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "import random, math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_te = pd.read_csv('../all-te.txt', sep='\\n', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbs = pd.read_csv('../verbs_no_homonyms.txt', sep='\\n', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbs = verbs[0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_file = open(\"../raw_file.txt\", \"r\", encoding='utf-8').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [word_tokenize(s) for s in sent_tokenize(raw_file)]\n",
    "sentences_lower = [[t.lower() for t in s]for s in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged = [[(w,'Y') if w.lower() in verbs else (w,'N') for w in s] for s in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_index = (0, math.ceil(len(tagged)*0.7))\n",
    "test_index = (train_index[1], train_index[1]+math.ceil(len(tagged)*0.2))\n",
    "val_index = (test_index[1], test_index[1]+math.ceil(len(tagged)*0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = tagged[train_index[0]:train_index[1]]\n",
    "test_sentences = tagged[test_index[0]:test_index[1]]\n",
    "val_sentences = tagged[val_index[0]:val_index[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "3f72d1da-c1d1-4e25-a782-e12a66f1cedf"
    }
   },
   "source": [
    "Print some statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "nbpresent": {
     "id": "57ee9c67-7ec6-4054-91bc-a9aa896ba18a"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tagged sentences in train set:  1662\n",
      "Tagged words in train set: 22025\n",
      "========================================\n",
      "Tagged sentences in dev set:  237\n",
      "Tagged words in dev set: 2927\n",
      "========================================\n",
      "Tagged sentences in test set:  475\n",
      "Tagged words in test set: 6198\n",
      "****************************************\n",
      "Total sentences in dataset: 2374\n"
     ]
    }
   ],
   "source": [
    "print(\"Tagged sentences in train set: \", len(train_sentences))\n",
    "print(\"Tagged words in train set:\", len([item for sublist in train_sentences for item in sublist]))\n",
    "print(40*'=')\n",
    "print(\"Tagged sentences in dev set: \", len(val_sentences))\n",
    "print(\"Tagged words in dev set:\", len([item for sublist in val_sentences for item in sublist]))\n",
    "print(40*'=')\n",
    "print(\"Tagged sentences in test set: \", len(test_sentences))\n",
    "print(\"Tagged words in test set:\", len([item for sublist in test_sentences for item in sublist]))\n",
    "print(40*'*')\n",
    "print(\"Total sentences in dataset:\", len(train_sentences)+len(val_sentences)+len(test_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "nbpresent": {
     "id": "eb97895a-d649-4318-b3ac-d7d6b6088a12"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Не', 'N'), ('се', 'N'), ('знае', 'Y'), ('колко', 'N'), ('ще', 'N'), ('издържат', 'Y'), ('.', 'N')]\n"
     ]
    }
   ],
   "source": [
    "print(train_sentences[213])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "nbpresent": {
     "id": "573e2ec4-2df3-49f2-913d-3d0af2d6dc36"
    }
   },
   "outputs": [],
   "source": [
    "# Some usefull functions\n",
    "def tag_sequence(sentences):\n",
    "    return [[t for w, t in sentence] for sentence in sentences]\n",
    "\n",
    "def text_sequence(sentences):\n",
    "    return [[w for w, t in sentence] for sentence in sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "533dd9e2-9e0e-43c1-8d13-b096a81a8984"
    }
   },
   "source": [
    "# Build dictionary with tag vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL TAGS:  2\n"
     ]
    }
   ],
   "source": [
    "tags = set([item for sublist in train_sentences+test_sentences+val_sentences for _, item in sublist])\n",
    "print('TOTAL TAGS: ', len(tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add '-PAD-' tag with index=0 as used in BERT code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "nbpresent": {
     "id": "98944f66-c2ae-4b29-ba01-99287044a815"
    }
   },
   "outputs": [],
   "source": [
    "tag2int = {}\n",
    "int2tag = {}\n",
    "\n",
    "for i, tag in enumerate(sorted(tags)):\n",
    "    tag2int[tag] = i+1\n",
    "    int2tag[i+1] = tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "nbpresent": {
     "id": "a935a8bd-0522-42a1-8fea-285b671df0b8"
    }
   },
   "outputs": [],
   "source": [
    "# Special character for the tags\n",
    "tag2int['-PAD-'] = 0\n",
    "int2tag[0] = '-PAD-'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "nbpresent": {
     "id": "3e55930b-4268-4b18-a7c7-de6675cdcc01"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tags: 3\n"
     ]
    }
   ],
   "source": [
    "n_tags = len(tag2int)\n",
    "print('Total tags:', n_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['N', 'Y', '-PAD-']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(tag2int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "6d6c1924-4661-4d3c-a446-ce81c05f9b71"
    }
   },
   "source": [
    "======================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "0b2c54e8-30d1-4f19-b03e-533a9e27b069"
    }
   },
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "nbpresent": {
     "id": "5a212b96-46c2-4147-9dba-9eea08757f36"
    }
   },
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 70\n",
    "EPOCHS = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "e1ee0fe5-a699-487d-9d36-b0342bc17d79"
    }
   },
   "source": [
    "# Special preprocessing for NNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "e2fff7c1-9822-4192-95b8-6721acef446f"
    }
   },
   "source": [
    "### This is a very importand step for the whole process! So be careful, to rerun this step in case you change one of the following parameters:\n",
    "- MAX_SEQUENCE_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "nbpresent": {
     "id": "12c47160-79d6-42d8-822d-e2d958e9ccb2"
    }
   },
   "outputs": [],
   "source": [
    "# train_sentences = read_conllu(UD_ENGLISH_TRAIN)\n",
    "# val_sentences = read_conllu(UD_ENGLISH_DEV)\n",
    "# test_sentences = read_conllu(UD_ENGLISH_TEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "f67342ab-29db-4565-8912-1a7efb49f808"
    }
   },
   "source": [
    "Lets examine the train set sentences distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "nbpresent": {
     "id": "844de2c1-e861-45db-8bc0-4e1f47724a95"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.hist([len(s) for s in train_sentences], bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "nbpresent": {
     "id": "d9a2a74a-dff8-4dce-a9d2-b251e8a33976"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sentence length: 165\n"
     ]
    }
   ],
   "source": [
    "print('Max sentence length:',len(max(train_sentences+val_sentences, key=len)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "0bff7138-d0ee-43b8-82ae-8d577b6e18f1"
    }
   },
   "source": [
    "To improve speed we will use a MAX_SEQUENCE_LENGTH shorter than the max lengthed sentence. To avoid truncating sequences during padding we split our sentences to MAX_SEQUENCE_LENGTH and so the number of samples increases accordingly. For example, if MAX_SEQUENCE_LENGTH=70, a sentence with length 150 splits in 3 sentences: 150=70+70+10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "nbpresent": {
     "id": "6b60d427-a8d8-4bb7-ad74-6b890098b6f7"
    }
   },
   "outputs": [],
   "source": [
    "def  split(sentences, max):\n",
    "    new=[]\n",
    "    for data in sentences:\n",
    "        new.append(([data[x:x+max] for x in range(0, len(data), max)]))\n",
    "    new = [val for sublist in new for val in sublist]\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "nbpresent": {
     "id": "067299e9-7857-445b-99dd-1df2ddd4d740"
    }
   },
   "outputs": [],
   "source": [
    "train_sentences = split(train_sentences, MAX_SEQUENCE_LENGTH)\n",
    "val_sentences = split(val_sentences, MAX_SEQUENCE_LENGTH)\n",
    "test_sentences = split(test_sentences, MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "nbpresent": {
     "id": "b180dbde-a00e-4f4b-aad7-33b0b31ad693"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(max(train_sentences+val_sentences, key=len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = train_sentences + val_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "bf4abb0c-ba12-4be4-918d-06172bc011f1"
    }
   },
   "source": [
    "# BERT implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "nbpresent": {
     "id": "b2e4df36-f6ed-4961-b2d2-540fcbf651fd"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize session\n",
    "sess = tf.Session()\n",
    "# Params for bert model and tokenization\n",
    "bert_path = \"https://tfhub.dev/google/bert_multi_cased_L-12_H-768_A-12/1\" #use multi lang version! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "nbpresent": {
     "id": "81781553-5840-4539-8e31-ebf941647e22"
    }
   },
   "outputs": [],
   "source": [
    "train_text = text_sequence(train_sentences)\n",
    "test_text = text_sequence(test_sentences)\n",
    "#val_text = text_sequence(val_sentences)\n",
    "\n",
    "train_label = tag_sequence(train_sentences)\n",
    "test_label= tag_sequence(test_sentences)\n",
    "#val_label= tag_sequence(val_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1908"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['„',\n",
       "  'Йоанис',\n",
       "  'идвал',\n",
       "  'да',\n",
       "  'подпомогне',\n",
       "  'обсадените',\n",
       "  'в',\n",
       "  'Одрин',\n",
       "  'с',\n",
       "  'твърде',\n",
       "  'голяма',\n",
       "  'войска',\n",
       "  ',',\n",
       "  'защото',\n",
       "  'той',\n",
       "  'водел',\n",
       "  'власи',\n",
       "  'и',\n",
       "  'българи',\n",
       "  'и',\n",
       "  'около',\n",
       "  '14',\n",
       "  '000',\n",
       "  'кумани',\n",
       "  ',',\n",
       "  'които',\n",
       "  'не',\n",
       "  'били',\n",
       "  'покръстени',\n",
       "  '“',\n",
       "  '—',\n",
       "  'пише',\n",
       "  'Вилардуен',\n",
       "  '.'],\n",
       " ['N',\n",
       "  'N',\n",
       "  'Y',\n",
       "  'N',\n",
       "  'Y',\n",
       "  'N',\n",
       "  'N',\n",
       "  'N',\n",
       "  'N',\n",
       "  'N',\n",
       "  'N',\n",
       "  'N',\n",
       "  'N',\n",
       "  'N',\n",
       "  'N',\n",
       "  'Y',\n",
       "  'N',\n",
       "  'N',\n",
       "  'N',\n",
       "  'N',\n",
       "  'N',\n",
       "  'N',\n",
       "  'N',\n",
       "  'N',\n",
       "  'N',\n",
       "  'N',\n",
       "  'N',\n",
       "  'Y',\n",
       "  'N',\n",
       "  'N',\n",
       "  'N',\n",
       "  'Y',\n",
       "  'N',\n",
       "  'N'])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text[133], train_label[133]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions used for BERT representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "nbpresent": {
     "id": "1165aa8f-9bd2-48d7-8ad2-3109c960334f"
    }
   },
   "outputs": [],
   "source": [
    "class PaddingInputExample(object):\n",
    "    \"\"\"Fake example so the num input examples is a multiple of the batch size.\n",
    "  When running eval/predict on the TPU, we need to pad the number of examples\n",
    "  to be a multiple of the batch size, because the TPU requires a fixed batch\n",
    "  size. The alternative is to drop the last batch, which is bad because it means\n",
    "  the entire output data won't be generated.\n",
    "  We use this class instead of `None` because treating `None` as padding\n",
    "  battches could cause silent errors.\n",
    "  \"\"\"\n",
    "\n",
    "class InputExample(object):\n",
    "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
    "\n",
    "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
    "        \"\"\"Constructs a InputExample.\n",
    "    Args:\n",
    "      guid: Unique id for the example.\n",
    "      text_a: string. The untokenized text of the first sequence. For single\n",
    "        sequence tasks, only this sequence must be specified.\n",
    "      text_b: (Optional) string. The untokenized text of the second sequence.\n",
    "        Only must be specified for sequence pair tasks.\n",
    "      label: (Optional) string. The label of the example. This should be\n",
    "        specified for train and dev examples, but not for test examples.\n",
    "    \"\"\"\n",
    "        self.guid = guid\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b\n",
    "        self.label = label\n",
    "\n",
    "def create_tokenizer_from_hub_module():\n",
    "    \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
    "    bert_module =  hub.Module(bert_path)\n",
    "    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
    "    vocab_file, do_lower_case = sess.run(\n",
    "        [\n",
    "            tokenization_info[\"vocab_file\"],\n",
    "            tokenization_info[\"do_lower_case\"],\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return FullTokenizer(vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
    "\n",
    "def convert_single_example(tokenizer, example, max_seq_length=256):\n",
    "    \"\"\"Converts a single `InputExample` into a single `InputFeatures`.\"\"\"\n",
    "\n",
    "    if isinstance(example, PaddingInputExample):\n",
    "        input_ids = [0] * max_seq_length\n",
    "        input_mask = [0] * max_seq_length\n",
    "        segment_ids = [0] * max_seq_length\n",
    "        label_ids = [0] * max_seq_length\n",
    "        return input_ids, input_mask, segment_ids, label_ids\n",
    "    \n",
    "    tokens_a = example.text_a\n",
    "    if len(tokens_a) > max_seq_length-2:\n",
    "        tokens_a = tokens_a[0 : (max_seq_length-2)]\n",
    "\n",
    "# Token map will be an int -> int mapping between the `orig_tokens` index and\n",
    "# the `bert_tokens` index.\n",
    "\n",
    "# bert_tokens == [\"[CLS]\", \"john\", \"johan\", \"##son\", \"'\", \"s\", \"house\", \"[SEP]\"]\n",
    "# orig_to_tok_map == [1, 2, 4, 6]   \n",
    "    orig_to_tok_map = []              \n",
    "    tokens = []\n",
    "    segment_ids = []\n",
    "    \n",
    "    tokens.append(\"[CLS]\")\n",
    "    segment_ids.append(0)\n",
    "    orig_to_tok_map.append(len(tokens)-1)\n",
    "    #print(len(tokens_a))\n",
    "    for token in tokens_a:       \n",
    "        tokens.extend(tokenizer.tokenize(token))\n",
    "        orig_to_tok_map.append(len(tokens)-1)\n",
    "        segment_ids.append(0)\n",
    "    tokens.append(\"[SEP]\")\n",
    "    segment_ids.append(0)\n",
    "    orig_to_tok_map.append(len(tokens)-1)\n",
    "    input_ids = tokenizer.convert_tokens_to_ids([tokens[i] for i in orig_to_tok_map])\n",
    "    #print(len(orig_to_tok_map), len(tokens), len(input_ids), len(segment_ids)) #for debugging\n",
    "\n",
    "    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "    # tokens are attended to.\n",
    "    input_mask = [1] * len(input_ids)\n",
    "    \n",
    "    label_ids = []\n",
    "    labels = example.label\n",
    "    label_ids.append(0)\n",
    "    label_ids.extend([tag2int[label] for label in labels])\n",
    "    label_ids.append(0)\n",
    "    #print(len(label_ids)) #for debugging\n",
    "    # Zero-pad up to the sequence length.\n",
    "    while len(input_ids) < max_seq_length:\n",
    "        input_ids.append(0)\n",
    "        input_mask.append(0)\n",
    "        segment_ids.append(0)\n",
    "        label_ids.append(0)\n",
    "\n",
    "    assert len(input_ids) == max_seq_length\n",
    "    assert len(input_mask) == max_seq_length\n",
    "    assert len(segment_ids) == max_seq_length\n",
    "    assert len(label_ids) == max_seq_length\n",
    "\n",
    "    return input_ids, input_mask, segment_ids, label_ids\n",
    "\n",
    "def convert_examples_to_features(tokenizer, examples, max_seq_length=256):\n",
    "    \"\"\"Convert a set of `InputExample`s to a list of `InputFeatures`.\"\"\"\n",
    "\n",
    "    input_ids, input_masks, segment_ids, labels = [], [], [], []\n",
    "    for example in tqdm_notebook(examples, desc=\"Converting examples to features\"):\n",
    "        input_id, input_mask, segment_id, label = convert_single_example(\n",
    "            tokenizer, example, max_seq_length\n",
    "        )\n",
    "        input_ids.append(input_id)\n",
    "        input_masks.append(input_mask)\n",
    "        segment_ids.append(segment_id)\n",
    "        labels.append(label)\n",
    "    return (\n",
    "        np.array(input_ids),\n",
    "        np.array(input_masks),\n",
    "        np.array(segment_ids),\n",
    "        np.array(labels),\n",
    "    )\n",
    "\n",
    "def convert_text_to_examples(texts, labels):\n",
    "    \"\"\"Create InputExamples\"\"\"\n",
    "    InputExamples = []\n",
    "    for text, label in zip(texts, labels):\n",
    "        InputExamples.append(\n",
    "            InputExample(guid=None, text_a=text, text_b=None, label=label)\n",
    "        )\n",
    "    return InputExamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "nbpresent": {
     "id": "c5d22fdb-2f46-44b2-9962-cea0cc6927a8"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0411 18:19:42.880198 126028 module_wrapper.py:139] From c:\\users\\pnedelev\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages\\bert\\tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Instantiate tokenizer\n",
    "tokenizer = create_tokenizer_from_hub_module()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alignment code (token -> tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following suggestions from WordPiece tokenization part in https://github.com/google-research/bert, for POS tagging task, input sentences are converted to the following format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# orig_tokens = [\"John\", \"Johanson\", \"'s\",  \"house\"]\n",
    "# labels      = [\"NNP\",  \"NNP\",      \"POS\", \"NN\"]\n",
    "# bert_tokens = ['[CLS]\", \"john\", \"johan\", \"##son\", \"'\",   \"s\",  \"house\", \"[SEP]\"]\n",
    "# bert_labels = ['[CLS]\", \"NNP\",  \"NNP\",   \"##\",    \"POS\", \"##\", \"NN\",    \"[SEP]\"] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, due to **Wordpiece** tokenizer we must take special care for the correct alignment of token to tag. This is done in function *convert_single_example(tokenizer, example, max_seq_length=256)* with the following piece of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_labels(labels):\n",
    "    train_label_bert = []\n",
    "    train_label_bert.append('-PAD-')\n",
    "    for i in labels:\n",
    "        train_label_bert.append(i)\n",
    "    train_label_bert.append('-PAD-')\n",
    "    print('BERT labels:', train_label_bert)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_a = train_text[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_to_tok_map = []              \n",
    "tokens = []\n",
    "segment_ids = []\n",
    "tokens.append(\"[CLS]\")\n",
    "segment_ids.append(0)\n",
    "orig_to_tok_map.append(len(tokens)-1)\n",
    "for token in tokens_a:\n",
    "    #orig_to_tok_map.append(len(tokens)) # keep first piece of tokenized term\n",
    "    tokens.extend(tokenizer.tokenize(token))\n",
    "    orig_to_tok_map.append(len(tokens)-1) # # keep last piece of tokenized term -->> gives better results!\n",
    "    segment_ids.append(0)\n",
    "tokens.append(\"[SEP]\")\n",
    "segment_ids.append(0)\n",
    "orig_to_tok_map.append(len(tokens)-1)\n",
    "input_ids = tokenizer.convert_tokens_to_ids([tokens[i] for i in orig_to_tok_map])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "nbpresent": {
     "id": "b31bd2f4-6a3b-4836-888a-5362ebdbc492"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tokens: ['Златото', 'е', 'подлудило', 'всички', '.']\n"
     ]
    }
   ],
   "source": [
    "print('Original tokens:',tokens_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "nbpresent": {
     "id": "b31bd2f4-6a3b-4836-888a-5362ebdbc492"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT tokens: ['[CLS]', 'З', '##лат', '##ото', 'е', 'под', '##лу', '##дил', '##о', 'всички', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print('BERT tokens:',tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT labels: ['-PAD-', 'N', 'N', 'Y', 'N', 'N', '-PAD-']\n"
     ]
    }
   ],
   "source": [
    "bert_labels(train_label[2])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "So the allignment 'warranties' -> 'NOUN' becomes after alligning '##ies' -> 'NOUN'\n",
    "We tested the other (proposed in run_classifier.py) possible allignment 'warrant' -> 'NOUN', but with worse results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 3, 4, 8, 9, 10, 11]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_to_tok_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 19517, 546, 10316, 31425, 119, 102]\n"
     ]
    }
   ],
   "source": [
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the conversion of an example to BERT representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "nbpresent": {
     "id": "a8524cd7-a390-455a-b94d-995aabcbfb02"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Create InputExamples\"\"\"\n",
    "InputExamples = []\n",
    "for text, label in zip(train_text[0:1], train_label[0:1]):\n",
    "    InputExamples.append(\n",
    "        InputExample(guid=None, text_a=text, text_b=None, label=label)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "nbpresent": {
     "id": "da8ff012-152f-4533-975c-b23c3a63611c"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "868f9536017a49488afe689a516dcd4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Converting examples to features', max=1, style=ProgressStyle(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "input_ids, input_masks, segment_ids, labels = [], [], [], []\n",
    "for example in tqdm_notebook(InputExamples, desc=\"Converting examples to features\"):\n",
    "    input_id, input_mask, segment_id, label = convert_single_example(\n",
    "        tokenizer, example, MAX_SEQUENCE_LENGTH+2\n",
    "    )\n",
    "    input_ids.append(input_id)\n",
    "    input_masks.append(input_mask)\n",
    "    segment_ids.append(segment_id)\n",
    "    labels.append(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert examples to BERT representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "nbpresent": {
     "id": "eb4915fc-babf-4bf9-8789-ab05ef24be45"
    }
   },
   "outputs": [],
   "source": [
    "# Convert data to InputExample format\n",
    "train_examples = convert_text_to_examples(train_text, train_label)\n",
    "test_examples = convert_text_to_examples(test_text, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "nbpresent": {
     "id": "4f427047-23e0-4dd4-b8b7-333c93eb88ce"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3990d555997d417ca96c8cdfec8415ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Converting examples to features', max=1908, style=ProgressSty…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dec5ba47e554b039774ca7e4c57106e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Converting examples to features', max=477, style=ProgressStyl…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert to features\n",
    "(train_input_ids, train_input_masks, train_segment_ids, train_labels_ids\n",
    ") = convert_examples_to_features(tokenizer, train_examples, max_seq_length=MAX_SEQUENCE_LENGTH+2)\n",
    "(test_input_ids, test_input_masks, test_segment_ids, test_labels_ids\n",
    ") = convert_examples_to_features(tokenizer, test_examples, max_seq_length=MAX_SEQUENCE_LENGTH+2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See the form of BERT representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72\n",
      "72\n",
      "72\n",
      "72\n"
     ]
    }
   ],
   "source": [
    "print(len(train_input_ids[0]))\n",
    "print(len(train_input_masks[0]))\n",
    "print(len(train_segment_ids[0]))\n",
    "print(len(train_labels_ids[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "nbpresent": {
     "id": "e3ab4f77-4587-4fd0-8fbc-b06840e852f5"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  101,   164,   115, 29672,   100, 12184, 14614,   119,   166,\n",
       "         102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "nbpresent": {
     "id": "996d81bf-c1ce-4d09-8315-8d3c4512ddb8"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input_masks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "nbpresent": {
     "id": "d16548f1-b1a4-4003-b90e-c28164d4012d"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_segment_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "nbpresent": {
     "id": "4343b8c0-e6e3-4198-b209-968b28ec4fa4"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "nbpresent": {
     "id": "080448a8-1f00-46b8-b4c9-e8aff53658ae"
    }
   },
   "outputs": [],
   "source": [
    "# One-hot encode labels\n",
    "train_labels = to_categorical(train_labels_ids, num_classes=n_tags)\n",
    "test_labels = to_categorical(test_labels_ids, num_classes=n_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "nbpresent": {
     "id": "2a4bc11b-a307-4121-8c42-44dde65ae4cf"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  101,   164,   115, 29672,   100, 12184, 14614,   119,   166,\n",
       "          102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0]),\n",
       " array([[1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.]], dtype=float32))"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input_ids[0], train_labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "nbpresent": {
     "id": "4098aad1-cdf7-4282-ba3b-70897cdcfe1d"
    }
   },
   "outputs": [],
   "source": [
    "class BertLayer(Layer):\n",
    "    def __init__(self, output_representation='sequence_output', trainable=True, **kwargs):\n",
    "        self.bert = None\n",
    "        super(BertLayer, self).__init__(**kwargs)\n",
    "        \n",
    "        self.trainable = trainable\n",
    "        self.output_representation = output_representation\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # SetUp tensorflow Hub module\n",
    "        self.bert = hub.Module(bert_path,\n",
    "                               trainable=self.trainable, \n",
    "                               name=\"{}_module\".format(self.name))\n",
    "\n",
    "        # Assign module's trainable weights to model\n",
    "        # Remove unused layers and set trainable parameters\n",
    "        # s = [\"/cls/\", \"/pooler/\", 'layer_11', 'layer_10', 'layer_9', 'layer_8', 'layer_7', 'layer_6']\n",
    "        s = [\"/cls/\", \"/pooler/\"]\n",
    "        self.trainable_weights += [var for var in self.bert.variables[:] if not any(x in var.name for x in s)]\n",
    "            \n",
    "        for var in self.bert.variables:\n",
    "            if var not in self._trainable_weights:\n",
    "                self._non_trainable_weights.append(var)\n",
    "                \n",
    "        # See Trainable Variables\n",
    "        #tf.logging.info(\"**** Trainable Variables ****\")\n",
    "        #for var in self.trainable_weights:\n",
    "        #    init_string = \", *INIT_FROM_CKPT*\"\n",
    "        #    tf.logging.info(\"  name = %s, shape = %s%s\", var.name, var.shape, init_string)\n",
    "            \n",
    "        print('Trainable weights:',len(self.trainable_weights))\n",
    "        super(BertLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        inputs = [K.cast(x, dtype=\"int32\") for x in inputs]\n",
    "        input_ids, input_mask, segment_ids = inputs\n",
    "        bert_inputs = dict(\n",
    "            input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids\n",
    "        )\n",
    "        result = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\n",
    "            self.output_representation\n",
    "        ]\n",
    "        return result\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return K.not_equal(inputs[0], 0.0)   \n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if self.output_representation == 'pooled_output':\n",
    "            return (None, 768)\n",
    "        else:\n",
    "            return (None, None, 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "nbpresent": {
     "id": "177cc483-831b-4a10-b659-e1c6cf653877"
    }
   },
   "outputs": [],
   "source": [
    "#del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "nbpresent": {
     "id": "08a4002f-f656-47f2-814d-447e0bd56858"
    }
   },
   "outputs": [],
   "source": [
    "# Build model\n",
    "def build_model(max_seq_length):\n",
    "    seed = 0 \n",
    "    in_id = keras.layers.Input(shape=(max_seq_length,), name=\"input_ids\")\n",
    "    in_mask = keras.layers.Input(shape=(max_seq_length,), name=\"input_masks\")\n",
    "    in_segment = keras.layers.Input(shape=(max_seq_length,), name=\"segment_ids\")\n",
    "    bert_inputs = [in_id, in_mask, in_segment]\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    bert_output = BertLayer()(bert_inputs)\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    outputs = keras.layers.Dense(n_tags, activation=keras.activations.softmax)(bert_output)\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    model = keras.models.Model(inputs=bert_inputs, outputs=outputs)\n",
    "    np.random.seed(seed)\n",
    "    model.compile(optimizer=keras.optimizers.Adam(lr=0.00004), loss=keras.losses.categorical_crossentropy, metrics=['accuracy'])   \n",
    "    model.summary(100)\n",
    "    return model\n",
    "\n",
    "def initialize_vars(sess):\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.tables_initializer())\n",
    "    K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "nbpresent": {
     "id": "c1391d48-0f3d-4b90-b1b4-4d8cae268050"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable weights: 197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0411 18:24:19.460079 126028 deprecation.py:506] From c:\\users\\pnedelev\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_ids (InputLayer)           (None, 72)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_masks (InputLayer)         (None, 72)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)         (None, 72)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "bert_layer_1 (BertLayer)         (None, None, 768)     178565115   input_ids[0][0]                  \n",
      "                                                                   input_masks[0][0]                \n",
      "                                                                   segment_ids[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, None, 3)       2307        bert_layer_1[0][0]               \n",
      "====================================================================================================\n",
      "Total params: 178,567,422\n",
      "Trainable params: 177,265,155\n",
      "Non-trainable params: 1,302,267\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(MAX_SEQUENCE_LENGTH+2) # We sum 2 for [CLS], [SEP] tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "`pydot` failed to call GraphViz.Please install GraphViz (https://www.graphviz.org/) and ensure that its executables are in the $PATH.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\users\\pnedelev\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages\\pydot.py\u001b[0m in \u001b[0;36mcreate\u001b[1;34m(self, prog, format, encoding)\u001b[0m\n\u001b[0;32m   1914\u001b[0m                 \u001b[0marguments\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marguments\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1915\u001b[1;33m                 \u001b[0mworking_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtmp_dir\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1916\u001b[0m             )\n",
      "\u001b[1;32mc:\\users\\pnedelev\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages\\pydot.py\u001b[0m in \u001b[0;36mcall_graphviz\u001b[1;34m(program, arguments, working_dir, **kwargs)\u001b[0m\n\u001b[0;32m    135\u001b[0m         \u001b[0mstdout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 136\u001b[1;33m         \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    137\u001b[0m     )\n",
      "\u001b[1;32mc:\\users\\pnedelev\\appdata\\local\\continuum\\anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors, text)\u001b[0m\n\u001b[0;32m    774\u001b[0m                                 \u001b[0merrread\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrwrite\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 775\u001b[1;33m                                 restore_signals, start_new_session)\n\u001b[0m\u001b[0;32m    776\u001b[0m         \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\pnedelev\\appdata\\local\\continuum\\anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_start_new_session)\u001b[0m\n\u001b[0;32m   1177\u001b[0m                                          \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcwd\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mcwd\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1178\u001b[1;33m                                          startupinfo)\n\u001b[0m\u001b[0;32m   1179\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] The system cannot find the file specified",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\users\\pnedelev\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages\\keras\\utils\\vis_utils.py\u001b[0m in \u001b[0;36m_check_pydot\u001b[1;34m()\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[1;31m# to check the pydot/graphviz installation.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[0mpydot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpydot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\pnedelev\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages\\pydot.py\u001b[0m in \u001b[0;36mcreate\u001b[1;34m(self, prog, format, encoding)\u001b[0m\n\u001b[0;32m   1921\u001b[0m                     prog=prog)\n\u001b[1;32m-> 1922\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1923\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] \"dot\" not found in path.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-115-dd929bcf50a3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplot_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'model.png'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'model.png'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\pnedelev\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages\\keras\\utils\\vis_utils.py\u001b[0m in \u001b[0;36mplot_model\u001b[1;34m(model, to_file, show_shapes, show_layer_names, rankdir, expand_nested, dpi)\u001b[0m\n\u001b[0;32m    238\u001b[0m     \"\"\"\n\u001b[0;32m    239\u001b[0m     dot = model_to_dot(model, show_shapes, show_layer_names, rankdir,\n\u001b[1;32m--> 240\u001b[1;33m                        expand_nested, dpi)\n\u001b[0m\u001b[0;32m    241\u001b[0m     \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextension\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplitext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mto_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mextension\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\pnedelev\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages\\keras\\utils\\vis_utils.py\u001b[0m in \u001b[0;36mmodel_to_dot\u001b[1;34m(model, show_shapes, show_layer_names, rankdir, expand_nested, dpi, subgraph)\u001b[0m\n\u001b[0;32m     77\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m     \u001b[0m_check_pydot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msubgraph\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[0mdot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpydot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCluster\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstyle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'dashed'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgraph_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\pnedelev\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages\\keras\\utils\\vis_utils.py\u001b[0m in \u001b[0;36m_check_pydot\u001b[1;34m()\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         raise OSError(\n\u001b[1;32m---> 31\u001b[1;33m             \u001b[1;34m'`pydot` failed to call GraphViz.'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m             \u001b[1;34m'Please install GraphViz (https://www.graphviz.org/) '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m             'and ensure that its executables are in the $PATH.')\n",
      "\u001b[1;31mOSError\u001b[0m: `pydot` failed to call GraphViz.Please install GraphViz (https://www.graphviz.org/) and ensure that its executables are in the $PATH."
     ]
    }
   ],
   "source": [
    "plot_model(model, to_file='model.png', show_shapes=True)\n",
    "Image('model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "nbpresent": {
     "id": "e5b1510d-5965-4f77-8c1c-a019bb081441"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1908, 72)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "nbpresent": {
     "id": "e2ced586-54d2-4f87-87b4-11447a074836"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1908, 72)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input_masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "nbpresent": {
     "id": "d4e8c809-bc83-428b-824d-4640127d0a6f"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1908, 72)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_segment_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1908, 72, 3)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training (BERT fine-tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialize_vars(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "nbpresent": {
     "id": "fc7dd289-a946-479a-838e-e25e6384f16a"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0411 18:25:24.001424 126028 deprecation.py:323] From c:\\users\\pnedelev\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "c:\\users\\pnedelev\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "W0411 18:25:51.783643 126028 module_wrapper.py:139] From c:\\users\\pnedelev\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "W0411 18:25:51.786636 126028 module_wrapper.py:139] From c:\\users\\pnedelev\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:431: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1908 samples, validate on 477 samples\n",
      "Epoch 1/30\n",
      "1908/1908 [==============================] - ETA: 52:04 - loss: 0.2790 - accuracy: 0.290 - ETA: 43:40 - loss: 0.2062 - accuracy: 0.502 - ETA: 39:20 - loss: 0.1793 - accuracy: 0.582 - ETA: 36:17 - loss: 0.1787 - accuracy: 0.633 - ETA: 34:40 - loss: 0.1680 - accuracy: 0.666 - ETA: 33:37 - loss: 0.1565 - accuracy: 0.709 - ETA: 32:29 - loss: 0.1482 - accuracy: 0.740 - ETA: 31:05 - loss: 0.1388 - accuracy: 0.766 - ETA: 29:54 - loss: 0.1312 - accuracy: 0.778 - ETA: 28:54 - loss: 0.1240 - accuracy: 0.789 - ETA: 28:11 - loss: 0.1171 - accuracy: 0.797 - ETA: 28:00 - loss: 0.1109 - accuracy: 0.807 - ETA: 27:30 - loss: 0.1079 - accuracy: 0.815 - ETA: 26:53 - loss: 0.1030 - accuracy: 0.822 - ETA: 26:21 - loss: 0.0983 - accuracy: 0.829 - ETA: 25:52 - loss: 0.0943 - accuracy: 0.836 - ETA: 25:26 - loss: 0.0913 - accuracy: 0.840 - ETA: 24:56 - loss: 0.0880 - accuracy: 0.845 - ETA: 24:34 - loss: 0.0851 - accuracy: 0.849 - ETA: 24:20 - loss: 0.0827 - accuracy: 0.855 - ETA: 24:09 - loss: 0.0799 - accuracy: 0.861 - ETA: 23:51 - loss: 0.0775 - accuracy: 0.866 - ETA: 23:27 - loss: 0.0747 - accuracy: 0.869 - ETA: 23:05 - loss: 0.0726 - accuracy: 0.872 - ETA: 22:46 - loss: 0.0704 - accuracy: 0.875 - ETA: 22:23 - loss: 0.0687 - accuracy: 0.878 - ETA: 22:03 - loss: 0.0671 - accuracy: 0.881 - ETA: 21:45 - loss: 0.0657 - accuracy: 0.884 - ETA: 21:25 - loss: 0.0643 - accuracy: 0.886 - ETA: 21:08 - loss: 0.0628 - accuracy: 0.890 - ETA: 20:52 - loss: 0.0611 - accuracy: 0.892 - ETA: 20:35 - loss: 0.0594 - accuracy: 0.895 - ETA: 20:16 - loss: 0.0582 - accuracy: 0.897 - ETA: 20:01 - loss: 0.0570 - accuracy: 0.899 - ETA: 19:46 - loss: 0.0560 - accuracy: 0.902 - ETA: 19:28 - loss: 0.0548 - accuracy: 0.904 - ETA: 19:12 - loss: 0.0538 - accuracy: 0.906 - ETA: 18:55 - loss: 0.0527 - accuracy: 0.908 - ETA: 18:38 - loss: 0.0517 - accuracy: 0.910 - ETA: 18:22 - loss: 0.0507 - accuracy: 0.912 - ETA: 18:06 - loss: 0.0500 - accuracy: 0.914 - ETA: 17:51 - loss: 0.0491 - accuracy: 0.915 - ETA: 17:37 - loss: 0.0482 - accuracy: 0.917 - ETA: 17:24 - loss: 0.0473 - accuracy: 0.919 - ETA: 17:09 - loss: 0.0465 - accuracy: 0.920 - ETA: 16:57 - loss: 0.0456 - accuracy: 0.921 - ETA: 16:46 - loss: 0.0449 - accuracy: 0.922 - ETA: 16:33 - loss: 0.0442 - accuracy: 0.924 - ETA: 16:19 - loss: 0.0435 - accuracy: 0.925 - ETA: 16:04 - loss: 0.0431 - accuracy: 0.925 - ETA: 15:49 - loss: 0.0425 - accuracy: 0.927 - ETA: 15:34 - loss: 0.0422 - accuracy: 0.927 - ETA: 15:18 - loss: 0.0417 - accuracy: 0.928 - ETA: 15:03 - loss: 0.0414 - accuracy: 0.929 - ETA: 14:49 - loss: 0.0408 - accuracy: 0.930 - ETA: 14:34 - loss: 0.0402 - accuracy: 0.931 - ETA: 14:19 - loss: 0.0398 - accuracy: 0.932 - ETA: 14:04 - loss: 0.0394 - accuracy: 0.933 - ETA: 13:49 - loss: 0.0390 - accuracy: 0.934 - ETA: 13:34 - loss: 0.0385 - accuracy: 0.935 - ETA: 13:19 - loss: 0.0380 - accuracy: 0.936 - ETA: 13:05 - loss: 0.0376 - accuracy: 0.936 - ETA: 12:52 - loss: 0.0372 - accuracy: 0.937 - ETA: 12:38 - loss: 0.0368 - accuracy: 0.938 - ETA: 12:24 - loss: 0.0364 - accuracy: 0.939 - ETA: 12:10 - loss: 0.0360 - accuracy: 0.939 - ETA: 11:55 - loss: 0.0356 - accuracy: 0.940 - ETA: 11:42 - loss: 0.0351 - accuracy: 0.941 - ETA: 11:28 - loss: 0.0347 - accuracy: 0.942 - ETA: 11:13 - loss: 0.0344 - accuracy: 0.942 - ETA: 10:59 - loss: 0.0341 - accuracy: 0.943 - ETA: 10:44 - loss: 0.0337 - accuracy: 0.944 - ETA: 10:30 - loss: 0.0334 - accuracy: 0.944 - ETA: 10:15 - loss: 0.0331 - accuracy: 0.945 - ETA: 10:00 - loss: 0.0327 - accuracy: 0.945 - ETA: 9:46 - loss: 0.0325 - accuracy: 0.945 - ETA: 9:32 - loss: 0.0321 - accuracy: 0.94 - ETA: 9:18 - loss: 0.0319 - accuracy: 0.94 - ETA: 9:04 - loss: 0.0316 - accuracy: 0.94 - ETA: 8:50 - loss: 0.0313 - accuracy: 0.94 - ETA: 8:36 - loss: 0.0310 - accuracy: 0.94 - ETA: 8:22 - loss: 0.0308 - accuracy: 0.94 - ETA: 8:08 - loss: 0.0305 - accuracy: 0.94 - ETA: 7:54 - loss: 0.0302 - accuracy: 0.95 - ETA: 7:40 - loss: 0.0300 - accuracy: 0.95 - ETA: 7:26 - loss: 0.0297 - accuracy: 0.95 - ETA: 7:12 - loss: 0.0296 - accuracy: 0.95 - ETA: 6:58 - loss: 0.0294 - accuracy: 0.95 - ETA: 6:45 - loss: 0.0291 - accuracy: 0.95 - ETA: 6:31 - loss: 0.0288 - accuracy: 0.95 - ETA: 6:17 - loss: 0.0286 - accuracy: 0.95 - ETA: 6:04 - loss: 0.0283 - accuracy: 0.95 - ETA: 5:50 - loss: 0.0282 - accuracy: 0.95 - ETA: 5:37 - loss: 0.0279 - accuracy: 0.95 - ETA: 5:23 - loss: 0.0276 - accuracy: 0.95 - ETA: 5:09 - loss: 0.0274 - accuracy: 0.95 - ETA: 4:56 - loss: 0.0272 - accuracy: 0.95 - ETA: 4:42 - loss: 0.0270 - accuracy: 0.95 - ETA: 4:29 - loss: 0.0268 - accuracy: 0.95 - ETA: 4:16 - loss: 0.0266 - accuracy: 0.95 - ETA: 4:02 - loss: 0.0263 - accuracy: 0.95 - ETA: 3:49 - loss: 0.0261 - accuracy: 0.95 - ETA: 3:35 - loss: 0.0259 - accuracy: 0.95 - ETA: 3:21 - loss: 0.0257 - accuracy: 0.95 - ETA: 3:08 - loss: 0.0256 - accuracy: 0.95 - ETA: 2:55 - loss: 0.0255 - accuracy: 0.95 - ETA: 2:41 - loss: 0.0254 - accuracy: 0.95 - ETA: 2:28 - loss: 0.0254 - accuracy: 0.95 - ETA: 2:15 - loss: 0.0252 - accuracy: 0.95 - ETA: 2:01 - loss: 0.0250 - accuracy: 0.95 - ETA: 1:48 - loss: 0.0248 - accuracy: 0.95 - ETA: 1:35 - loss: 0.0247 - accuracy: 0.95 - ETA: 1:22 - loss: 0.0246 - accuracy: 0.95 - ETA: 1:08 - loss: 0.0245 - accuracy: 0.95 - ETA: 55s - loss: 0.0244 - accuracy: 0.9600 - ETA: 42s - loss: 0.0242 - accuracy: 0.960 - ETA: 29s - loss: 0.0240 - accuracy: 0.960 - ETA: 16s - loss: 0.0240 - accuracy: 0.960 - ETA: 3s - loss: 0.0238 - accuracy: 0.960 - 1656s 868ms/step - loss: 0.0238 - accuracy: 0.9608 - val_loss: 0.0099 - val_accuracy: 0.9832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\pnedelev\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages\\keras\\callbacks\\callbacks.py:707: RuntimeWarning: Can save best model only with val_acc available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n",
      "c:\\users\\pnedelev\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages\\keras\\callbacks\\callbacks.py:846: RuntimeWarning: Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: val_loss,val_accuracy,loss,accuracy\n",
      "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30\n",
      "1908/1908 [==============================] - ETA: 26:15 - loss: 0.0031 - accuracy: 0.996 - ETA: 25:12 - loss: 0.0046 - accuracy: 0.995 - ETA: 24:31 - loss: 0.0056 - accuracy: 0.993 - ETA: 23:47 - loss: 0.0051 - accuracy: 0.993 - ETA: 23:26 - loss: 0.0056 - accuracy: 0.992 - ETA: 23:31 - loss: 0.0069 - accuracy: 0.990 - ETA: 23:47 - loss: 0.0068 - accuracy: 0.989 - ETA: 23:47 - loss: 0.0070 - accuracy: 0.988 - ETA: 23:45 - loss: 0.0063 - accuracy: 0.989 - ETA: 23:18 - loss: 0.0061 - accuracy: 0.989 - ETA: 22:55 - loss: 0.0060 - accuracy: 0.989 - ETA: 22:28 - loss: 0.0057 - accuracy: 0.990 - ETA: 22:12 - loss: 0.0057 - accuracy: 0.990 - ETA: 21:55 - loss: 0.0054 - accuracy: 0.991 - ETA: 21:32 - loss: 0.0052 - accuracy: 0.991 - ETA: 21:18 - loss: 0.0050 - accuracy: 0.991 - ETA: 21:02 - loss: 0.0049 - accuracy: 0.991 - ETA: 20:45 - loss: 0.0050 - accuracy: 0.991 - ETA: 20:28 - loss: 0.0048 - accuracy: 0.991 - ETA: 20:15 - loss: 0.0051 - accuracy: 0.991 - ETA: 20:02 - loss: 0.0051 - accuracy: 0.991 - ETA: 19:46 - loss: 0.0051 - accuracy: 0.991 - ETA: 19:28 - loss: 0.0051 - accuracy: 0.991 - ETA: 19:11 - loss: 0.0050 - accuracy: 0.991 - ETA: 18:57 - loss: 0.0051 - accuracy: 0.991 - ETA: 18:43 - loss: 0.0050 - accuracy: 0.991 - ETA: 18:26 - loss: 0.0050 - accuracy: 0.991 - ETA: 18:11 - loss: 0.0049 - accuracy: 0.991 - ETA: 17:56 - loss: 0.0049 - accuracy: 0.991 - ETA: 17:43 - loss: 0.0048 - accuracy: 0.991 - ETA: 17:32 - loss: 0.0048 - accuracy: 0.991 - ETA: 17:18 - loss: 0.0048 - accuracy: 0.991 - ETA: 17:06 - loss: 0.0049 - accuracy: 0.991 - ETA: 16:54 - loss: 0.0049 - accuracy: 0.991 - ETA: 16:44 - loss: 0.0050 - accuracy: 0.990 - ETA: 16:30 - loss: 0.0051 - accuracy: 0.990 - ETA: 16:19 - loss: 0.0050 - accuracy: 0.990 - ETA: 16:08 - loss: 0.0050 - accuracy: 0.990 - ETA: 15:58 - loss: 0.0049 - accuracy: 0.991 - ETA: 15:48 - loss: 0.0048 - accuracy: 0.991 - ETA: 15:38 - loss: 0.0048 - accuracy: 0.991 - ETA: 15:27 - loss: 0.0048 - accuracy: 0.991 - ETA: 15:17 - loss: 0.0047 - accuracy: 0.991 - ETA: 15:03 - loss: 0.0048 - accuracy: 0.991 - ETA: 14:50 - loss: 0.0047 - accuracy: 0.991 - ETA: 14:35 - loss: 0.0047 - accuracy: 0.991 - ETA: 14:21 - loss: 0.0046 - accuracy: 0.991 - ETA: 14:08 - loss: 0.0047 - accuracy: 0.991 - ETA: 13:56 - loss: 0.0046 - accuracy: 0.992 - ETA: 13:43 - loss: 0.0045 - accuracy: 0.992 - ETA: 13:30 - loss: 0.0044 - accuracy: 0.992 - ETA: 13:17 - loss: 0.0046 - accuracy: 0.992 - ETA: 13:05 - loss: 0.0046 - accuracy: 0.992 - ETA: 12:53 - loss: 0.0047 - accuracy: 0.992 - ETA: 12:41 - loss: 0.0047 - accuracy: 0.992 - ETA: 12:29 - loss: 0.0046 - accuracy: 0.992 - ETA: 12:17 - loss: 0.0046 - accuracy: 0.992 - ETA: 12:04 - loss: 0.0046 - accuracy: 0.992 - ETA: 11:52 - loss: 0.0046 - accuracy: 0.992 - ETA: 11:41 - loss: 0.0046 - accuracy: 0.992 - ETA: 11:29 - loss: 0.0051 - accuracy: 0.991 - ETA: 11:18 - loss: 0.0050 - accuracy: 0.991 - ETA: 11:07 - loss: 0.0050 - accuracy: 0.992 - ETA: 10:56 - loss: 0.0050 - accuracy: 0.992 - ETA: 10:44 - loss: 0.0051 - accuracy: 0.991 - ETA: 10:32 - loss: 0.0050 - accuracy: 0.991 - ETA: 10:21 - loss: 0.0050 - accuracy: 0.991 - ETA: 10:09 - loss: 0.0049 - accuracy: 0.992 - ETA: 9:57 - loss: 0.0049 - accuracy: 0.992 - ETA: 9:46 - loss: 0.0050 - accuracy: 0.99 - ETA: 9:34 - loss: 0.0050 - accuracy: 0.99 - ETA: 9:22 - loss: 0.0050 - accuracy: 0.99 - ETA: 9:11 - loss: 0.0050 - accuracy: 0.99 - ETA: 8:59 - loss: 0.0051 - accuracy: 0.99 - ETA: 8:47 - loss: 0.0051 - accuracy: 0.99 - ETA: 8:35 - loss: 0.0051 - accuracy: 0.99 - ETA: 8:23 - loss: 0.0052 - accuracy: 0.99 - ETA: 8:11 - loss: 0.0052 - accuracy: 0.99 - ETA: 7:59 - loss: 0.0053 - accuracy: 0.99 - ETA: 7:47 - loss: 0.0052 - accuracy: 0.99 - ETA: 7:36 - loss: 0.0052 - accuracy: 0.99 - ETA: 7:24 - loss: 0.0052 - accuracy: 0.99 - ETA: 7:12 - loss: 0.0052 - accuracy: 0.99 - ETA: 7:01 - loss: 0.0053 - accuracy: 0.99 - ETA: 6:50 - loss: 0.0053 - accuracy: 0.99 - ETA: 6:38 - loss: 0.0053 - accuracy: 0.99 - ETA: 6:26 - loss: 0.0053 - accuracy: 0.99 - ETA: 6:15 - loss: 0.0053 - accuracy: 0.99 - ETA: 6:03 - loss: 0.0053 - accuracy: 0.99 - ETA: 5:51 - loss: 0.0053 - accuracy: 0.99 - ETA: 5:39 - loss: 0.0053 - accuracy: 0.99 - ETA: 5:27 - loss: 0.0053 - accuracy: 0.99 - ETA: 5:15 - loss: 0.0053 - accuracy: 0.99 - ETA: 5:03 - loss: 0.0053 - accuracy: 0.99 - ETA: 4:51 - loss: 0.0052 - accuracy: 0.99 - ETA: 4:39 - loss: 0.0052 - accuracy: 0.99 - ETA: 4:27 - loss: 0.0052 - accuracy: 0.99 - ETA: 4:15 - loss: 0.0053 - accuracy: 0.99 - ETA: 4:03 - loss: 0.0052 - accuracy: 0.99 - ETA: 3:51 - loss: 0.0053 - accuracy: 0.99 - ETA: 3:39 - loss: 0.0053 - accuracy: 0.99 - ETA: 3:27 - loss: 0.0052 - accuracy: 0.99 - ETA: 3:14 - loss: 0.0052 - accuracy: 0.99 - ETA: 3:02 - loss: 0.0052 - accuracy: 0.99 - ETA: 2:51 - loss: 0.0051 - accuracy: 0.99 - ETA: 2:39 - loss: 0.0051 - accuracy: 0.99 - ETA: 2:27 - loss: 0.0051 - accuracy: 0.99 - ETA: 2:15 - loss: 0.0052 - accuracy: 0.99 - ETA: 2:03 - loss: 0.0051 - accuracy: 0.99 - ETA: 1:51 - loss: 0.0051 - accuracy: 0.99 - ETA: 1:39 - loss: 0.0051 - accuracy: 0.99 - ETA: 1:27 - loss: 0.0051 - accuracy: 0.99 - ETA: 1:15 - loss: 0.0052 - accuracy: 0.99 - ETA: 1:03 - loss: 0.0051 - accuracy: 0.99 - ETA: 51s - loss: 0.0051 - accuracy: 0.9920 - ETA: 39s - loss: 0.0051 - accuracy: 0.992 - ETA: 27s - loss: 0.0051 - accuracy: 0.992 - ETA: 15s - loss: 0.0051 - accuracy: 0.992 - ETA: 3s - loss: 0.0051 - accuracy: 0.991 - 1533s 803ms/step - loss: 0.0051 - accuracy: 0.9919 - val_loss: 0.0090 - val_accuracy: 0.9856\n",
      "Epoch 3/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1908/1908 [==============================] - ETA: 23:09 - loss: 0.0026 - accuracy: 0.984 - ETA: 22:59 - loss: 0.0015 - accuracy: 0.992 - ETA: 22:48 - loss: 0.0026 - accuracy: 0.993 - ETA: 22:47 - loss: 0.0021 - accuracy: 0.995 - ETA: 22:50 - loss: 0.0018 - accuracy: 0.996 - ETA: 22:37 - loss: 0.0016 - accuracy: 0.996 - ETA: 22:37 - loss: 0.0014 - accuracy: 0.997 - ETA: 22:23 - loss: 0.0015 - accuracy: 0.997 - ETA: 22:14 - loss: 0.0014 - accuracy: 0.997 - ETA: 22:00 - loss: 0.0015 - accuracy: 0.997 - ETA: 21:44 - loss: 0.0015 - accuracy: 0.997 - ETA: 21:28 - loss: 0.0014 - accuracy: 0.997 - ETA: 21:15 - loss: 0.0014 - accuracy: 0.998 - ETA: 21:05 - loss: 0.0015 - accuracy: 0.997 - ETA: 20:52 - loss: 0.0014 - accuracy: 0.997 - ETA: 20:39 - loss: 0.0019 - accuracy: 0.997 - ETA: 20:31 - loss: 0.0019 - accuracy: 0.997 - ETA: 20:18 - loss: 0.0020 - accuracy: 0.996 - ETA: 20:06 - loss: 0.0020 - accuracy: 0.996 - ETA: 19:54 - loss: 0.0021 - accuracy: 0.996 - ETA: 19:42 - loss: 0.0020 - accuracy: 0.996 - ETA: 19:31 - loss: 0.0020 - accuracy: 0.996 - ETA: 19:17 - loss: 0.0019 - accuracy: 0.996 - ETA: 19:02 - loss: 0.0019 - accuracy: 0.996 - ETA: 18:50 - loss: 0.0020 - accuracy: 0.996 - ETA: 18:37 - loss: 0.0020 - accuracy: 0.996 - ETA: 18:27 - loss: 0.0019 - accuracy: 0.996 - ETA: 18:14 - loss: 0.0019 - accuracy: 0.996 - ETA: 18:04 - loss: 0.0019 - accuracy: 0.996 - ETA: 17:52 - loss: 0.0019 - accuracy: 0.996 - ETA: 17:39 - loss: 0.0018 - accuracy: 0.996 - ETA: 17:28 - loss: 0.0019 - accuracy: 0.996 - ETA: 17:15 - loss: 0.0018 - accuracy: 0.996 - ETA: 17:03 - loss: 0.0018 - accuracy: 0.996 - ETA: 16:53 - loss: 0.0018 - accuracy: 0.996 - ETA: 16:43 - loss: 0.0018 - accuracy: 0.996 - ETA: 16:33 - loss: 0.0019 - accuracy: 0.996 - ETA: 16:20 - loss: 0.0018 - accuracy: 0.996 - ETA: 16:08 - loss: 0.0018 - accuracy: 0.996 - ETA: 15:56 - loss: 0.0017 - accuracy: 0.996 - ETA: 15:44 - loss: 0.0017 - accuracy: 0.996 - ETA: 15:33 - loss: 0.0017 - accuracy: 0.997 - ETA: 15:20 - loss: 0.0017 - accuracy: 0.997 - ETA: 15:08 - loss: 0.0017 - accuracy: 0.996 - ETA: 14:56 - loss: 0.0019 - accuracy: 0.996 - ETA: 14:43 - loss: 0.0018 - accuracy: 0.996 - ETA: 14:31 - loss: 0.0018 - accuracy: 0.996 - ETA: 14:18 - loss: 0.0018 - accuracy: 0.996 - ETA: 14:06 - loss: 0.0018 - accuracy: 0.996 - ETA: 13:53 - loss: 0.0018 - accuracy: 0.996 - ETA: 13:41 - loss: 0.0018 - accuracy: 0.996 - ETA: 13:29 - loss: 0.0018 - accuracy: 0.996 - ETA: 13:18 - loss: 0.0018 - accuracy: 0.997 - ETA: 13:05 - loss: 0.0018 - accuracy: 0.997 - ETA: 12:54 - loss: 0.0018 - accuracy: 0.997 - ETA: 12:42 - loss: 0.0019 - accuracy: 0.997 - ETA: 12:30 - loss: 0.0019 - accuracy: 0.996 - ETA: 12:18 - loss: 0.0019 - accuracy: 0.996 - ETA: 12:06 - loss: 0.0019 - accuracy: 0.996 - ETA: 11:54 - loss: 0.0019 - accuracy: 0.996 - ETA: 11:42 - loss: 0.0019 - accuracy: 0.996 - ETA: 11:30 - loss: 0.0019 - accuracy: 0.996 - ETA: 11:17 - loss: 0.0019 - accuracy: 0.996 - ETA: 11:05 - loss: 0.0019 - accuracy: 0.997 - ETA: 10:53 - loss: 0.0019 - accuracy: 0.997 - ETA: 10:42 - loss: 0.0019 - accuracy: 0.997 - ETA: 10:30 - loss: 0.0019 - accuracy: 0.997 - ETA: 10:17 - loss: 0.0019 - accuracy: 0.997 - ETA: 10:05 - loss: 0.0018 - accuracy: 0.997 - ETA: 9:54 - loss: 0.0018 - accuracy: 0.997 - ETA: 9:42 - loss: 0.0018 - accuracy: 0.99 - ETA: 9:30 - loss: 0.0018 - accuracy: 0.99 - ETA: 9:18 - loss: 0.0018 - accuracy: 0.99 - ETA: 9:06 - loss: 0.0018 - accuracy: 0.99 - ETA: 8:54 - loss: 0.0018 - accuracy: 0.99 - ETA: 8:42 - loss: 0.0019 - accuracy: 0.99 - ETA: 8:30 - loss: 0.0019 - accuracy: 0.99 - ETA: 8:18 - loss: 0.0019 - accuracy: 0.99 - ETA: 8:06 - loss: 0.0019 - accuracy: 0.99 - ETA: 7:54 - loss: 0.0018 - accuracy: 0.99 - ETA: 7:41 - loss: 0.0019 - accuracy: 0.99 - ETA: 7:29 - loss: 0.0019 - accuracy: 0.99 - ETA: 7:17 - loss: 0.0019 - accuracy: 0.99 - ETA: 7:05 - loss: 0.0019 - accuracy: 0.99 - ETA: 6:53 - loss: 0.0019 - accuracy: 0.99 - ETA: 6:41 - loss: 0.0019 - accuracy: 0.99 - ETA: 6:29 - loss: 0.0018 - accuracy: 0.99 - ETA: 6:17 - loss: 0.0018 - accuracy: 0.99 - ETA: 6:05 - loss: 0.0019 - accuracy: 0.99 - ETA: 5:53 - loss: 0.0019 - accuracy: 0.99 - ETA: 5:41 - loss: 0.0019 - accuracy: 0.99 - ETA: 5:29 - loss: 0.0019 - accuracy: 0.99 - ETA: 5:17 - loss: 0.0019 - accuracy: 0.99 - ETA: 5:05 - loss: 0.0019 - accuracy: 0.99 - ETA: 4:53 - loss: 0.0019 - accuracy: 0.99 - ETA: 4:41 - loss: 0.0019 - accuracy: 0.99 - ETA: 4:29 - loss: 0.0019 - accuracy: 0.99 - ETA: 4:17 - loss: 0.0019 - accuracy: 0.99 - ETA: 4:04 - loss: 0.0019 - accuracy: 0.99 - ETA: 3:52 - loss: 0.0019 - accuracy: 0.99 - ETA: 3:40 - loss: 0.0019 - accuracy: 0.99 - ETA: 3:28 - loss: 0.0019 - accuracy: 0.99 - ETA: 3:16 - loss: 0.0019 - accuracy: 0.99 - ETA: 3:04 - loss: 0.0018 - accuracy: 0.99 - ETA: 2:52 - loss: 0.0018 - accuracy: 0.99 - ETA: 2:40 - loss: 0.0018 - accuracy: 0.99 - ETA: 2:28 - loss: 0.0018 - accuracy: 0.99 - ETA: 2:16 - loss: 0.0018 - accuracy: 0.99 - ETA: 2:04 - loss: 0.0019 - accuracy: 0.99 - ETA: 1:51 - loss: 0.0019 - accuracy: 0.99 - ETA: 1:40 - loss: 0.0019 - accuracy: 0.99 - ETA: 1:28 - loss: 0.0019 - accuracy: 0.99 - ETA: 1:16 - loss: 0.0019 - accuracy: 0.99 - ETA: 1:04 - loss: 0.0018 - accuracy: 0.99 - ETA: 51s - loss: 0.0019 - accuracy: 0.9970 - ETA: 39s - loss: 0.0018 - accuracy: 0.997 - ETA: 27s - loss: 0.0018 - accuracy: 0.997 - ETA: 15s - loss: 0.0018 - accuracy: 0.997 - ETA: 3s - loss: 0.0018 - accuracy: 0.997 - 1569s 822ms/step - loss: 0.0018 - accuracy: 0.9971 - val_loss: 0.0090 - val_accuracy: 0.9892\n",
      "Epoch 4/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1908/1908 [==============================] - ETA: 24:41 - loss: 6.2522e-04 - accuracy: 1.000 - ETA: 25:05 - loss: 3.8898e-04 - accuracy: 1.000 - ETA: 25:52 - loss: 2.7626e-04 - accuracy: 1.000 - ETA: 25:10 - loss: 4.7657e-04 - accuracy: 0.998 - ETA: 24:44 - loss: 4.0596e-04 - accuracy: 0.999 - ETA: 24:31 - loss: 5.7111e-04 - accuracy: 0.998 - ETA: 24:10 - loss: 9.3675e-04 - accuracy: 0.998 - ETA: 23:52 - loss: 8.6885e-04 - accuracy: 0.998 - ETA: 23:35 - loss: 9.5388e-04 - accuracy: 0.998 - ETA: 23:24 - loss: 9.4397e-04 - accuracy: 0.998 - ETA: 23:16 - loss: 0.0011 - accuracy: 0.9981    - ETA: 22:57 - loss: 0.0011 - accuracy: 0.998 - ETA: 22:39 - loss: 0.0010 - accuracy: 0.998 - ETA: 22:21 - loss: 9.4353e-04 - accuracy: 0.998 - ETA: 22:06 - loss: 9.4744e-04 - accuracy: 0.998 - ETA: 21:49 - loss: 8.9419e-04 - accuracy: 0.998 - ETA: 21:32 - loss: 8.7067e-04 - accuracy: 0.998 - ETA: 21:14 - loss: 8.8620e-04 - accuracy: 0.998 - ETA: 20:56 - loss: 8.4675e-04 - accuracy: 0.998 - ETA: 20:41 - loss: 8.1039e-04 - accuracy: 0.998 - ETA: 20:26 - loss: 7.9135e-04 - accuracy: 0.998 - ETA: 20:13 - loss: 8.1023e-04 - accuracy: 0.998 - ETA: 20:04 - loss: 8.2236e-04 - accuracy: 0.998 - ETA: 19:49 - loss: 8.2341e-04 - accuracy: 0.998 - ETA: 19:36 - loss: 7.9837e-04 - accuracy: 0.998 - ETA: 19:22 - loss: 7.7212e-04 - accuracy: 0.998 - ETA: 19:10 - loss: 7.4526e-04 - accuracy: 0.998 - ETA: 18:57 - loss: 7.2100e-04 - accuracy: 0.998 - ETA: 18:43 - loss: 7.8654e-04 - accuracy: 0.998 - ETA: 18:29 - loss: 7.9029e-04 - accuracy: 0.998 - ETA: 18:19 - loss: 7.7131e-04 - accuracy: 0.998 - ETA: 18:06 - loss: 7.5529e-04 - accuracy: 0.998 - ETA: 17:54 - loss: 7.3516e-04 - accuracy: 0.998 - ETA: 17:40 - loss: 7.1718e-04 - accuracy: 0.998 - ETA: 17:29 - loss: 7.0081e-04 - accuracy: 0.998 - ETA: 17:16 - loss: 6.8286e-04 - accuracy: 0.998 - ETA: 17:03 - loss: 6.9072e-04 - accuracy: 0.998 - ETA: 16:51 - loss: 6.7345e-04 - accuracy: 0.998 - ETA: 16:38 - loss: 6.5934e-04 - accuracy: 0.998 - ETA: 16:24 - loss: 6.4662e-04 - accuracy: 0.998 - ETA: 16:10 - loss: 7.8658e-04 - accuracy: 0.998 - ETA: 15:57 - loss: 7.7943e-04 - accuracy: 0.998 - ETA: 15:45 - loss: 7.6794e-04 - accuracy: 0.998 - ETA: 15:32 - loss: 7.5277e-04 - accuracy: 0.998 - ETA: 15:20 - loss: 7.7799e-04 - accuracy: 0.998 - ETA: 15:08 - loss: 7.7929e-04 - accuracy: 0.998 - ETA: 14:57 - loss: 8.3030e-04 - accuracy: 0.998 - ETA: 14:45 - loss: 8.3019e-04 - accuracy: 0.998 - ETA: 14:32 - loss: 0.0011 - accuracy: 0.9981    - ETA: 14:20 - loss: 0.0011 - accuracy: 0.998 - ETA: 14:08 - loss: 0.0011 - accuracy: 0.998 - ETA: 13:56 - loss: 0.0010 - accuracy: 0.998 - ETA: 13:43 - loss: 0.0010 - accuracy: 0.998 - ETA: 13:30 - loss: 0.0010 - accuracy: 0.998 - ETA: 13:18 - loss: 0.0011 - accuracy: 0.998 - ETA: 13:07 - loss: 0.0011 - accuracy: 0.998 - ETA: 12:54 - loss: 0.0011 - accuracy: 0.998 - ETA: 12:41 - loss: 0.0010 - accuracy: 0.998 - ETA: 12:29 - loss: 0.0010 - accuracy: 0.998 - ETA: 12:16 - loss: 0.0010 - accuracy: 0.998 - ETA: 12:03 - loss: 0.0010 - accuracy: 0.998 - ETA: 11:51 - loss: 0.0010 - accuracy: 0.998 - ETA: 11:38 - loss: 0.0010 - accuracy: 0.998 - ETA: 11:25 - loss: 0.0010 - accuracy: 0.998 - ETA: 11:12 - loss: 0.0010 - accuracy: 0.998 - ETA: 10:59 - loss: 0.0010 - accuracy: 0.998 - ETA: 10:46 - loss: 9.9557e-04 - accuracy: 0.998 - ETA: 10:33 - loss: 9.8224e-04 - accuracy: 0.998 - ETA: 10:21 - loss: 9.8646e-04 - accuracy: 0.998 - ETA: 10:08 - loss: 9.8806e-04 - accuracy: 0.998 - ETA: 9:56 - loss: 0.0010 - accuracy: 0.9984    - ETA: 9:44 - loss: 0.0010 - accuracy: 0.99 - ETA: 9:31 - loss: 0.0010 - accuracy: 0.99 - ETA: 9:19 - loss: 9.9230e-04 - accuracy: 0.99 - ETA: 9:06 - loss: 9.8058e-04 - accuracy: 0.99 - ETA: 8:54 - loss: 9.7154e-04 - accuracy: 0.99 - ETA: 8:41 - loss: 9.8653e-04 - accuracy: 0.99 - ETA: 8:29 - loss: 9.8595e-04 - accuracy: 0.99 - ETA: 8:16 - loss: 0.0010 - accuracy: 0.9984   - ETA: 8:03 - loss: 9.9382e-04 - accuracy: 0.99 - ETA: 7:51 - loss: 9.9384e-04 - accuracy: 0.99 - ETA: 7:39 - loss: 9.8576e-04 - accuracy: 0.99 - ETA: 7:27 - loss: 9.7750e-04 - accuracy: 0.99 - ETA: 7:15 - loss: 9.7308e-04 - accuracy: 0.99 - ETA: 7:03 - loss: 0.0010 - accuracy: 0.9983   - ETA: 6:51 - loss: 9.9843e-04 - accuracy: 0.99 - ETA: 6:39 - loss: 9.8998e-04 - accuracy: 0.99 - ETA: 6:27 - loss: 9.8551e-04 - accuracy: 0.99 - ETA: 6:14 - loss: 9.8003e-04 - accuracy: 0.99 - ETA: 6:02 - loss: 9.7048e-04 - accuracy: 0.99 - ETA: 5:49 - loss: 9.6178e-04 - accuracy: 0.99 - ETA: 5:37 - loss: 0.0010 - accuracy: 0.9984   - ETA: 5:24 - loss: 0.0010 - accuracy: 0.99 - ETA: 5:12 - loss: 0.0010 - accuracy: 0.99 - ETA: 4:59 - loss: 0.0010 - accuracy: 0.99 - ETA: 4:47 - loss: 9.9861e-04 - accuracy: 0.99 - ETA: 4:35 - loss: 9.9255e-04 - accuracy: 0.99 - ETA: 4:22 - loss: 0.0010 - accuracy: 0.9984   - ETA: 4:10 - loss: 9.9515e-04 - accuracy: 0.99 - ETA: 3:57 - loss: 9.8791e-04 - accuracy: 0.99 - ETA: 3:45 - loss: 9.7992e-04 - accuracy: 0.99 - ETA: 3:33 - loss: 9.7162e-04 - accuracy: 0.99 - ETA: 3:20 - loss: 9.6692e-04 - accuracy: 0.99 - ETA: 3:08 - loss: 9.5824e-04 - accuracy: 0.99 - ETA: 2:56 - loss: 9.5481e-04 - accuracy: 0.99 - ETA: 2:43 - loss: 9.4787e-04 - accuracy: 0.99 - ETA: 2:31 - loss: 9.4609e-04 - accuracy: 0.99 - ETA: 2:19 - loss: 9.3985e-04 - accuracy: 0.99 - ETA: 2:06 - loss: 9.3354e-04 - accuracy: 0.99 - ETA: 1:54 - loss: 9.3967e-04 - accuracy: 0.99 - ETA: 1:41 - loss: 9.3187e-04 - accuracy: 0.99 - ETA: 1:29 - loss: 9.2952e-04 - accuracy: 0.99 - ETA: 1:17 - loss: 9.2286e-04 - accuracy: 0.99 - ETA: 1:04 - loss: 9.1573e-04 - accuracy: 0.99 - ETA: 52s - loss: 9.1790e-04 - accuracy: 0.9986 - ETA: 40s - loss: 9.1076e-04 - accuracy: 0.998 - ETA: 27s - loss: 9.1999e-04 - accuracy: 0.998 - ETA: 15s - loss: 9.1348e-04 - accuracy: 0.998 - ETA: 3s - loss: 9.1267e-04 - accuracy: 0.998 - 1566s 821ms/step - loss: 9.1079e-04 - accuracy: 0.9985 - val_loss: 0.0113 - val_accuracy: 0.9864\n",
      "Epoch 5/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1908/1908 [==============================] - ETA: 24:08 - loss: 1.8384e-04 - accuracy: 1.000 - ETA: 24:07 - loss: 1.3864e-04 - accuracy: 1.000 - ETA: 23:31 - loss: 2.8441e-04 - accuracy: 1.000 - ETA: 23:21 - loss: 2.4297e-04 - accuracy: 1.000 - ETA: 23:15 - loss: 2.1572e-04 - accuracy: 1.000 - ETA: 23:00 - loss: 1.9251e-04 - accuracy: 1.000 - ETA: 22:46 - loss: 1.7749e-04 - accuracy: 1.000 - ETA: 22:32 - loss: 4.5887e-04 - accuracy: 0.999 - ETA: 22:22 - loss: 4.1823e-04 - accuracy: 0.999 - ETA: 22:03 - loss: 3.8062e-04 - accuracy: 0.999 - ETA: 21:45 - loss: 3.6030e-04 - accuracy: 0.999 - ETA: 21:30 - loss: 3.3994e-04 - accuracy: 0.999 - ETA: 21:15 - loss: 3.2235e-04 - accuracy: 0.999 - ETA: 21:01 - loss: 3.0476e-04 - accuracy: 0.999 - ETA: 20:49 - loss: 2.9373e-04 - accuracy: 0.999 - ETA: 20:37 - loss: 2.8176e-04 - accuracy: 0.999 - ETA: 20:23 - loss: 3.1285e-04 - accuracy: 0.999 - ETA: 20:15 - loss: 3.2659e-04 - accuracy: 0.999 - ETA: 20:04 - loss: 3.6623e-04 - accuracy: 0.999 - ETA: 19:53 - loss: 3.7785e-04 - accuracy: 0.999 - ETA: 19:40 - loss: 3.6307e-04 - accuracy: 0.999 - ETA: 19:29 - loss: 3.4869e-04 - accuracy: 0.999 - ETA: 19:20 - loss: 3.3625e-04 - accuracy: 0.999 - ETA: 19:06 - loss: 3.5616e-04 - accuracy: 0.999 - ETA: 18:53 - loss: 3.7521e-04 - accuracy: 0.999 - ETA: 18:41 - loss: 3.6120e-04 - accuracy: 0.999 - ETA: 18:34 - loss: 3.5420e-04 - accuracy: 0.999 - ETA: 18:23 - loss: 3.5169e-04 - accuracy: 0.999 - ETA: 18:12 - loss: 3.4276e-04 - accuracy: 0.999 - ETA: 18:04 - loss: 3.3162e-04 - accuracy: 0.999 - ETA: 17:54 - loss: 3.4709e-04 - accuracy: 0.999 - ETA: 17:42 - loss: 3.5685e-04 - accuracy: 0.999 - ETA: 17:32 - loss: 3.4696e-04 - accuracy: 0.999 - ETA: 17:20 - loss: 3.3958e-04 - accuracy: 0.999 - ETA: 17:08 - loss: 3.3227e-04 - accuracy: 0.999 - ETA: 16:56 - loss: 3.2574e-04 - accuracy: 0.999 - ETA: 16:43 - loss: 3.1808e-04 - accuracy: 0.999 - ETA: 16:30 - loss: 3.1037e-04 - accuracy: 0.999 - ETA: 16:16 - loss: 3.0269e-04 - accuracy: 0.999 - ETA: 16:03 - loss: 2.9614e-04 - accuracy: 0.999 - ETA: 15:51 - loss: 2.8952e-04 - accuracy: 0.999 - ETA: 15:39 - loss: 2.9131e-04 - accuracy: 0.999 - ETA: 15:28 - loss: 2.8493e-04 - accuracy: 0.999 - ETA: 15:16 - loss: 2.7885e-04 - accuracy: 0.999 - ETA: 15:03 - loss: 2.7659e-04 - accuracy: 0.999 - ETA: 14:51 - loss: 2.8929e-04 - accuracy: 0.999 - ETA: 14:39 - loss: 2.8368e-04 - accuracy: 0.999 - ETA: 14:27 - loss: 2.7978e-04 - accuracy: 0.999 - ETA: 14:15 - loss: 2.7455e-04 - accuracy: 0.999 - ETA: 14:02 - loss: 2.6971e-04 - accuracy: 0.999 - ETA: 13:49 - loss: 2.6464e-04 - accuracy: 0.999 - ETA: 13:37 - loss: 2.9116e-04 - accuracy: 0.999 - ETA: 13:25 - loss: 2.8591e-04 - accuracy: 0.999 - ETA: 13:13 - loss: 2.8085e-04 - accuracy: 0.999 - ETA: 13:01 - loss: 2.7627e-04 - accuracy: 0.999 - ETA: 12:49 - loss: 2.8902e-04 - accuracy: 0.999 - ETA: 12:36 - loss: 2.8447e-04 - accuracy: 0.999 - ETA: 12:24 - loss: 2.8160e-04 - accuracy: 0.999 - ETA: 12:12 - loss: 2.7911e-04 - accuracy: 0.999 - ETA: 12:00 - loss: 2.7475e-04 - accuracy: 0.999 - ETA: 11:47 - loss: 2.7105e-04 - accuracy: 0.999 - ETA: 11:35 - loss: 2.6690e-04 - accuracy: 0.999 - ETA: 11:22 - loss: 2.6293e-04 - accuracy: 0.999 - ETA: 11:10 - loss: 2.6014e-04 - accuracy: 0.999 - ETA: 10:58 - loss: 2.5924e-04 - accuracy: 0.999 - ETA: 10:46 - loss: 2.5588e-04 - accuracy: 0.999 - ETA: 10:34 - loss: 2.5518e-04 - accuracy: 0.999 - ETA: 10:22 - loss: 2.5247e-04 - accuracy: 0.999 - ETA: 10:10 - loss: 2.5004e-04 - accuracy: 0.999 - ETA: 9:57 - loss: 2.4744e-04 - accuracy: 0.999 - ETA: 9:45 - loss: 2.6939e-04 - accuracy: 0.99 - ETA: 9:33 - loss: 2.6581e-04 - accuracy: 0.99 - ETA: 9:21 - loss: 2.9094e-04 - accuracy: 0.99 - ETA: 9:09 - loss: 2.8768e-04 - accuracy: 0.99 - ETA: 8:57 - loss: 2.8401e-04 - accuracy: 0.99 - ETA: 8:45 - loss: 2.8047e-04 - accuracy: 0.99 - ETA: 8:33 - loss: 2.8049e-04 - accuracy: 0.99 - ETA: 8:20 - loss: 2.8924e-04 - accuracy: 0.99 - ETA: 8:08 - loss: 2.8580e-04 - accuracy: 0.99 - ETA: 7:57 - loss: 2.8401e-04 - accuracy: 0.99 - ETA: 7:44 - loss: 2.8072e-04 - accuracy: 0.99 - ETA: 7:32 - loss: 2.7744e-04 - accuracy: 0.99 - ETA: 7:20 - loss: 2.7473e-04 - accuracy: 0.99 - ETA: 7:08 - loss: 2.7165e-04 - accuracy: 0.99 - ETA: 6:56 - loss: 2.7201e-04 - accuracy: 0.99 - ETA: 6:43 - loss: 2.7235e-04 - accuracy: 0.99 - ETA: 6:31 - loss: 2.6944e-04 - accuracy: 0.99 - ETA: 6:19 - loss: 2.6648e-04 - accuracy: 0.99 - ETA: 6:06 - loss: 2.6382e-04 - accuracy: 0.99 - ETA: 5:54 - loss: 2.6160e-04 - accuracy: 0.99 - ETA: 5:42 - loss: 2.5898e-04 - accuracy: 0.99 - ETA: 5:30 - loss: 2.6812e-04 - accuracy: 0.99 - ETA: 5:18 - loss: 2.6539e-04 - accuracy: 0.99 - ETA: 5:06 - loss: 2.6306e-04 - accuracy: 0.99 - ETA: 4:54 - loss: 2.6920e-04 - accuracy: 0.99 - ETA: 4:42 - loss: 2.6647e-04 - accuracy: 0.99 - ETA: 4:30 - loss: 2.6598e-04 - accuracy: 0.99 - ETA: 4:17 - loss: 2.6405e-04 - accuracy: 0.99 - ETA: 4:05 - loss: 2.6189e-04 - accuracy: 0.99 - ETA: 3:53 - loss: 2.5952e-04 - accuracy: 0.99 - ETA: 3:41 - loss: 2.5797e-04 - accuracy: 0.99 - ETA: 3:29 - loss: 2.5907e-04 - accuracy: 0.99 - ETA: 3:16 - loss: 2.5665e-04 - accuracy: 0.99 - ETA: 3:04 - loss: 2.6041e-04 - accuracy: 0.99 - ETA: 2:52 - loss: 2.5986e-04 - accuracy: 0.99 - ETA: 2:40 - loss: 2.5783e-04 - accuracy: 0.99 - ETA: 2:28 - loss: 3.2685e-04 - accuracy: 0.99 - ETA: 2:16 - loss: 3.2443e-04 - accuracy: 0.99 - ETA: 2:04 - loss: 3.2178e-04 - accuracy: 0.99 - ETA: 1:52 - loss: 3.1901e-04 - accuracy: 0.99 - ETA: 1:40 - loss: 3.1738e-04 - accuracy: 0.99 - ETA: 1:27 - loss: 3.1467e-04 - accuracy: 0.99 - ETA: 1:15 - loss: 3.5195e-04 - accuracy: 0.99 - ETA: 1:03 - loss: 3.4924e-04 - accuracy: 0.99 - ETA: 51s - loss: 3.4673e-04 - accuracy: 0.9994 - ETA: 39s - loss: 3.4401e-04 - accuracy: 0.999 - ETA: 27s - loss: 3.4344e-04 - accuracy: 0.999 - ETA: 15s - loss: 3.4163e-04 - accuracy: 0.999 - ETA: 3s - loss: 3.4073e-04 - accuracy: 0.999 - 1545s 810ms/step - loss: 3.4019e-04 - accuracy: 0.9994 - val_loss: 0.0118 - val_accuracy: 0.9871\n",
      "Epoch 6/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1908/1908 [==============================] - ETA: 22:44 - loss: 1.3594e-04 - accuracy: 1.000 - ETA: 23:23 - loss: 8.6021e-05 - accuracy: 1.000 - ETA: 24:02 - loss: 9.6483e-05 - accuracy: 1.000 - ETA: 23:41 - loss: 1.1246e-04 - accuracy: 1.000 - ETA: 23:31 - loss: 1.5447e-04 - accuracy: 1.000 - ETA: 23:19 - loss: 1.9718e-04 - accuracy: 1.000 - ETA: 23:22 - loss: 1.9407e-04 - accuracy: 1.000 - ETA: 22:59 - loss: 1.8752e-04 - accuracy: 1.000 - ETA: 22:49 - loss: 1.7256e-04 - accuracy: 1.000 - ETA: 22:40 - loss: 1.5980e-04 - accuracy: 1.000 - ETA: 22:28 - loss: 1.5242e-04 - accuracy: 1.000 - ETA: 22:15 - loss: 1.4568e-04 - accuracy: 1.000 - ETA: 22:02 - loss: 1.6847e-04 - accuracy: 1.000 - ETA: 21:46 - loss: 1.5938e-04 - accuracy: 1.000 - ETA: 21:33 - loss: 1.6267e-04 - accuracy: 1.000 - ETA: 21:18 - loss: 1.5424e-04 - accuracy: 1.000 - ETA: 21:06 - loss: 1.4791e-04 - accuracy: 1.000 - ETA: 20:53 - loss: 1.4230e-04 - accuracy: 1.000 - ETA: 20:38 - loss: 1.8620e-04 - accuracy: 0.999 - ETA: 20:22 - loss: 1.7794e-04 - accuracy: 0.999 - ETA: 20:08 - loss: 1.7040e-04 - accuracy: 0.999 - ETA: 19:54 - loss: 1.7974e-04 - accuracy: 0.999 - ETA: 19:41 - loss: 1.7925e-04 - accuracy: 0.999 - ETA: 19:27 - loss: 1.9447e-04 - accuracy: 0.999 - ETA: 19:14 - loss: 1.8813e-04 - accuracy: 0.999 - ETA: 19:03 - loss: 1.8173e-04 - accuracy: 0.999 - ETA: 18:53 - loss: 1.7650e-04 - accuracy: 0.999 - ETA: 18:40 - loss: 1.7075e-04 - accuracy: 0.999 - ETA: 18:27 - loss: 1.6526e-04 - accuracy: 0.999 - ETA: 18:13 - loss: 1.6023e-04 - accuracy: 0.999 - ETA: 18:00 - loss: 1.5581e-04 - accuracy: 0.999 - ETA: 17:47 - loss: 1.5131e-04 - accuracy: 0.999 - ETA: 17:35 - loss: 1.4834e-04 - accuracy: 0.999 - ETA: 17:22 - loss: 1.4640e-04 - accuracy: 0.999 - ETA: 17:09 - loss: 1.4524e-04 - accuracy: 0.999 - ETA: 16:56 - loss: 3.3413e-04 - accuracy: 0.999 - ETA: 16:44 - loss: 3.2888e-04 - accuracy: 0.999 - ETA: 16:31 - loss: 3.3623e-04 - accuracy: 0.999 - ETA: 16:20 - loss: 3.2791e-04 - accuracy: 0.999 - ETA: 16:07 - loss: 3.2003e-04 - accuracy: 0.999 - ETA: 15:54 - loss: 3.2201e-04 - accuracy: 0.999 - ETA: 15:42 - loss: 3.1974e-04 - accuracy: 0.999 - ETA: 15:30 - loss: 3.1512e-04 - accuracy: 0.999 - ETA: 15:17 - loss: 3.1496e-04 - accuracy: 0.999 - ETA: 15:04 - loss: 3.0959e-04 - accuracy: 0.999 - ETA: 14:51 - loss: 3.4911e-04 - accuracy: 0.999 - ETA: 14:38 - loss: 3.6198e-04 - accuracy: 0.999 - ETA: 14:26 - loss: 3.6039e-04 - accuracy: 0.999 - ETA: 14:14 - loss: 3.5349e-04 - accuracy: 0.999 - ETA: 14:01 - loss: 3.4676e-04 - accuracy: 0.999 - ETA: 13:50 - loss: 3.4016e-04 - accuracy: 0.999 - ETA: 13:38 - loss: 3.3417e-04 - accuracy: 0.999 - ETA: 13:25 - loss: 3.2829e-04 - accuracy: 0.999 - ETA: 13:13 - loss: 3.2365e-04 - accuracy: 0.999 - ETA: 13:00 - loss: 3.1806e-04 - accuracy: 0.999 - ETA: 12:48 - loss: 3.1260e-04 - accuracy: 0.999 - ETA: 12:35 - loss: 3.0731e-04 - accuracy: 0.999 - ETA: 12:23 - loss: 3.0410e-04 - accuracy: 0.999 - ETA: 12:10 - loss: 2.9911e-04 - accuracy: 0.999 - ETA: 11:57 - loss: 2.9517e-04 - accuracy: 0.999 - ETA: 11:45 - loss: 2.9071e-04 - accuracy: 0.999 - ETA: 11:33 - loss: 2.8724e-04 - accuracy: 0.999 - ETA: 11:21 - loss: 2.8295e-04 - accuracy: 0.999 - ETA: 11:09 - loss: 2.8057e-04 - accuracy: 0.999 - ETA: 10:58 - loss: 2.7722e-04 - accuracy: 0.999 - ETA: 10:45 - loss: 2.7316e-04 - accuracy: 0.999 - ETA: 10:33 - loss: 2.6976e-04 - accuracy: 0.999 - ETA: 10:22 - loss: 2.6697e-04 - accuracy: 0.999 - ETA: 10:10 - loss: 2.6450e-04 - accuracy: 0.999 - ETA: 9:58 - loss: 2.6185e-04 - accuracy: 0.999 - ETA: 9:45 - loss: 2.5883e-04 - accuracy: 0.99 - ETA: 9:33 - loss: 2.5558e-04 - accuracy: 0.99 - ETA: 9:21 - loss: 2.7059e-04 - accuracy: 0.99 - ETA: 9:09 - loss: 2.6727e-04 - accuracy: 0.99 - ETA: 8:56 - loss: 2.6395e-04 - accuracy: 0.99 - ETA: 8:45 - loss: 2.6107e-04 - accuracy: 0.99 - ETA: 8:32 - loss: 2.5786e-04 - accuracy: 0.99 - ETA: 8:20 - loss: 2.5473e-04 - accuracy: 0.99 - ETA: 8:08 - loss: 2.7259e-04 - accuracy: 0.99 - ETA: 7:56 - loss: 2.6927e-04 - accuracy: 0.99 - ETA: 7:44 - loss: 2.6815e-04 - accuracy: 0.99 - ETA: 7:31 - loss: 2.6502e-04 - accuracy: 0.99 - ETA: 7:19 - loss: 2.6209e-04 - accuracy: 0.99 - ETA: 7:07 - loss: 2.5950e-04 - accuracy: 0.99 - ETA: 6:54 - loss: 2.5652e-04 - accuracy: 0.99 - ETA: 6:42 - loss: 2.5376e-04 - accuracy: 0.99 - ETA: 6:30 - loss: 3.4315e-04 - accuracy: 0.99 - ETA: 6:18 - loss: 3.4172e-04 - accuracy: 0.99 - ETA: 6:06 - loss: 3.6115e-04 - accuracy: 0.99 - ETA: 5:54 - loss: 3.5796e-04 - accuracy: 0.99 - ETA: 5:42 - loss: 3.5733e-04 - accuracy: 0.99 - ETA: 5:29 - loss: 3.5528e-04 - accuracy: 0.99 - ETA: 5:17 - loss: 5.1047e-04 - accuracy: 0.99 - ETA: 5:05 - loss: 5.7201e-04 - accuracy: 0.99 - ETA: 4:53 - loss: 5.8539e-04 - accuracy: 0.99 - ETA: 4:41 - loss: 5.8367e-04 - accuracy: 0.99 - ETA: 4:29 - loss: 5.7912e-04 - accuracy: 0.99 - ETA: 4:17 - loss: 5.7449e-04 - accuracy: 0.99 - ETA: 4:04 - loss: 5.7109e-04 - accuracy: 0.99 - ETA: 3:52 - loss: 5.6748e-04 - accuracy: 0.99 - ETA: 3:40 - loss: 5.8529e-04 - accuracy: 0.99 - ETA: 3:28 - loss: 5.8462e-04 - accuracy: 0.99 - ETA: 3:16 - loss: 5.9442e-04 - accuracy: 0.99 - ETA: 3:04 - loss: 6.0457e-04 - accuracy: 0.99 - ETA: 2:52 - loss: 6.0288e-04 - accuracy: 0.99 - ETA: 2:40 - loss: 5.9889e-04 - accuracy: 0.99 - ETA: 2:28 - loss: 5.9717e-04 - accuracy: 0.99 - ETA: 2:15 - loss: 6.0461e-04 - accuracy: 0.99 - ETA: 2:03 - loss: 6.0030e-04 - accuracy: 0.99 - ETA: 1:51 - loss: 5.9828e-04 - accuracy: 0.99 - ETA: 1:39 - loss: 6.4549e-04 - accuracy: 0.99 - ETA: 1:27 - loss: 6.8217e-04 - accuracy: 0.99 - ETA: 1:15 - loss: 6.7842e-04 - accuracy: 0.99 - ETA: 1:03 - loss: 6.7801e-04 - accuracy: 0.99 - ETA: 51s - loss: 6.7721e-04 - accuracy: 0.9991 - ETA: 39s - loss: 6.7425e-04 - accuracy: 0.999 - ETA: 27s - loss: 6.7224e-04 - accuracy: 0.999 - ETA: 15s - loss: 6.7223e-04 - accuracy: 0.999 - ETA: 3s - loss: 6.6721e-04 - accuracy: 0.999 - 1534s 804ms/step - loss: 6.6589e-04 - accuracy: 0.9992 - val_loss: 0.0128 - val_accuracy: 0.9838\n",
      "Epoch 7/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1908/1908 [==============================] - ETA: 21:34 - loss: 0.0035 - accuracy: 0.992 - ETA: 22:16 - loss: 0.0024 - accuracy: 0.994 - ETA: 22:24 - loss: 0.0016 - accuracy: 0.996 - ETA: 22:28 - loss: 0.0013 - accuracy: 0.997 - ETA: 22:35 - loss: 0.0011 - accuracy: 0.997 - ETA: 22:51 - loss: 9.5497e-04 - accuracy: 0.997 - ETA: 22:53 - loss: 8.8803e-04 - accuracy: 0.998 - ETA: 22:52 - loss: 0.0011 - accuracy: 0.9975    - ETA: 22:55 - loss: 0.0010 - accuracy: 0.997 - ETA: 22:56 - loss: 9.4294e-04 - accuracy: 0.997 - ETA: 22:47 - loss: 8.6276e-04 - accuracy: 0.998 - ETA: 22:30 - loss: 8.0124e-04 - accuracy: 0.998 - ETA: 22:11 - loss: 9.9429e-04 - accuracy: 0.997 - ETA: 21:57 - loss: 0.0010 - accuracy: 0.9976    - ETA: 21:49 - loss: 0.0010 - accuracy: 0.997 - ETA: 21:33 - loss: 0.0010 - accuracy: 0.997 - ETA: 21:17 - loss: 0.0011 - accuracy: 0.997 - ETA: 21:03 - loss: 0.0011 - accuracy: 0.997 - ETA: 20:50 - loss: 0.0011 - accuracy: 0.997 - ETA: 20:38 - loss: 0.0010 - accuracy: 0.997 - ETA: 20:27 - loss: 0.0013 - accuracy: 0.997 - ETA: 20:14 - loss: 0.0012 - accuracy: 0.997 - ETA: 20:03 - loss: 0.0012 - accuracy: 0.997 - ETA: 19:52 - loss: 0.0011 - accuracy: 0.997 - ETA: 19:38 - loss: 0.0012 - accuracy: 0.997 - ETA: 19:25 - loss: 0.0012 - accuracy: 0.997 - ETA: 19:12 - loss: 0.0012 - accuracy: 0.997 - ETA: 18:58 - loss: 0.0011 - accuracy: 0.997 - ETA: 18:43 - loss: 0.0011 - accuracy: 0.997 - ETA: 18:29 - loss: 0.0011 - accuracy: 0.997 - ETA: 18:18 - loss: 0.0010 - accuracy: 0.998 - ETA: 18:04 - loss: 0.0010 - accuracy: 0.998 - ETA: 17:50 - loss: 9.9156e-04 - accuracy: 0.998 - ETA: 17:37 - loss: 9.7714e-04 - accuracy: 0.998 - ETA: 17:25 - loss: 9.5695e-04 - accuracy: 0.998 - ETA: 17:13 - loss: 9.3215e-04 - accuracy: 0.998 - ETA: 17:00 - loss: 9.4144e-04 - accuracy: 0.998 - ETA: 16:47 - loss: 9.1773e-04 - accuracy: 0.998 - ETA: 16:35 - loss: 8.9899e-04 - accuracy: 0.998 - ETA: 16:23 - loss: 8.9926e-04 - accuracy: 0.998 - ETA: 16:11 - loss: 8.9607e-04 - accuracy: 0.998 - ETA: 15:59 - loss: 8.7909e-04 - accuracy: 0.998 - ETA: 15:46 - loss: 8.5919e-04 - accuracy: 0.998 - ETA: 15:34 - loss: 8.5346e-04 - accuracy: 0.998 - ETA: 15:22 - loss: 8.3474e-04 - accuracy: 0.998 - ETA: 15:09 - loss: 8.3584e-04 - accuracy: 0.998 - ETA: 14:57 - loss: 8.1880e-04 - accuracy: 0.998 - ETA: 14:45 - loss: 8.1807e-04 - accuracy: 0.998 - ETA: 14:32 - loss: 8.0367e-04 - accuracy: 0.998 - ETA: 14:20 - loss: 7.9189e-04 - accuracy: 0.998 - ETA: 14:07 - loss: 8.8438e-04 - accuracy: 0.998 - ETA: 13:54 - loss: 8.6756e-04 - accuracy: 0.998 - ETA: 13:41 - loss: 8.5160e-04 - accuracy: 0.998 - ETA: 13:28 - loss: 8.4178e-04 - accuracy: 0.998 - ETA: 13:15 - loss: 8.2746e-04 - accuracy: 0.998 - ETA: 13:03 - loss: 8.1334e-04 - accuracy: 0.998 - ETA: 12:50 - loss: 7.9962e-04 - accuracy: 0.998 - ETA: 12:38 - loss: 7.8607e-04 - accuracy: 0.998 - ETA: 12:26 - loss: 7.7353e-04 - accuracy: 0.998 - ETA: 12:14 - loss: 7.6893e-04 - accuracy: 0.998 - ETA: 12:01 - loss: 7.5764e-04 - accuracy: 0.998 - ETA: 11:48 - loss: 7.7045e-04 - accuracy: 0.998 - ETA: 11:36 - loss: 7.5842e-04 - accuracy: 0.998 - ETA: 11:23 - loss: 8.3882e-04 - accuracy: 0.998 - ETA: 11:11 - loss: 8.3444e-04 - accuracy: 0.998 - ETA: 10:58 - loss: 8.2639e-04 - accuracy: 0.998 - ETA: 10:46 - loss: 8.1475e-04 - accuracy: 0.998 - ETA: 10:33 - loss: 8.0373e-04 - accuracy: 0.998 - ETA: 10:20 - loss: 8.4044e-04 - accuracy: 0.998 - ETA: 10:08 - loss: 9.2747e-04 - accuracy: 0.998 - ETA: 9:56 - loss: 9.3029e-04 - accuracy: 0.998 - ETA: 9:44 - loss: 0.0010 - accuracy: 0.9984   - ETA: 9:32 - loss: 9.9392e-04 - accuracy: 0.99 - ETA: 9:19 - loss: 9.8715e-04 - accuracy: 0.99 - ETA: 9:07 - loss: 9.8337e-04 - accuracy: 0.99 - ETA: 8:54 - loss: 0.0010 - accuracy: 0.9984   - ETA: 8:42 - loss: 0.0010 - accuracy: 0.99 - ETA: 8:29 - loss: 9.9434e-04 - accuracy: 0.99 - ETA: 8:16 - loss: 0.0011 - accuracy: 0.9984   - ETA: 8:04 - loss: 0.0011 - accuracy: 0.99 - ETA: 7:52 - loss: 0.0011 - accuracy: 0.99 - ETA: 7:39 - loss: 0.0012 - accuracy: 0.99 - ETA: 7:27 - loss: 0.0012 - accuracy: 0.99 - ETA: 7:14 - loss: 0.0012 - accuracy: 0.99 - ETA: 7:02 - loss: 0.0012 - accuracy: 0.99 - ETA: 6:50 - loss: 0.0013 - accuracy: 0.99 - ETA: 6:37 - loss: 0.0013 - accuracy: 0.99 - ETA: 6:25 - loss: 0.0013 - accuracy: 0.99 - ETA: 6:13 - loss: 0.0013 - accuracy: 0.99 - ETA: 6:00 - loss: 0.0013 - accuracy: 0.99 - ETA: 5:48 - loss: 0.0013 - accuracy: 0.99 - ETA: 5:35 - loss: 0.0013 - accuracy: 0.99 - ETA: 5:23 - loss: 0.0013 - accuracy: 0.99 - ETA: 5:10 - loss: 0.0014 - accuracy: 0.99 - ETA: 4:58 - loss: 0.0014 - accuracy: 0.99 - ETA: 4:46 - loss: 0.0014 - accuracy: 0.99 - ETA: 4:33 - loss: 0.0014 - accuracy: 0.99 - ETA: 4:21 - loss: 0.0014 - accuracy: 0.99 - ETA: 4:09 - loss: 0.0014 - accuracy: 0.99 - ETA: 3:56 - loss: 0.0014 - accuracy: 0.99 - ETA: 3:44 - loss: 0.0014 - accuracy: 0.99 - ETA: 3:32 - loss: 0.0014 - accuracy: 0.99 - ETA: 3:19 - loss: 0.0014 - accuracy: 0.99 - ETA: 3:07 - loss: 0.0014 - accuracy: 0.99 - ETA: 2:55 - loss: 0.0014 - accuracy: 0.99 - ETA: 2:42 - loss: 0.0014 - accuracy: 0.99 - ETA: 2:30 - loss: 0.0014 - accuracy: 0.99 - ETA: 2:18 - loss: 0.0014 - accuracy: 0.99 - ETA: 2:05 - loss: 0.0014 - accuracy: 0.99 - ETA: 1:53 - loss: 0.0014 - accuracy: 0.99 - ETA: 1:41 - loss: 0.0014 - accuracy: 0.99 - ETA: 1:29 - loss: 0.0014 - accuracy: 0.99 - ETA: 1:16 - loss: 0.0014 - accuracy: 0.99 - ETA: 1:04 - loss: 0.0014 - accuracy: 0.99 - ETA: 52s - loss: 0.0014 - accuracy: 0.9979 - ETA: 39s - loss: 0.0014 - accuracy: 0.997 - ETA: 27s - loss: 0.0015 - accuracy: 0.997 - ETA: 15s - loss: 0.0015 - accuracy: 0.997 - ETA: 3s - loss: 0.0015 - accuracy: 0.997 - 1560s 818ms/step - loss: 0.0015 - accuracy: 0.9978 - val_loss: 0.0117 - val_accuracy: 0.9870\n",
      "Epoch 8/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1908/1908 [==============================] - ETA: 22:01 - loss: 1.3464e-04 - accuracy: 1.000 - ETA: 22:17 - loss: 3.8999e-04 - accuracy: 1.000 - ETA: 22:40 - loss: 7.3586e-04 - accuracy: 0.998 - ETA: 22:39 - loss: 0.0012 - accuracy: 0.9976    - ETA: 22:28 - loss: 0.0022 - accuracy: 0.995 - ETA: 22:21 - loss: 0.0024 - accuracy: 0.994 - ETA: 22:23 - loss: 0.0035 - accuracy: 0.993 - ETA: 22:16 - loss: 0.0034 - accuracy: 0.993 - ETA: 22:14 - loss: 0.0030 - accuracy: 0.993 - ETA: 21:59 - loss: 0.0028 - accuracy: 0.994 - ETA: 21:51 - loss: 0.0026 - accuracy: 0.995 - ETA: 21:39 - loss: 0.0026 - accuracy: 0.995 - ETA: 21:23 - loss: 0.0026 - accuracy: 0.995 - ETA: 21:10 - loss: 0.0032 - accuracy: 0.994 - ETA: 20:53 - loss: 0.0031 - accuracy: 0.994 - ETA: 20:41 - loss: 0.0029 - accuracy: 0.994 - ETA: 20:30 - loss: 0.0029 - accuracy: 0.994 - ETA: 20:17 - loss: 0.0028 - accuracy: 0.994 - ETA: 20:13 - loss: 0.0026 - accuracy: 0.994 - ETA: 20:01 - loss: 0.0026 - accuracy: 0.994 - ETA: 19:48 - loss: 0.0026 - accuracy: 0.995 - ETA: 19:38 - loss: 0.0025 - accuracy: 0.995 - ETA: 19:24 - loss: 0.0024 - accuracy: 0.995 - ETA: 19:11 - loss: 0.0024 - accuracy: 0.995 - ETA: 18:56 - loss: 0.0023 - accuracy: 0.995 - ETA: 18:43 - loss: 0.0023 - accuracy: 0.995 - ETA: 18:30 - loss: 0.0022 - accuracy: 0.995 - ETA: 18:17 - loss: 0.0021 - accuracy: 0.995 - ETA: 18:05 - loss: 0.0021 - accuracy: 0.996 - ETA: 17:53 - loss: 0.0020 - accuracy: 0.996 - ETA: 17:44 - loss: 0.0020 - accuracy: 0.996 - ETA: 17:32 - loss: 0.0019 - accuracy: 0.996 - ETA: 17:19 - loss: 0.0019 - accuracy: 0.996 - ETA: 17:08 - loss: 0.0018 - accuracy: 0.996 - ETA: 16:56 - loss: 0.0018 - accuracy: 0.996 - ETA: 16:43 - loss: 0.0017 - accuracy: 0.996 - ETA: 16:30 - loss: 0.0017 - accuracy: 0.996 - ETA: 16:18 - loss: 0.0018 - accuracy: 0.996 - ETA: 16:06 - loss: 0.0018 - accuracy: 0.996 - ETA: 15:53 - loss: 0.0017 - accuracy: 0.996 - ETA: 15:43 - loss: 0.0017 - accuracy: 0.996 - ETA: 15:33 - loss: 0.0017 - accuracy: 0.996 - ETA: 15:23 - loss: 0.0017 - accuracy: 0.996 - ETA: 15:13 - loss: 0.0017 - accuracy: 0.997 - ETA: 15:01 - loss: 0.0017 - accuracy: 0.997 - ETA: 14:50 - loss: 0.0016 - accuracy: 0.997 - ETA: 14:37 - loss: 0.0016 - accuracy: 0.997 - ETA: 14:25 - loss: 0.0016 - accuracy: 0.997 - ETA: 14:14 - loss: 0.0016 - accuracy: 0.997 - ETA: 14:01 - loss: 0.0015 - accuracy: 0.997 - ETA: 13:48 - loss: 0.0015 - accuracy: 0.997 - ETA: 13:35 - loss: 0.0015 - accuracy: 0.997 - ETA: 13:23 - loss: 0.0016 - accuracy: 0.997 - ETA: 13:11 - loss: 0.0016 - accuracy: 0.997 - ETA: 12:59 - loss: 0.0016 - accuracy: 0.997 - ETA: 12:47 - loss: 0.0017 - accuracy: 0.997 - ETA: 12:35 - loss: 0.0017 - accuracy: 0.997 - ETA: 12:22 - loss: 0.0016 - accuracy: 0.997 - ETA: 12:11 - loss: 0.0016 - accuracy: 0.997 - ETA: 11:59 - loss: 0.0016 - accuracy: 0.997 - ETA: 11:47 - loss: 0.0016 - accuracy: 0.997 - ETA: 11:34 - loss: 0.0015 - accuracy: 0.997 - ETA: 11:22 - loss: 0.0015 - accuracy: 0.997 - ETA: 11:09 - loss: 0.0016 - accuracy: 0.997 - ETA: 10:57 - loss: 0.0016 - accuracy: 0.997 - ETA: 10:45 - loss: 0.0016 - accuracy: 0.997 - ETA: 10:32 - loss: 0.0015 - accuracy: 0.997 - ETA: 10:20 - loss: 0.0016 - accuracy: 0.997 - ETA: 10:09 - loss: 0.0016 - accuracy: 0.997 - ETA: 9:57 - loss: 0.0016 - accuracy: 0.997 - ETA: 9:45 - loss: 0.0016 - accuracy: 0.99 - ETA: 9:32 - loss: 0.0016 - accuracy: 0.99 - ETA: 9:20 - loss: 0.0016 - accuracy: 0.99 - ETA: 9:08 - loss: 0.0016 - accuracy: 0.99 - ETA: 8:56 - loss: 0.0016 - accuracy: 0.99 - ETA: 8:44 - loss: 0.0016 - accuracy: 0.99 - ETA: 8:31 - loss: 0.0016 - accuracy: 0.99 - ETA: 8:19 - loss: 0.0016 - accuracy: 0.99 - ETA: 8:07 - loss: 0.0017 - accuracy: 0.99 - ETA: 7:55 - loss: 0.0017 - accuracy: 0.99 - ETA: 7:43 - loss: 0.0017 - accuracy: 0.99 - ETA: 7:31 - loss: 0.0017 - accuracy: 0.99 - ETA: 7:18 - loss: 0.0017 - accuracy: 0.99 - ETA: 7:06 - loss: 0.0016 - accuracy: 0.99 - ETA: 6:54 - loss: 0.0017 - accuracy: 0.99 - ETA: 6:42 - loss: 0.0016 - accuracy: 0.99 - ETA: 6:30 - loss: 0.0016 - accuracy: 0.99 - ETA: 6:17 - loss: 0.0017 - accuracy: 0.99 - ETA: 6:05 - loss: 0.0017 - accuracy: 0.99 - ETA: 5:53 - loss: 0.0017 - accuracy: 0.99 - ETA: 5:41 - loss: 0.0017 - accuracy: 0.99 - ETA: 5:29 - loss: 0.0017 - accuracy: 0.99 - ETA: 5:17 - loss: 0.0017 - accuracy: 0.99 - ETA: 5:05 - loss: 0.0017 - accuracy: 0.99 - ETA: 4:53 - loss: 0.0016 - accuracy: 0.99 - ETA: 4:41 - loss: 0.0016 - accuracy: 0.99 - ETA: 4:29 - loss: 0.0016 - accuracy: 0.99 - ETA: 4:16 - loss: 0.0016 - accuracy: 0.99 - ETA: 4:04 - loss: 0.0016 - accuracy: 0.99 - ETA: 3:52 - loss: 0.0017 - accuracy: 0.99 - ETA: 3:40 - loss: 0.0016 - accuracy: 0.99 - ETA: 3:28 - loss: 0.0016 - accuracy: 0.99 - ETA: 3:16 - loss: 0.0016 - accuracy: 0.99 - ETA: 3:04 - loss: 0.0016 - accuracy: 0.99 - ETA: 2:51 - loss: 0.0016 - accuracy: 0.99 - ETA: 2:39 - loss: 0.0016 - accuracy: 0.99 - ETA: 2:27 - loss: 0.0016 - accuracy: 0.99 - ETA: 2:15 - loss: 0.0017 - accuracy: 0.99 - ETA: 2:03 - loss: 0.0017 - accuracy: 0.99 - ETA: 1:51 - loss: 0.0017 - accuracy: 0.99 - ETA: 1:39 - loss: 0.0017 - accuracy: 0.99 - ETA: 1:27 - loss: 0.0017 - accuracy: 0.99 - ETA: 1:15 - loss: 0.0017 - accuracy: 0.99 - ETA: 1:03 - loss: 0.0017 - accuracy: 0.99 - ETA: 51s - loss: 0.0017 - accuracy: 0.9974 - ETA: 39s - loss: 0.0017 - accuracy: 0.997 - ETA: 27s - loss: 0.0017 - accuracy: 0.997 - ETA: 15s - loss: 0.0016 - accuracy: 0.997 - ETA: 3s - loss: 0.0016 - accuracy: 0.997 - 1535s 804ms/step - loss: 0.0016 - accuracy: 0.9975 - val_loss: 0.0122 - val_accuracy: 0.9850\n",
      "Epoch 9/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1908/1908 [==============================] - ETA: 23:47 - loss: 4.6602e-05 - accuracy: 1.000 - ETA: 23:32 - loss: 8.4938e-04 - accuracy: 0.998 - ETA: 23:24 - loss: 8.9729e-04 - accuracy: 0.998 - ETA: 23:07 - loss: 6.9630e-04 - accuracy: 0.999 - ETA: 23:03 - loss: 6.0795e-04 - accuracy: 0.999 - ETA: 22:40 - loss: 9.6127e-04 - accuracy: 0.998 - ETA: 22:26 - loss: 8.3567e-04 - accuracy: 0.998 - ETA: 22:17 - loss: 7.3398e-04 - accuracy: 0.998 - ETA: 22:03 - loss: 7.1501e-04 - accuracy: 0.999 - ETA: 21:47 - loss: 8.8437e-04 - accuracy: 0.998 - ETA: 21:32 - loss: 8.4712e-04 - accuracy: 0.998 - ETA: 21:17 - loss: 7.9624e-04 - accuracy: 0.998 - ETA: 21:06 - loss: 7.3594e-04 - accuracy: 0.999 - ETA: 20:58 - loss: 6.8635e-04 - accuracy: 0.999 - ETA: 20:44 - loss: 6.4234e-04 - accuracy: 0.999 - ETA: 20:37 - loss: 6.0474e-04 - accuracy: 0.999 - ETA: 20:25 - loss: 5.9486e-04 - accuracy: 0.999 - ETA: 20:15 - loss: 7.0018e-04 - accuracy: 0.999 - ETA: 20:02 - loss: 6.6512e-04 - accuracy: 0.999 - ETA: 19:48 - loss: 6.3865e-04 - accuracy: 0.999 - ETA: 19:35 - loss: 6.0912e-04 - accuracy: 0.999 - ETA: 19:22 - loss: 5.8751e-04 - accuracy: 0.999 - ETA: 19:09 - loss: 5.6596e-04 - accuracy: 0.999 - ETA: 18:55 - loss: 7.4464e-04 - accuracy: 0.999 - ETA: 18:41 - loss: 7.1688e-04 - accuracy: 0.999 - ETA: 18:28 - loss: 6.9100e-04 - accuracy: 0.999 - ETA: 18:17 - loss: 6.8214e-04 - accuracy: 0.999 - ETA: 18:05 - loss: 6.6785e-04 - accuracy: 0.999 - ETA: 17:54 - loss: 6.4643e-04 - accuracy: 0.999 - ETA: 17:42 - loss: 6.4503e-04 - accuracy: 0.999 - ETA: 17:30 - loss: 6.3165e-04 - accuracy: 0.999 - ETA: 17:19 - loss: 6.2603e-04 - accuracy: 0.999 - ETA: 17:08 - loss: 6.8479e-04 - accuracy: 0.999 - ETA: 16:56 - loss: 6.7835e-04 - accuracy: 0.999 - ETA: 16:43 - loss: 6.8172e-04 - accuracy: 0.999 - ETA: 16:30 - loss: 6.7264e-04 - accuracy: 0.999 - ETA: 16:17 - loss: 7.0159e-04 - accuracy: 0.999 - ETA: 16:06 - loss: 7.5735e-04 - accuracy: 0.999 - ETA: 15:53 - loss: 7.5789e-04 - accuracy: 0.999 - ETA: 15:41 - loss: 7.4704e-04 - accuracy: 0.999 - ETA: 15:31 - loss: 7.3865e-04 - accuracy: 0.999 - ETA: 15:19 - loss: 7.2478e-04 - accuracy: 0.999 - ETA: 15:07 - loss: 7.0950e-04 - accuracy: 0.999 - ETA: 14:56 - loss: 6.9426e-04 - accuracy: 0.999 - ETA: 14:44 - loss: 6.8101e-04 - accuracy: 0.999 - ETA: 14:32 - loss: 6.9290e-04 - accuracy: 0.999 - ETA: 14:19 - loss: 6.8380e-04 - accuracy: 0.999 - ETA: 14:09 - loss: 6.7029e-04 - accuracy: 0.999 - ETA: 13:57 - loss: 6.8074e-04 - accuracy: 0.999 - ETA: 13:46 - loss: 6.7047e-04 - accuracy: 0.999 - ETA: 13:34 - loss: 6.7891e-04 - accuracy: 0.999 - ETA: 13:22 - loss: 6.7430e-04 - accuracy: 0.999 - ETA: 13:11 - loss: 6.6591e-04 - accuracy: 0.999 - ETA: 12:59 - loss: 6.5734e-04 - accuracy: 0.999 - ETA: 12:48 - loss: 6.4614e-04 - accuracy: 0.999 - ETA: 12:38 - loss: 6.3734e-04 - accuracy: 0.999 - ETA: 12:28 - loss: 6.2730e-04 - accuracy: 0.999 - ETA: 12:21 - loss: 6.3936e-04 - accuracy: 0.999 - ETA: 12:11 - loss: 6.4240e-04 - accuracy: 0.999 - ETA: 12:02 - loss: 6.3189e-04 - accuracy: 0.999 - ETA: 11:51 - loss: 6.2171e-04 - accuracy: 0.999 - ETA: 11:40 - loss: 6.1365e-04 - accuracy: 0.999 - ETA: 11:28 - loss: 6.1533e-04 - accuracy: 0.999 - ETA: 11:15 - loss: 6.0598e-04 - accuracy: 0.999 - ETA: 11:04 - loss: 6.0395e-04 - accuracy: 0.999 - ETA: 10:51 - loss: 7.0110e-04 - accuracy: 0.999 - ETA: 10:39 - loss: 7.0949e-04 - accuracy: 0.999 - ETA: 10:27 - loss: 6.9935e-04 - accuracy: 0.999 - ETA: 10:14 - loss: 6.9292e-04 - accuracy: 0.999 - ETA: 10:02 - loss: 6.9473e-04 - accuracy: 0.999 - ETA: 9:49 - loss: 6.8825e-04 - accuracy: 0.999 - ETA: 9:37 - loss: 6.7896e-04 - accuracy: 0.99 - ETA: 9:24 - loss: 6.8013e-04 - accuracy: 0.99 - ETA: 9:12 - loss: 7.1544e-04 - accuracy: 0.99 - ETA: 8:59 - loss: 7.0605e-04 - accuracy: 0.99 - ETA: 8:47 - loss: 6.9752e-04 - accuracy: 0.99 - ETA: 8:35 - loss: 6.9025e-04 - accuracy: 0.99 - ETA: 8:23 - loss: 7.0261e-04 - accuracy: 0.99 - ETA: 8:10 - loss: 7.0630e-04 - accuracy: 0.99 - ETA: 7:58 - loss: 7.0383e-04 - accuracy: 0.99 - ETA: 7:46 - loss: 7.3140e-04 - accuracy: 0.99 - ETA: 7:34 - loss: 7.2391e-04 - accuracy: 0.99 - ETA: 7:22 - loss: 7.2444e-04 - accuracy: 0.99 - ETA: 7:10 - loss: 7.2134e-04 - accuracy: 0.99 - ETA: 6:58 - loss: 7.2495e-04 - accuracy: 0.99 - ETA: 6:46 - loss: 7.1731e-04 - accuracy: 0.99 - ETA: 6:33 - loss: 7.1449e-04 - accuracy: 0.99 - ETA: 6:21 - loss: 7.0692e-04 - accuracy: 0.99 - ETA: 6:09 - loss: 6.9986e-04 - accuracy: 0.99 - ETA: 5:57 - loss: 7.1263e-04 - accuracy: 0.99 - ETA: 5:45 - loss: 7.0713e-04 - accuracy: 0.99 - ETA: 5:32 - loss: 7.0204e-04 - accuracy: 0.99 - ETA: 5:20 - loss: 7.0763e-04 - accuracy: 0.99 - ETA: 5:08 - loss: 7.1900e-04 - accuracy: 0.99 - ETA: 4:55 - loss: 7.1253e-04 - accuracy: 0.99 - ETA: 4:43 - loss: 7.0939e-04 - accuracy: 0.99 - ETA: 4:31 - loss: 7.0706e-04 - accuracy: 0.99 - ETA: 4:19 - loss: 7.0888e-04 - accuracy: 0.99 - ETA: 4:06 - loss: 7.0237e-04 - accuracy: 0.99 - ETA: 3:54 - loss: 6.9871e-04 - accuracy: 0.99 - ETA: 3:42 - loss: 6.9853e-04 - accuracy: 0.99 - ETA: 3:30 - loss: 6.9206e-04 - accuracy: 0.99 - ETA: 3:17 - loss: 6.8663e-04 - accuracy: 0.99 - ETA: 3:05 - loss: 7.0560e-04 - accuracy: 0.99 - ETA: 2:53 - loss: 7.7002e-04 - accuracy: 0.99 - ETA: 2:41 - loss: 7.8981e-04 - accuracy: 0.99 - ETA: 2:29 - loss: 8.5313e-04 - accuracy: 0.99 - ETA: 2:17 - loss: 8.4777e-04 - accuracy: 0.99 - ETA: 2:04 - loss: 8.4469e-04 - accuracy: 0.99 - ETA: 1:52 - loss: 8.5863e-04 - accuracy: 0.99 - ETA: 1:40 - loss: 8.7187e-04 - accuracy: 0.99 - ETA: 1:28 - loss: 8.8491e-04 - accuracy: 0.99 - ETA: 1:16 - loss: 9.0776e-04 - accuracy: 0.99 - ETA: 1:03 - loss: 9.0605e-04 - accuracy: 0.99 - ETA: 51s - loss: 9.0361e-04 - accuracy: 0.9988 - ETA: 39s - loss: 9.1213e-04 - accuracy: 0.998 - ETA: 27s - loss: 9.0586e-04 - accuracy: 0.998 - ETA: 15s - loss: 9.6798e-04 - accuracy: 0.998 - ETA: 3s - loss: 9.6596e-04 - accuracy: 0.998 - 1560s 818ms/step - loss: 9.6472e-04 - accuracy: 0.9987 - val_loss: 0.0103 - val_accuracy: 0.9874\n",
      "Epoch 10/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1908/1908 [==============================] - ETA: 27:33 - loss: 1.6948e-04 - accuracy: 1.000 - ETA: 26:37 - loss: 5.1438e-04 - accuracy: 1.000 - ETA: 25:53 - loss: 6.6580e-04 - accuracy: 0.998 - ETA: 25:29 - loss: 5.1914e-04 - accuracy: 0.999 - ETA: 24:56 - loss: 4.6590e-04 - accuracy: 0.999 - ETA: 24:17 - loss: 7.4924e-04 - accuracy: 0.998 - ETA: 23:52 - loss: 7.1689e-04 - accuracy: 0.998 - ETA: 23:36 - loss: 6.3157e-04 - accuracy: 0.999 - ETA: 23:19 - loss: 6.7010e-04 - accuracy: 0.998 - ETA: 23:11 - loss: 6.2521e-04 - accuracy: 0.998 - ETA: 23:07 - loss: 6.5611e-04 - accuracy: 0.998 - ETA: 22:56 - loss: 6.0388e-04 - accuracy: 0.999 - ETA: 22:50 - loss: 9.2374e-04 - accuracy: 0.998 - ETA: 22:46 - loss: 8.6755e-04 - accuracy: 0.998 - ETA: 22:30 - loss: 0.0011 - accuracy: 0.9984    - ETA: 22:12 - loss: 0.0011 - accuracy: 0.998 - ETA: 21:55 - loss: 0.0011 - accuracy: 0.998 - ETA: 21:39 - loss: 0.0011 - accuracy: 0.998 - ETA: 21:25 - loss: 0.0010 - accuracy: 0.998 - ETA: 21:12 - loss: 0.0011 - accuracy: 0.998 - ETA: 20:58 - loss: 0.0010 - accuracy: 0.998 - ETA: 20:48 - loss: 0.0012 - accuracy: 0.998 - ETA: 20:34 - loss: 0.0012 - accuracy: 0.998 - ETA: 20:20 - loss: 0.0015 - accuracy: 0.997 - ETA: 20:06 - loss: 0.0015 - accuracy: 0.997 - ETA: 19:54 - loss: 0.0014 - accuracy: 0.997 - ETA: 19:38 - loss: 0.0014 - accuracy: 0.997 - ETA: 19:28 - loss: 0.0014 - accuracy: 0.997 - ETA: 19:16 - loss: 0.0013 - accuracy: 0.997 - ETA: 19:02 - loss: 0.0015 - accuracy: 0.997 - ETA: 18:54 - loss: 0.0015 - accuracy: 0.997 - ETA: 18:40 - loss: 0.0015 - accuracy: 0.997 - ETA: 18:24 - loss: 0.0015 - accuracy: 0.997 - ETA: 18:07 - loss: 0.0015 - accuracy: 0.997 - ETA: 17:49 - loss: 0.0015 - accuracy: 0.997 - ETA: 17:32 - loss: 0.0015 - accuracy: 0.997 - ETA: 17:14 - loss: 0.0014 - accuracy: 0.997 - ETA: 16:56 - loss: 0.0014 - accuracy: 0.997 - ETA: 16:37 - loss: 0.0014 - accuracy: 0.997 - ETA: 16:20 - loss: 0.0014 - accuracy: 0.997 - ETA: 16:02 - loss: 0.0014 - accuracy: 0.997 - ETA: 15:46 - loss: 0.0014 - accuracy: 0.997 - ETA: 15:28 - loss: 0.0014 - accuracy: 0.997 - ETA: 15:12 - loss: 0.0013 - accuracy: 0.997 - ETA: 14:55 - loss: 0.0013 - accuracy: 0.997 - ETA: 14:40 - loss: 0.0013 - accuracy: 0.997 - ETA: 14:25 - loss: 0.0014 - accuracy: 0.997 - ETA: 14:10 - loss: 0.0014 - accuracy: 0.997 - ETA: 13:55 - loss: 0.0015 - accuracy: 0.997 - ETA: 13:40 - loss: 0.0014 - accuracy: 0.997 - ETA: 13:25 - loss: 0.0014 - accuracy: 0.997 - ETA: 13:11 - loss: 0.0016 - accuracy: 0.997 - ETA: 12:58 - loss: 0.0016 - accuracy: 0.997 - ETA: 12:45 - loss: 0.0016 - accuracy: 0.997 - ETA: 12:31 - loss: 0.0016 - accuracy: 0.997 - ETA: 12:17 - loss: 0.0016 - accuracy: 0.997 - ETA: 12:04 - loss: 0.0016 - accuracy: 0.997 - ETA: 11:51 - loss: 0.0015 - accuracy: 0.997 - ETA: 11:37 - loss: 0.0015 - accuracy: 0.997 - ETA: 11:24 - loss: 0.0016 - accuracy: 0.997 - ETA: 11:10 - loss: 0.0015 - accuracy: 0.997 - ETA: 10:57 - loss: 0.0015 - accuracy: 0.997 - ETA: 10:44 - loss: 0.0015 - accuracy: 0.997 - ETA: 10:31 - loss: 0.0015 - accuracy: 0.997 - ETA: 10:19 - loss: 0.0015 - accuracy: 0.997 - ETA: 10:06 - loss: 0.0015 - accuracy: 0.997 - ETA: 9:53 - loss: 0.0015 - accuracy: 0.997 - ETA: 9:41 - loss: 0.0014 - accuracy: 0.99 - ETA: 9:29 - loss: 0.0015 - accuracy: 0.99 - ETA: 9:16 - loss: 0.0014 - accuracy: 0.99 - ETA: 9:04 - loss: 0.0014 - accuracy: 0.99 - ETA: 8:52 - loss: 0.0014 - accuracy: 0.99 - ETA: 8:40 - loss: 0.0014 - accuracy: 0.99 - ETA: 8:28 - loss: 0.0014 - accuracy: 0.99 - ETA: 8:17 - loss: 0.0014 - accuracy: 0.99 - ETA: 8:05 - loss: 0.0014 - accuracy: 0.99 - ETA: 7:53 - loss: 0.0014 - accuracy: 0.99 - ETA: 7:42 - loss: 0.0014 - accuracy: 0.99 - ETA: 7:30 - loss: 0.0014 - accuracy: 0.99 - ETA: 7:19 - loss: 0.0014 - accuracy: 0.99 - ETA: 7:08 - loss: 0.0014 - accuracy: 0.99 - ETA: 6:56 - loss: 0.0013 - accuracy: 0.99 - ETA: 6:44 - loss: 0.0013 - accuracy: 0.99 - ETA: 6:32 - loss: 0.0013 - accuracy: 0.99 - ETA: 6:20 - loss: 0.0013 - accuracy: 0.99 - ETA: 6:09 - loss: 0.0013 - accuracy: 0.99 - ETA: 5:57 - loss: 0.0013 - accuracy: 0.99 - ETA: 5:46 - loss: 0.0013 - accuracy: 0.99 - ETA: 5:34 - loss: 0.0013 - accuracy: 0.99 - ETA: 5:23 - loss: 0.0014 - accuracy: 0.99 - ETA: 5:11 - loss: 0.0014 - accuracy: 0.99 - ETA: 5:00 - loss: 0.0013 - accuracy: 0.99 - ETA: 4:49 - loss: 0.0013 - accuracy: 0.99 - ETA: 4:37 - loss: 0.0013 - accuracy: 0.99 - ETA: 4:26 - loss: 0.0013 - accuracy: 0.99 - ETA: 4:15 - loss: 0.0013 - accuracy: 0.99 - ETA: 4:03 - loss: 0.0013 - accuracy: 0.99 - ETA: 3:52 - loss: 0.0013 - accuracy: 0.99 - ETA: 3:41 - loss: 0.0013 - accuracy: 0.99 - ETA: 3:30 - loss: 0.0013 - accuracy: 0.99 - ETA: 3:19 - loss: 0.0013 - accuracy: 0.99 - ETA: 3:08 - loss: 0.0013 - accuracy: 0.99 - ETA: 2:57 - loss: 0.0013 - accuracy: 0.99 - ETA: 2:45 - loss: 0.0012 - accuracy: 0.99 - ETA: 2:34 - loss: 0.0012 - accuracy: 0.99 - ETA: 2:24 - loss: 0.0012 - accuracy: 0.99 - ETA: 2:13 - loss: 0.0012 - accuracy: 0.99 - ETA: 2:02 - loss: 0.0012 - accuracy: 0.99 - ETA: 1:51 - loss: 0.0012 - accuracy: 0.99 - ETA: 1:40 - loss: 0.0012 - accuracy: 0.99 - ETA: 1:29 - loss: 0.0012 - accuracy: 0.99 - ETA: 1:18 - loss: 0.0012 - accuracy: 0.99 - ETA: 1:07 - loss: 0.0012 - accuracy: 0.99 - ETA: 56s - loss: 0.0012 - accuracy: 0.9983 - ETA: 45s - loss: 0.0013 - accuracy: 0.998 - ETA: 35s - loss: 0.0012 - accuracy: 0.998 - ETA: 24s - loss: 0.0012 - accuracy: 0.998 - ETA: 13s - loss: 0.0012 - accuracy: 0.998 - ETA: 2s - loss: 0.0012 - accuracy: 0.998 - 1364s 715ms/step - loss: 0.0012 - accuracy: 0.9983 - val_loss: 0.0127 - val_accuracy: 0.9855\n",
      "Epoch 11/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1908/1908 [==============================] - ETA: 18:44 - loss: 5.5335e-04 - accuracy: 1.000 - ETA: 19:09 - loss: 3.8482e-04 - accuracy: 1.000 - ETA: 19:05 - loss: 3.5189e-04 - accuracy: 1.000 - ETA: 19:06 - loss: 3.3651e-04 - accuracy: 1.000 - ETA: 18:58 - loss: 2.8890e-04 - accuracy: 1.000 - ETA: 18:48 - loss: 2.5023e-04 - accuracy: 1.000 - ETA: 18:45 - loss: 2.2194e-04 - accuracy: 1.000 - ETA: 18:35 - loss: 2.0318e-04 - accuracy: 1.000 - ETA: 18:21 - loss: 1.8475e-04 - accuracy: 1.000 - ETA: 18:05 - loss: 2.3118e-04 - accuracy: 1.000 - ETA: 17:55 - loss: 3.8689e-04 - accuracy: 0.999 - ETA: 17:45 - loss: 3.5579e-04 - accuracy: 0.999 - ETA: 17:32 - loss: 4.4792e-04 - accuracy: 0.999 - ETA: 17:21 - loss: 4.2062e-04 - accuracy: 0.999 - ETA: 17:12 - loss: 3.9653e-04 - accuracy: 0.999 - ETA: 17:01 - loss: 3.7308e-04 - accuracy: 0.999 - ETA: 16:54 - loss: 7.4547e-04 - accuracy: 0.998 - ETA: 16:45 - loss: 7.0492e-04 - accuracy: 0.998 - ETA: 16:37 - loss: 6.8700e-04 - accuracy: 0.998 - ETA: 16:26 - loss: 6.5475e-04 - accuracy: 0.998 - ETA: 16:22 - loss: 6.2450e-04 - accuracy: 0.999 - ETA: 16:12 - loss: 5.9661e-04 - accuracy: 0.999 - ETA: 16:02 - loss: 5.7209e-04 - accuracy: 0.999 - ETA: 15:51 - loss: 5.4852e-04 - accuracy: 0.999 - ETA: 15:40 - loss: 5.2895e-04 - accuracy: 0.999 - ETA: 15:32 - loss: 5.2677e-04 - accuracy: 0.999 - ETA: 15:23 - loss: 7.0811e-04 - accuracy: 0.999 - ETA: 15:12 - loss: 6.8487e-04 - accuracy: 0.999 - ETA: 15:02 - loss: 7.0793e-04 - accuracy: 0.999 - ETA: 14:51 - loss: 6.8587e-04 - accuracy: 0.999 - ETA: 14:41 - loss: 6.9647e-04 - accuracy: 0.998 - ETA: 14:29 - loss: 6.8686e-04 - accuracy: 0.998 - ETA: 14:19 - loss: 6.6771e-04 - accuracy: 0.999 - ETA: 14:09 - loss: 6.5017e-04 - accuracy: 0.999 - ETA: 13:58 - loss: 6.3213e-04 - accuracy: 0.999 - ETA: 13:47 - loss: 6.2687e-04 - accuracy: 0.999 - ETA: 13:36 - loss: 6.9880e-04 - accuracy: 0.999 - ETA: 13:26 - loss: 6.8242e-04 - accuracy: 0.999 - ETA: 13:17 - loss: 6.6726e-04 - accuracy: 0.999 - ETA: 13:07 - loss: 6.6359e-04 - accuracy: 0.999 - ETA: 12:58 - loss: 6.5355e-04 - accuracy: 0.999 - ETA: 12:48 - loss: 6.7317e-04 - accuracy: 0.999 - ETA: 12:39 - loss: 7.5773e-04 - accuracy: 0.999 - ETA: 12:28 - loss: 7.7899e-04 - accuracy: 0.998 - ETA: 12:17 - loss: 7.6336e-04 - accuracy: 0.999 - ETA: 12:07 - loss: 7.4816e-04 - accuracy: 0.999 - ETA: 11:58 - loss: 7.3246e-04 - accuracy: 0.999 - ETA: 11:48 - loss: 8.0552e-04 - accuracy: 0.998 - ETA: 11:38 - loss: 7.9092e-04 - accuracy: 0.999 - ETA: 11:29 - loss: 7.7741e-04 - accuracy: 0.999 - ETA: 11:19 - loss: 7.6498e-04 - accuracy: 0.999 - ETA: 11:09 - loss: 7.5070e-04 - accuracy: 0.999 - ETA: 10:59 - loss: 7.3895e-04 - accuracy: 0.999 - ETA: 10:49 - loss: 7.2992e-04 - accuracy: 0.999 - ETA: 10:38 - loss: 7.3477e-04 - accuracy: 0.999 - ETA: 10:28 - loss: 7.2417e-04 - accuracy: 0.999 - ETA: 10:18 - loss: 7.1312e-04 - accuracy: 0.999 - ETA: 10:08 - loss: 7.1517e-04 - accuracy: 0.999 - ETA: 9:58 - loss: 7.1343e-04 - accuracy: 0.999 - ETA: 9:48 - loss: 7.0283e-04 - accuracy: 0.99 - ETA: 9:39 - loss: 6.9752e-04 - accuracy: 0.99 - ETA: 9:29 - loss: 7.0354e-04 - accuracy: 0.99 - ETA: 9:19 - loss: 6.9277e-04 - accuracy: 0.99 - ETA: 9:09 - loss: 6.8304e-04 - accuracy: 0.99 - ETA: 9:00 - loss: 6.7296e-04 - accuracy: 0.99 - ETA: 8:50 - loss: 6.6293e-04 - accuracy: 0.99 - ETA: 8:40 - loss: 7.4667e-04 - accuracy: 0.99 - ETA: 8:30 - loss: 7.3583e-04 - accuracy: 0.99 - ETA: 8:20 - loss: 7.4286e-04 - accuracy: 0.99 - ETA: 8:10 - loss: 7.3259e-04 - accuracy: 0.99 - ETA: 8:01 - loss: 7.2242e-04 - accuracy: 0.99 - ETA: 7:51 - loss: 7.1262e-04 - accuracy: 0.99 - ETA: 7:41 - loss: 7.0569e-04 - accuracy: 0.99 - ETA: 7:31 - loss: 6.9721e-04 - accuracy: 0.99 - ETA: 7:21 - loss: 6.9078e-04 - accuracy: 0.99 - ETA: 7:10 - loss: 6.8193e-04 - accuracy: 0.99 - ETA: 7:01 - loss: 6.7417e-04 - accuracy: 0.99 - ETA: 6:51 - loss: 6.6792e-04 - accuracy: 0.99 - ETA: 6:41 - loss: 6.5983e-04 - accuracy: 0.99 - ETA: 6:31 - loss: 6.5211e-04 - accuracy: 0.99 - ETA: 6:21 - loss: 6.4894e-04 - accuracy: 0.99 - ETA: 6:11 - loss: 6.4193e-04 - accuracy: 0.99 - ETA: 6:01 - loss: 6.3494e-04 - accuracy: 0.99 - ETA: 5:51 - loss: 6.2863e-04 - accuracy: 0.99 - ETA: 5:42 - loss: 6.6336e-04 - accuracy: 0.99 - ETA: 5:32 - loss: 6.5862e-04 - accuracy: 0.99 - ETA: 5:23 - loss: 6.5345e-04 - accuracy: 0.99 - ETA: 5:13 - loss: 6.5014e-04 - accuracy: 0.99 - ETA: 5:04 - loss: 6.4343e-04 - accuracy: 0.99 - ETA: 4:54 - loss: 6.3731e-04 - accuracy: 0.99 - ETA: 4:44 - loss: 6.3202e-04 - accuracy: 0.99 - ETA: 4:34 - loss: 6.2548e-04 - accuracy: 0.99 - ETA: 4:24 - loss: 6.2187e-04 - accuracy: 0.99 - ETA: 4:14 - loss: 6.1685e-04 - accuracy: 0.99 - ETA: 4:04 - loss: 6.6983e-04 - accuracy: 0.99 - ETA: 3:54 - loss: 6.6563e-04 - accuracy: 0.99 - ETA: 3:44 - loss: 6.6199e-04 - accuracy: 0.99 - ETA: 3:34 - loss: 6.6231e-04 - accuracy: 0.99 - ETA: 3:24 - loss: 6.5854e-04 - accuracy: 0.99 - ETA: 3:14 - loss: 6.5217e-04 - accuracy: 0.99 - ETA: 3:05 - loss: 6.4708e-04 - accuracy: 0.99 - ETA: 2:54 - loss: 6.4130e-04 - accuracy: 0.99 - ETA: 2:44 - loss: 6.3610e-04 - accuracy: 0.99 - ETA: 2:34 - loss: 6.3152e-04 - accuracy: 0.99 - ETA: 2:24 - loss: 6.3038e-04 - accuracy: 0.99 - ETA: 2:14 - loss: 6.2502e-04 - accuracy: 0.99 - ETA: 2:04 - loss: 6.1942e-04 - accuracy: 0.99 - ETA: 1:54 - loss: 6.2046e-04 - accuracy: 0.99 - ETA: 1:44 - loss: 6.1818e-04 - accuracy: 0.99 - ETA: 1:34 - loss: 6.1296e-04 - accuracy: 0.99 - ETA: 1:24 - loss: 6.1097e-04 - accuracy: 0.99 - ETA: 1:13 - loss: 6.0563e-04 - accuracy: 0.99 - ETA: 1:03 - loss: 6.0142e-04 - accuracy: 0.99 - ETA: 53s - loss: 5.9624e-04 - accuracy: 0.9992 - ETA: 43s - loss: 5.9127e-04 - accuracy: 0.999 - ETA: 33s - loss: 6.1553e-04 - accuracy: 0.999 - ETA: 23s - loss: 6.1052e-04 - accuracy: 0.999 - ETA: 12s - loss: 6.3224e-04 - accuracy: 0.999 - ETA: 2s - loss: 6.2714e-04 - accuracy: 0.999 - 1311s 687ms/step - loss: 6.2591e-04 - accuracy: 0.9992 - val_loss: 0.0130 - val_accuracy: 0.9864\n",
      "Epoch 12/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1908/1908 [==============================] - ETA: 20:17 - loss: 3.1492e-04 - accuracy: 1.000 - ETA: 20:15 - loss: 2.5423e-04 - accuracy: 1.000 - ETA: 20:20 - loss: 2.5010e-04 - accuracy: 1.000 - ETA: 20:08 - loss: 2.5390e-04 - accuracy: 1.000 - ETA: 19:46 - loss: 2.4742e-04 - accuracy: 1.000 - ETA: 19:38 - loss: 0.0015 - accuracy: 0.9986    - ETA: 19:24 - loss: 0.0013 - accuracy: 0.998 - ETA: 19:05 - loss: 0.0011 - accuracy: 0.999 - ETA: 18:51 - loss: 0.0010 - accuracy: 0.999 - ETA: 18:38 - loss: 9.4906e-04 - accuracy: 0.999 - ETA: 18:24 - loss: 8.7924e-04 - accuracy: 0.999 - ETA: 18:12 - loss: 8.2264e-04 - accuracy: 0.999 - ETA: 18:02 - loss: 7.6943e-04 - accuracy: 0.999 - ETA: 17:50 - loss: 7.5021e-04 - accuracy: 0.999 - ETA: 17:45 - loss: 7.3000e-04 - accuracy: 0.999 - ETA: 17:36 - loss: 6.9318e-04 - accuracy: 0.999 - ETA: 17:24 - loss: 6.5821e-04 - accuracy: 0.999 - ETA: 17:13 - loss: 6.8877e-04 - accuracy: 0.999 - ETA: 17:02 - loss: 6.5697e-04 - accuracy: 0.999 - ETA: 16:49 - loss: 6.3170e-04 - accuracy: 0.999 - ETA: 16:38 - loss: 6.2146e-04 - accuracy: 0.999 - ETA: 16:29 - loss: 6.5677e-04 - accuracy: 0.999 - ETA: 16:18 - loss: 6.4292e-04 - accuracy: 0.999 - ETA: 16:06 - loss: 6.1730e-04 - accuracy: 0.999 - ETA: 15:55 - loss: 5.9419e-04 - accuracy: 0.999 - ETA: 15:44 - loss: 5.7399e-04 - accuracy: 0.999 - ETA: 15:35 - loss: 5.5414e-04 - accuracy: 0.999 - ETA: 15:26 - loss: 5.6031e-04 - accuracy: 0.999 - ETA: 15:15 - loss: 5.5412e-04 - accuracy: 0.999 - ETA: 15:06 - loss: 5.3603e-04 - accuracy: 0.999 - ETA: 14:56 - loss: 5.1926e-04 - accuracy: 0.999 - ETA: 14:45 - loss: 5.0908e-04 - accuracy: 0.999 - ETA: 14:34 - loss: 4.9503e-04 - accuracy: 0.999 - ETA: 14:25 - loss: 4.8464e-04 - accuracy: 0.999 - ETA: 14:15 - loss: 4.7177e-04 - accuracy: 0.999 - ETA: 14:05 - loss: 4.6037e-04 - accuracy: 0.999 - ETA: 13:55 - loss: 4.4956e-04 - accuracy: 0.999 - ETA: 13:45 - loss: 4.3786e-04 - accuracy: 0.999 - ETA: 13:35 - loss: 4.2724e-04 - accuracy: 0.999 - ETA: 13:26 - loss: 4.1681e-04 - accuracy: 0.999 - ETA: 13:15 - loss: 4.3368e-04 - accuracy: 0.999 - ETA: 13:05 - loss: 4.2372e-04 - accuracy: 0.999 - ETA: 12:54 - loss: 4.1401e-04 - accuracy: 0.999 - ETA: 12:43 - loss: 4.2456e-04 - accuracy: 0.999 - ETA: 12:33 - loss: 4.2397e-04 - accuracy: 0.999 - ETA: 12:22 - loss: 4.1492e-04 - accuracy: 0.999 - ETA: 12:12 - loss: 4.0624e-04 - accuracy: 0.999 - ETA: 12:02 - loss: 3.9812e-04 - accuracy: 0.999 - ETA: 11:51 - loss: 3.9184e-04 - accuracy: 0.999 - ETA: 11:41 - loss: 3.8413e-04 - accuracy: 0.999 - ETA: 11:31 - loss: 3.7677e-04 - accuracy: 0.999 - ETA: 11:22 - loss: 3.6961e-04 - accuracy: 0.999 - ETA: 11:12 - loss: 3.6276e-04 - accuracy: 0.999 - ETA: 11:02 - loss: 3.9166e-04 - accuracy: 0.999 - ETA: 10:52 - loss: 5.4683e-04 - accuracy: 0.999 - ETA: 10:41 - loss: 5.3710e-04 - accuracy: 0.999 - ETA: 10:31 - loss: 5.2823e-04 - accuracy: 0.999 - ETA: 10:21 - loss: 5.1969e-04 - accuracy: 0.999 - ETA: 10:11 - loss: 5.1125e-04 - accuracy: 0.999 - ETA: 10:01 - loss: 5.0342e-04 - accuracy: 0.999 - ETA: 9:51 - loss: 4.9582e-04 - accuracy: 0.999 - ETA: 9:41 - loss: 4.8864e-04 - accuracy: 0.99 - ETA: 9:30 - loss: 6.2981e-04 - accuracy: 0.99 - ETA: 9:20 - loss: 6.2728e-04 - accuracy: 0.99 - ETA: 9:10 - loss: 6.1796e-04 - accuracy: 0.99 - ETA: 8:59 - loss: 6.1125e-04 - accuracy: 0.99 - ETA: 8:49 - loss: 6.0253e-04 - accuracy: 0.99 - ETA: 8:39 - loss: 5.9394e-04 - accuracy: 0.99 - ETA: 8:28 - loss: 5.8558e-04 - accuracy: 0.99 - ETA: 8:18 - loss: 5.7765e-04 - accuracy: 0.99 - ETA: 8:08 - loss: 5.7059e-04 - accuracy: 0.99 - ETA: 7:58 - loss: 5.6305e-04 - accuracy: 0.99 - ETA: 7:48 - loss: 5.5572e-04 - accuracy: 0.99 - ETA: 7:38 - loss: 5.4857e-04 - accuracy: 0.99 - ETA: 7:28 - loss: 5.4474e-04 - accuracy: 0.99 - ETA: 7:18 - loss: 5.3830e-04 - accuracy: 0.99 - ETA: 7:08 - loss: 5.3221e-04 - accuracy: 0.99 - ETA: 6:57 - loss: 5.2572e-04 - accuracy: 0.99 - ETA: 6:47 - loss: 5.1928e-04 - accuracy: 0.99 - ETA: 6:37 - loss: 5.1416e-04 - accuracy: 0.99 - ETA: 6:27 - loss: 5.0829e-04 - accuracy: 0.99 - ETA: 6:17 - loss: 5.0231e-04 - accuracy: 0.99 - ETA: 6:06 - loss: 4.9772e-04 - accuracy: 0.99 - ETA: 5:56 - loss: 4.9222e-04 - accuracy: 0.99 - ETA: 5:46 - loss: 4.9981e-04 - accuracy: 0.99 - ETA: 5:36 - loss: 4.9812e-04 - accuracy: 0.99 - ETA: 5:26 - loss: 4.9328e-04 - accuracy: 0.99 - ETA: 5:15 - loss: 4.8780e-04 - accuracy: 0.99 - ETA: 5:05 - loss: 4.8259e-04 - accuracy: 0.99 - ETA: 4:55 - loss: 4.8066e-04 - accuracy: 0.99 - ETA: 4:45 - loss: 4.7551e-04 - accuracy: 0.99 - ETA: 4:35 - loss: 4.7177e-04 - accuracy: 0.99 - ETA: 4:25 - loss: 4.6786e-04 - accuracy: 0.99 - ETA: 4:15 - loss: 4.6410e-04 - accuracy: 0.99 - ETA: 4:05 - loss: 4.6028e-04 - accuracy: 0.99 - ETA: 3:55 - loss: 4.5623e-04 - accuracy: 0.99 - ETA: 3:45 - loss: 4.5292e-04 - accuracy: 0.99 - ETA: 3:35 - loss: 4.4852e-04 - accuracy: 0.99 - ETA: 3:25 - loss: 4.4472e-04 - accuracy: 0.99 - ETA: 3:15 - loss: 4.4053e-04 - accuracy: 0.99 - ETA: 3:04 - loss: 4.3641e-04 - accuracy: 0.99 - ETA: 2:54 - loss: 4.3298e-04 - accuracy: 0.99 - ETA: 2:44 - loss: 4.2980e-04 - accuracy: 0.99 - ETA: 2:34 - loss: 4.2644e-04 - accuracy: 0.99 - ETA: 2:24 - loss: 4.2262e-04 - accuracy: 0.99 - ETA: 2:14 - loss: 4.1916e-04 - accuracy: 0.99 - ETA: 2:04 - loss: 4.1556e-04 - accuracy: 0.99 - ETA: 1:53 - loss: 4.1185e-04 - accuracy: 0.99 - ETA: 1:43 - loss: 4.0820e-04 - accuracy: 0.99 - ETA: 1:33 - loss: 4.0457e-04 - accuracy: 0.99 - ETA: 1:23 - loss: 4.0145e-04 - accuracy: 0.99 - ETA: 1:13 - loss: 3.9827e-04 - accuracy: 0.99 - ETA: 1:03 - loss: 3.9485e-04 - accuracy: 0.99 - ETA: 53s - loss: 3.9144e-04 - accuracy: 0.9996 - ETA: 43s - loss: 3.8811e-04 - accuracy: 0.999 - ETA: 32s - loss: 3.9040e-04 - accuracy: 0.999 - ETA: 22s - loss: 3.8804e-04 - accuracy: 0.999 - ETA: 12s - loss: 3.8482e-04 - accuracy: 0.999 - ETA: 2s - loss: 3.8176e-04 - accuracy: 0.999 - 1292s 677ms/step - loss: 3.8097e-04 - accuracy: 0.9995 - val_loss: 0.0131 - val_accuracy: 0.9877\n",
      "Epoch 13/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1908/1908 [==============================] - ETA: 20:31 - loss: 0.0017 - accuracy: 0.995 - ETA: 19:38 - loss: 8.5288e-04 - accuracy: 0.997 - ETA: 19:27 - loss: 5.6989e-04 - accuracy: 0.998 - ETA: 19:15 - loss: 4.4696e-04 - accuracy: 0.998 - ETA: 19:07 - loss: 3.6096e-04 - accuracy: 0.999 - ETA: 19:08 - loss: 3.0248e-04 - accuracy: 0.999 - ETA: 18:56 - loss: 2.9447e-04 - accuracy: 0.999 - ETA: 18:49 - loss: 2.5825e-04 - accuracy: 0.999 - ETA: 18:38 - loss: 2.5202e-04 - accuracy: 0.999 - ETA: 18:27 - loss: 2.2715e-04 - accuracy: 0.999 - ETA: 18:14 - loss: 2.0701e-04 - accuracy: 0.999 - ETA: 18:01 - loss: 1.9065e-04 - accuracy: 0.999 - ETA: 17:53 - loss: 1.8051e-04 - accuracy: 0.999 - ETA: 17:40 - loss: 1.6792e-04 - accuracy: 0.999 - ETA: 17:33 - loss: 1.5708e-04 - accuracy: 0.999 - ETA: 17:28 - loss: 1.4971e-04 - accuracy: 0.999 - ETA: 17:23 - loss: 1.4215e-04 - accuracy: 0.999 - ETA: 17:16 - loss: 1.3472e-04 - accuracy: 0.999 - ETA: 17:07 - loss: 1.3171e-04 - accuracy: 0.999 - ETA: 17:00 - loss: 1.2599e-04 - accuracy: 0.999 - ETA: 16:50 - loss: 1.2012e-04 - accuracy: 0.999 - ETA: 16:40 - loss: 1.1479e-04 - accuracy: 0.999 - ETA: 16:30 - loss: 1.1000e-04 - accuracy: 0.999 - ETA: 16:21 - loss: 1.0564e-04 - accuracy: 0.999 - ETA: 16:11 - loss: 1.0155e-04 - accuracy: 0.999 - ETA: 16:01 - loss: 9.8185e-05 - accuracy: 0.999 - ETA: 15:49 - loss: 9.4671e-05 - accuracy: 0.999 - ETA: 15:37 - loss: 9.1462e-05 - accuracy: 0.999 - ETA: 15:26 - loss: 8.8459e-05 - accuracy: 0.999 - ETA: 15:14 - loss: 8.5618e-05 - accuracy: 0.999 - ETA: 15:03 - loss: 8.2918e-05 - accuracy: 0.999 - ETA: 14:52 - loss: 8.0434e-05 - accuracy: 0.999 - ETA: 14:41 - loss: 7.8191e-05 - accuracy: 0.999 - ETA: 14:31 - loss: 7.6191e-05 - accuracy: 0.999 - ETA: 14:21 - loss: 7.4123e-05 - accuracy: 0.999 - ETA: 14:11 - loss: 7.2110e-05 - accuracy: 0.999 - ETA: 14:01 - loss: 7.0225e-05 - accuracy: 0.999 - ETA: 13:50 - loss: 6.8587e-05 - accuracy: 0.999 - ETA: 13:39 - loss: 6.6874e-05 - accuracy: 0.999 - ETA: 13:29 - loss: 6.5272e-05 - accuracy: 0.999 - ETA: 13:18 - loss: 6.4976e-05 - accuracy: 0.999 - ETA: 13:08 - loss: 6.3491e-05 - accuracy: 0.999 - ETA: 12:58 - loss: 6.2067e-05 - accuracy: 0.999 - ETA: 12:48 - loss: 6.1116e-05 - accuracy: 0.999 - ETA: 12:37 - loss: 6.1885e-05 - accuracy: 0.999 - ETA: 12:27 - loss: 6.0693e-05 - accuracy: 0.999 - ETA: 12:16 - loss: 5.9459e-05 - accuracy: 0.999 - ETA: 12:07 - loss: 5.9052e-05 - accuracy: 0.999 - ETA: 11:56 - loss: 5.8199e-05 - accuracy: 0.999 - ETA: 11:47 - loss: 5.7101e-05 - accuracy: 0.999 - ETA: 11:36 - loss: 5.6064e-05 - accuracy: 0.999 - ETA: 11:26 - loss: 5.5012e-05 - accuracy: 0.999 - ETA: 11:15 - loss: 5.4277e-05 - accuracy: 0.999 - ETA: 11:06 - loss: 5.3600e-05 - accuracy: 0.999 - ETA: 10:57 - loss: 5.2715e-05 - accuracy: 0.999 - ETA: 10:47 - loss: 5.6545e-05 - accuracy: 0.999 - ETA: 10:38 - loss: 5.5582e-05 - accuracy: 0.999 - ETA: 10:28 - loss: 5.4680e-05 - accuracy: 0.999 - ETA: 10:18 - loss: 5.3877e-05 - accuracy: 0.999 - ETA: 10:09 - loss: 5.3000e-05 - accuracy: 0.999 - ETA: 9:59 - loss: 5.2292e-05 - accuracy: 0.999 - ETA: 9:48 - loss: 5.2085e-05 - accuracy: 0.99 - ETA: 9:38 - loss: 5.1310e-05 - accuracy: 0.99 - ETA: 9:28 - loss: 5.0618e-05 - accuracy: 0.99 - ETA: 9:18 - loss: 5.0250e-05 - accuracy: 0.99 - ETA: 9:07 - loss: 4.9965e-05 - accuracy: 0.99 - ETA: 8:57 - loss: 1.1392e-04 - accuracy: 0.99 - ETA: 8:47 - loss: 1.1227e-04 - accuracy: 0.99 - ETA: 8:37 - loss: 1.1066e-04 - accuracy: 0.99 - ETA: 8:26 - loss: 1.0916e-04 - accuracy: 0.99 - ETA: 8:16 - loss: 1.0783e-04 - accuracy: 0.99 - ETA: 8:05 - loss: 1.0635e-04 - accuracy: 0.99 - ETA: 7:55 - loss: 1.0503e-04 - accuracy: 0.99 - ETA: 7:44 - loss: 1.0364e-04 - accuracy: 0.99 - ETA: 7:34 - loss: 1.0250e-04 - accuracy: 0.99 - ETA: 7:23 - loss: 1.0120e-04 - accuracy: 0.99 - ETA: 7:13 - loss: 1.0580e-04 - accuracy: 0.99 - ETA: 7:02 - loss: 1.0483e-04 - accuracy: 0.99 - ETA: 6:52 - loss: 1.0362e-04 - accuracy: 0.99 - ETA: 6:42 - loss: 1.0237e-04 - accuracy: 0.99 - ETA: 6:32 - loss: 1.0118e-04 - accuracy: 0.99 - ETA: 6:22 - loss: 1.0002e-04 - accuracy: 0.99 - ETA: 6:11 - loss: 1.0366e-04 - accuracy: 0.99 - ETA: 6:01 - loss: 1.0251e-04 - accuracy: 0.99 - ETA: 5:51 - loss: 1.0136e-04 - accuracy: 0.99 - ETA: 5:41 - loss: 1.0808e-04 - accuracy: 0.99 - ETA: 5:30 - loss: 1.0703e-04 - accuracy: 0.99 - ETA: 5:20 - loss: 1.0587e-04 - accuracy: 0.99 - ETA: 5:10 - loss: 1.0483e-04 - accuracy: 0.99 - ETA: 4:59 - loss: 1.0407e-04 - accuracy: 0.99 - ETA: 4:49 - loss: 1.0294e-04 - accuracy: 0.99 - ETA: 4:39 - loss: 1.0193e-04 - accuracy: 0.99 - ETA: 4:28 - loss: 1.0148e-04 - accuracy: 0.99 - ETA: 4:18 - loss: 1.0051e-04 - accuracy: 0.99 - ETA: 4:08 - loss: 9.9704e-05 - accuracy: 0.99 - ETA: 3:57 - loss: 1.0256e-04 - accuracy: 0.99 - ETA: 3:47 - loss: 1.0161e-04 - accuracy: 0.99 - ETA: 3:37 - loss: 1.0067e-04 - accuracy: 0.99 - ETA: 3:27 - loss: 9.9730e-05 - accuracy: 0.99 - ETA: 3:16 - loss: 9.8778e-05 - accuracy: 0.99 - ETA: 3:06 - loss: 9.7907e-05 - accuracy: 0.99 - ETA: 2:56 - loss: 9.7017e-05 - accuracy: 0.99 - ETA: 2:46 - loss: 9.6177e-05 - accuracy: 0.99 - ETA: 2:35 - loss: 9.5674e-05 - accuracy: 0.99 - ETA: 2:25 - loss: 9.4820e-05 - accuracy: 0.99 - ETA: 2:15 - loss: 9.3984e-05 - accuracy: 0.99 - ETA: 2:05 - loss: 9.3405e-05 - accuracy: 0.99 - ETA: 1:55 - loss: 9.4273e-05 - accuracy: 0.99 - ETA: 1:44 - loss: 9.3451e-05 - accuracy: 0.99 - ETA: 1:34 - loss: 9.5034e-05 - accuracy: 0.99 - ETA: 1:24 - loss: 9.4230e-05 - accuracy: 0.99 - ETA: 1:14 - loss: 9.3866e-05 - accuracy: 0.99 - ETA: 1:03 - loss: 9.3066e-05 - accuracy: 0.99 - ETA: 53s - loss: 9.7339e-05 - accuracy: 0.9999 - ETA: 43s - loss: 9.6512e-05 - accuracy: 0.999 - ETA: 33s - loss: 9.5704e-05 - accuracy: 0.999 - ETA: 22s - loss: 9.5058e-05 - accuracy: 0.999 - ETA: 12s - loss: 9.4299e-05 - accuracy: 0.999 - ETA: 2s - loss: 9.3664e-05 - accuracy: 0.999 - 1303s 683ms/step - loss: 9.3490e-05 - accuracy: 0.9999 - val_loss: 0.0145 - val_accuracy: 0.9867\n",
      "Epoch 14/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1908/1908 [==============================] - ETA: 20:50 - loss: 3.3358e-06 - accuracy: 1.000 - ETA: 20:14 - loss: 3.4853e-06 - accuracy: 1.000 - ETA: 19:54 - loss: 4.3208e-06 - accuracy: 1.000 - ETA: 19:26 - loss: 4.4639e-06 - accuracy: 1.000 - ETA: 19:11 - loss: 3.8439e-06 - accuracy: 1.000 - ETA: 18:58 - loss: 4.3608e-06 - accuracy: 1.000 - ETA: 18:46 - loss: 4.2878e-06 - accuracy: 1.000 - ETA: 18:37 - loss: 5.8916e-06 - accuracy: 1.000 - ETA: 18:27 - loss: 5.4983e-06 - accuracy: 1.000 - ETA: 18:21 - loss: 7.3846e-06 - accuracy: 1.000 - ETA: 18:12 - loss: 6.9893e-06 - accuracy: 1.000 - ETA: 18:02 - loss: 7.6154e-06 - accuracy: 1.000 - ETA: 17:52 - loss: 7.4995e-06 - accuracy: 1.000 - ETA: 17:43 - loss: 7.9437e-06 - accuracy: 1.000 - ETA: 17:32 - loss: 7.9887e-06 - accuracy: 1.000 - ETA: 17:23 - loss: 7.8014e-06 - accuracy: 1.000 - ETA: 17:13 - loss: 9.6563e-06 - accuracy: 1.000 - ETA: 17:04 - loss: 9.3163e-06 - accuracy: 1.000 - ETA: 16:53 - loss: 9.0651e-06 - accuracy: 1.000 - ETA: 16:44 - loss: 1.1383e-05 - accuracy: 1.000 - ETA: 16:34 - loss: 1.1016e-05 - accuracy: 1.000 - ETA: 16:24 - loss: 1.0673e-05 - accuracy: 1.000 - ETA: 16:13 - loss: 1.0416e-05 - accuracy: 1.000 - ETA: 16:02 - loss: 1.0375e-05 - accuracy: 1.000 - ETA: 15:50 - loss: 1.0339e-05 - accuracy: 1.000 - ETA: 15:41 - loss: 1.0018e-05 - accuracy: 1.000 - ETA: 15:30 - loss: 9.7217e-06 - accuracy: 1.000 - ETA: 15:19 - loss: 9.8842e-06 - accuracy: 1.000 - ETA: 15:09 - loss: 9.6321e-06 - accuracy: 1.000 - ETA: 14:59 - loss: 9.3563e-06 - accuracy: 1.000 - ETA: 14:49 - loss: 9.2599e-06 - accuracy: 1.000 - ETA: 14:41 - loss: 9.0225e-06 - accuracy: 1.000 - ETA: 14:31 - loss: 8.8476e-06 - accuracy: 1.000 - ETA: 14:21 - loss: 8.7582e-06 - accuracy: 1.000 - ETA: 14:11 - loss: 8.6582e-06 - accuracy: 1.000 - ETA: 14:01 - loss: 8.5462e-06 - accuracy: 1.000 - ETA: 13:51 - loss: 8.9913e-06 - accuracy: 1.000 - ETA: 13:41 - loss: 8.8433e-06 - accuracy: 1.000 - ETA: 13:31 - loss: 9.9920e-06 - accuracy: 1.000 - ETA: 13:22 - loss: 9.7812e-06 - accuracy: 1.000 - ETA: 13:11 - loss: 9.8304e-06 - accuracy: 1.000 - ETA: 13:01 - loss: 9.7305e-06 - accuracy: 1.000 - ETA: 12:51 - loss: 9.6860e-06 - accuracy: 1.000 - ETA: 12:41 - loss: 9.5740e-06 - accuracy: 1.000 - ETA: 12:30 - loss: 9.4042e-06 - accuracy: 1.000 - ETA: 12:21 - loss: 9.2200e-06 - accuracy: 1.000 - ETA: 12:10 - loss: 9.2841e-06 - accuracy: 1.000 - ETA: 11:59 - loss: 9.1269e-06 - accuracy: 1.000 - ETA: 11:49 - loss: 8.9881e-06 - accuracy: 1.000 - ETA: 11:39 - loss: 9.0724e-06 - accuracy: 1.000 - ETA: 11:28 - loss: 8.9237e-06 - accuracy: 1.000 - ETA: 11:18 - loss: 8.7739e-06 - accuracy: 1.000 - ETA: 11:08 - loss: 8.8311e-06 - accuracy: 1.000 - ETA: 10:58 - loss: 8.7013e-06 - accuracy: 1.000 - ETA: 10:48 - loss: 8.5651e-06 - accuracy: 1.000 - ETA: 10:38 - loss: 8.4389e-06 - accuracy: 1.000 - ETA: 10:28 - loss: 8.4940e-06 - accuracy: 1.000 - ETA: 10:18 - loss: 8.3835e-06 - accuracy: 1.000 - ETA: 10:08 - loss: 8.2551e-06 - accuracy: 1.000 - ETA: 9:58 - loss: 8.1406e-06 - accuracy: 1.000 - ETA: 9:47 - loss: 8.0238e-06 - accuracy: 1.00 - ETA: 9:37 - loss: 7.9204e-06 - accuracy: 1.00 - ETA: 9:27 - loss: 7.8230e-06 - accuracy: 1.00 - ETA: 9:17 - loss: 7.7823e-06 - accuracy: 1.00 - ETA: 9:07 - loss: 7.6819e-06 - accuracy: 1.00 - ETA: 8:56 - loss: 7.5856e-06 - accuracy: 1.00 - ETA: 8:46 - loss: 7.5710e-06 - accuracy: 1.00 - ETA: 8:36 - loss: 7.5055e-06 - accuracy: 1.00 - ETA: 8:26 - loss: 7.4256e-06 - accuracy: 1.00 - ETA: 8:16 - loss: 7.3406e-06 - accuracy: 1.00 - ETA: 8:06 - loss: 7.4142e-06 - accuracy: 1.00 - ETA: 7:56 - loss: 7.3341e-06 - accuracy: 1.00 - ETA: 7:46 - loss: 7.2879e-06 - accuracy: 1.00 - ETA: 7:36 - loss: 7.2063e-06 - accuracy: 1.00 - ETA: 7:26 - loss: 7.3443e-06 - accuracy: 1.00 - ETA: 7:16 - loss: 7.3102e-06 - accuracy: 1.00 - ETA: 7:06 - loss: 7.2246e-06 - accuracy: 1.00 - ETA: 6:56 - loss: 7.1952e-06 - accuracy: 1.00 - ETA: 6:46 - loss: 7.1338e-06 - accuracy: 1.00 - ETA: 6:36 - loss: 7.1013e-06 - accuracy: 1.00 - ETA: 6:26 - loss: 7.0787e-06 - accuracy: 1.00 - ETA: 6:16 - loss: 7.0295e-06 - accuracy: 1.00 - ETA: 6:06 - loss: 6.9551e-06 - accuracy: 1.00 - ETA: 5:56 - loss: 6.8874e-06 - accuracy: 1.00 - ETA: 5:45 - loss: 6.8242e-06 - accuracy: 1.00 - ETA: 5:35 - loss: 6.7521e-06 - accuracy: 1.00 - ETA: 5:25 - loss: 7.0266e-06 - accuracy: 1.00 - ETA: 5:15 - loss: 6.9562e-06 - accuracy: 1.00 - ETA: 5:05 - loss: 6.9077e-06 - accuracy: 1.00 - ETA: 4:55 - loss: 6.8859e-06 - accuracy: 1.00 - ETA: 4:45 - loss: 6.8206e-06 - accuracy: 1.00 - ETA: 4:35 - loss: 6.7693e-06 - accuracy: 1.00 - ETA: 4:24 - loss: 6.7038e-06 - accuracy: 1.00 - ETA: 4:14 - loss: 6.6545e-06 - accuracy: 1.00 - ETA: 4:04 - loss: 6.6047e-06 - accuracy: 1.00 - ETA: 3:54 - loss: 6.6801e-06 - accuracy: 1.00 - ETA: 3:44 - loss: 6.6693e-06 - accuracy: 1.00 - ETA: 3:34 - loss: 6.6261e-06 - accuracy: 1.00 - ETA: 3:24 - loss: 6.6148e-06 - accuracy: 1.00 - ETA: 3:14 - loss: 6.6818e-06 - accuracy: 1.00 - ETA: 3:04 - loss: 6.6255e-06 - accuracy: 1.00 - ETA: 2:54 - loss: 7.3645e-06 - accuracy: 1.00 - ETA: 2:44 - loss: 7.3045e-06 - accuracy: 1.00 - ETA: 2:33 - loss: 7.3009e-06 - accuracy: 1.00 - ETA: 2:23 - loss: 7.2586e-06 - accuracy: 1.00 - ETA: 2:13 - loss: 7.2103e-06 - accuracy: 1.00 - ETA: 2:03 - loss: 7.1564e-06 - accuracy: 1.00 - ETA: 1:53 - loss: 7.1166e-06 - accuracy: 1.00 - ETA: 1:43 - loss: 7.0590e-06 - accuracy: 1.00 - ETA: 1:33 - loss: 7.0229e-06 - accuracy: 1.00 - ETA: 1:23 - loss: 6.9642e-06 - accuracy: 1.00 - ETA: 1:13 - loss: 6.9192e-06 - accuracy: 1.00 - ETA: 1:03 - loss: 6.8636e-06 - accuracy: 1.00 - ETA: 53s - loss: 6.8236e-06 - accuracy: 1.0000 - ETA: 42s - loss: 6.7923e-06 - accuracy: 1.000 - ETA: 32s - loss: 6.7551e-06 - accuracy: 1.000 - ETA: 22s - loss: 6.7266e-06 - accuracy: 1.000 - ETA: 12s - loss: 6.6846e-06 - accuracy: 1.000 - ETA: 2s - loss: 6.6427e-06 - accuracy: 1.000 - 1287s 674ms/step - loss: 6.6317e-06 - accuracy: 1.0000 - val_loss: 0.0161 - val_accuracy: 0.9877\n",
      "Epoch 15/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1908/1908 [==============================] - ETA: 19:29 - loss: 1.4622e-06 - accuracy: 1.000 - ETA: 19:47 - loss: 1.0676e-06 - accuracy: 1.000 - ETA: 19:38 - loss: 1.4740e-06 - accuracy: 1.000 - ETA: 19:27 - loss: 1.4654e-06 - accuracy: 1.000 - ETA: 19:16 - loss: 1.7554e-06 - accuracy: 1.000 - ETA: 19:09 - loss: 1.6461e-06 - accuracy: 1.000 - ETA: 19:03 - loss: 1.5654e-06 - accuracy: 1.000 - ETA: 18:55 - loss: 1.4662e-06 - accuracy: 1.000 - ETA: 18:46 - loss: 1.3920e-06 - accuracy: 1.000 - ETA: 18:36 - loss: 1.5988e-06 - accuracy: 1.000 - ETA: 18:24 - loss: 1.6999e-06 - accuracy: 1.000 - ETA: 18:17 - loss: 1.9311e-06 - accuracy: 1.000 - ETA: 18:04 - loss: 1.8228e-06 - accuracy: 1.000 - ETA: 17:52 - loss: 1.8473e-06 - accuracy: 1.000 - ETA: 17:40 - loss: 1.7653e-06 - accuracy: 1.000 - ETA: 17:26 - loss: 1.7107e-06 - accuracy: 1.000 - ETA: 17:15 - loss: 1.8172e-06 - accuracy: 1.000 - ETA: 17:05 - loss: 1.7826e-06 - accuracy: 1.000 - ETA: 16:53 - loss: 1.7404e-06 - accuracy: 1.000 - ETA: 16:42 - loss: 1.7285e-06 - accuracy: 1.000 - ETA: 16:32 - loss: 1.6734e-06 - accuracy: 1.000 - ETA: 16:21 - loss: 1.6615e-06 - accuracy: 1.000 - ETA: 16:10 - loss: 1.8033e-06 - accuracy: 1.000 - ETA: 16:02 - loss: 1.7979e-06 - accuracy: 1.000 - ETA: 15:51 - loss: 1.7599e-06 - accuracy: 1.000 - ETA: 15:40 - loss: 1.7857e-06 - accuracy: 1.000 - ETA: 15:30 - loss: 1.8145e-06 - accuracy: 1.000 - ETA: 15:20 - loss: 1.8709e-06 - accuracy: 1.000 - ETA: 15:10 - loss: 1.8396e-06 - accuracy: 1.000 - ETA: 15:01 - loss: 1.7968e-06 - accuracy: 1.000 - ETA: 14:52 - loss: 1.7924e-06 - accuracy: 1.000 - ETA: 14:41 - loss: 1.7522e-06 - accuracy: 1.000 - ETA: 14:31 - loss: 1.7309e-06 - accuracy: 1.000 - ETA: 14:21 - loss: 1.7252e-06 - accuracy: 1.000 - ETA: 14:09 - loss: 1.7789e-06 - accuracy: 1.000 - ETA: 14:00 - loss: 1.7796e-06 - accuracy: 1.000 - ETA: 13:50 - loss: 1.7834e-06 - accuracy: 1.000 - ETA: 13:39 - loss: 1.7548e-06 - accuracy: 1.000 - ETA: 13:29 - loss: 1.7244e-06 - accuracy: 1.000 - ETA: 13:18 - loss: 1.7023e-06 - accuracy: 1.000 - ETA: 13:07 - loss: 1.7121e-06 - accuracy: 1.000 - ETA: 12:58 - loss: 1.7258e-06 - accuracy: 1.000 - ETA: 12:47 - loss: 1.7143e-06 - accuracy: 1.000 - ETA: 12:37 - loss: 1.6917e-06 - accuracy: 1.000 - ETA: 12:27 - loss: 1.6894e-06 - accuracy: 1.000 - ETA: 12:17 - loss: 1.7000e-06 - accuracy: 1.000 - ETA: 12:08 - loss: 1.6774e-06 - accuracy: 1.000 - ETA: 11:58 - loss: 1.6602e-06 - accuracy: 1.000 - ETA: 11:48 - loss: 1.6574e-06 - accuracy: 1.000 - ETA: 11:38 - loss: 1.6709e-06 - accuracy: 1.000 - ETA: 11:28 - loss: 1.6671e-06 - accuracy: 1.000 - ETA: 11:17 - loss: 1.6581e-06 - accuracy: 1.000 - ETA: 11:07 - loss: 1.6390e-06 - accuracy: 1.000 - ETA: 10:57 - loss: 1.6398e-06 - accuracy: 1.000 - ETA: 10:46 - loss: 1.6261e-06 - accuracy: 1.000 - ETA: 10:36 - loss: 1.6256e-06 - accuracy: 1.000 - ETA: 10:26 - loss: 1.6151e-06 - accuracy: 1.000 - ETA: 10:16 - loss: 1.6198e-06 - accuracy: 1.000 - ETA: 10:06 - loss: 1.6863e-06 - accuracy: 1.000 - ETA: 9:56 - loss: 1.6886e-06 - accuracy: 1.000 - ETA: 9:46 - loss: 1.6795e-06 - accuracy: 1.00 - ETA: 9:37 - loss: 1.7006e-06 - accuracy: 1.00 - ETA: 9:27 - loss: 1.6910e-06 - accuracy: 1.00 - ETA: 9:16 - loss: 1.7081e-06 - accuracy: 1.00 - ETA: 9:06 - loss: 1.6919e-06 - accuracy: 1.00 - ETA: 8:56 - loss: 1.6946e-06 - accuracy: 1.00 - ETA: 8:46 - loss: 1.6791e-06 - accuracy: 1.00 - ETA: 8:36 - loss: 1.6655e-06 - accuracy: 1.00 - ETA: 8:26 - loss: 1.7051e-06 - accuracy: 1.00 - ETA: 8:16 - loss: 1.7302e-06 - accuracy: 1.00 - ETA: 8:06 - loss: 1.7304e-06 - accuracy: 1.00 - ETA: 7:56 - loss: 1.7203e-06 - accuracy: 1.00 - ETA: 7:46 - loss: 1.7081e-06 - accuracy: 1.00 - ETA: 7:36 - loss: 1.6983e-06 - accuracy: 1.00 - ETA: 7:26 - loss: 1.6880e-06 - accuracy: 1.00 - ETA: 7:16 - loss: 1.6762e-06 - accuracy: 1.00 - ETA: 7:06 - loss: 1.6665e-06 - accuracy: 1.00 - ETA: 6:56 - loss: 1.6663e-06 - accuracy: 1.00 - ETA: 6:46 - loss: 1.6711e-06 - accuracy: 1.00 - ETA: 6:35 - loss: 1.6571e-06 - accuracy: 1.00 - ETA: 6:25 - loss: 1.6523e-06 - accuracy: 1.00 - ETA: 6:15 - loss: 1.6430e-06 - accuracy: 1.00 - ETA: 6:05 - loss: 1.6350e-06 - accuracy: 1.00 - ETA: 5:55 - loss: 1.6468e-06 - accuracy: 1.00 - ETA: 5:45 - loss: 1.6384e-06 - accuracy: 1.00 - ETA: 5:35 - loss: 1.6317e-06 - accuracy: 1.00 - ETA: 5:25 - loss: 1.6234e-06 - accuracy: 1.00 - ETA: 5:15 - loss: 1.6227e-06 - accuracy: 1.00 - ETA: 5:05 - loss: 1.6126e-06 - accuracy: 1.00 - ETA: 4:55 - loss: 1.6210e-06 - accuracy: 1.00 - ETA: 4:45 - loss: 1.6168e-06 - accuracy: 1.00 - ETA: 4:35 - loss: 1.6100e-06 - accuracy: 1.00 - ETA: 4:25 - loss: 1.6063e-06 - accuracy: 1.00 - ETA: 4:14 - loss: 1.6117e-06 - accuracy: 1.00 - ETA: 4:04 - loss: 1.6170e-06 - accuracy: 1.00 - ETA: 3:54 - loss: 1.6121e-06 - accuracy: 1.00 - ETA: 3:44 - loss: 1.6008e-06 - accuracy: 1.00 - ETA: 3:34 - loss: 1.5953e-06 - accuracy: 1.00 - ETA: 3:24 - loss: 1.5852e-06 - accuracy: 1.00 - ETA: 3:14 - loss: 1.5811e-06 - accuracy: 1.00 - ETA: 3:04 - loss: 1.5930e-06 - accuracy: 1.00 - ETA: 2:54 - loss: 1.5872e-06 - accuracy: 1.00 - ETA: 2:44 - loss: 1.5834e-06 - accuracy: 1.00 - ETA: 2:34 - loss: 1.5803e-06 - accuracy: 1.00 - ETA: 2:24 - loss: 1.5731e-06 - accuracy: 1.00 - ETA: 2:14 - loss: 1.5743e-06 - accuracy: 1.00 - ETA: 2:04 - loss: 1.5669e-06 - accuracy: 1.00 - ETA: 1:54 - loss: 1.5644e-06 - accuracy: 1.00 - ETA: 1:43 - loss: 1.5537e-06 - accuracy: 1.00 - ETA: 1:33 - loss: 1.5512e-06 - accuracy: 1.00 - ETA: 1:23 - loss: 1.5467e-06 - accuracy: 1.00 - ETA: 1:13 - loss: 1.5494e-06 - accuracy: 1.00 - ETA: 1:03 - loss: 1.5418e-06 - accuracy: 1.00 - ETA: 53s - loss: 1.5339e-06 - accuracy: 1.0000 - ETA: 43s - loss: 1.5274e-06 - accuracy: 1.000 - ETA: 32s - loss: 1.5258e-06 - accuracy: 1.000 - ETA: 22s - loss: 1.5379e-06 - accuracy: 1.000 - ETA: 12s - loss: 1.5381e-06 - accuracy: 1.000 - ETA: 2s - loss: 1.5553e-06 - accuracy: 1.000 - 1290s 676ms/step - loss: 1.5525e-06 - accuracy: 1.0000 - val_loss: 0.0165 - val_accuracy: 0.9877\n",
      "Epoch 16/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1908/1908 [==============================] - ETA: 20:53 - loss: 8.7821e-07 - accuracy: 1.000 - ETA: 20:04 - loss: 1.0467e-06 - accuracy: 1.000 - ETA: 19:43 - loss: 1.0344e-06 - accuracy: 1.000 - ETA: 19:30 - loss: 1.0146e-06 - accuracy: 1.000 - ETA: 19:13 - loss: 1.0084e-06 - accuracy: 1.000 - ETA: 19:02 - loss: 9.4368e-07 - accuracy: 1.000 - ETA: 18:47 - loss: 1.0100e-06 - accuracy: 1.000 - ETA: 18:33 - loss: 1.0106e-06 - accuracy: 1.000 - ETA: 18:34 - loss: 1.1522e-06 - accuracy: 1.000 - ETA: 18:19 - loss: 1.1438e-06 - accuracy: 1.000 - ETA: 18:08 - loss: 1.1378e-06 - accuracy: 1.000 - ETA: 17:56 - loss: 1.1178e-06 - accuracy: 1.000 - ETA: 17:45 - loss: 1.2599e-06 - accuracy: 1.000 - ETA: 17:38 - loss: 1.2084e-06 - accuracy: 1.000 - ETA: 17:31 - loss: 1.2097e-06 - accuracy: 1.000 - ETA: 17:20 - loss: 1.2133e-06 - accuracy: 1.000 - ETA: 17:10 - loss: 1.2759e-06 - accuracy: 1.000 - ETA: 17:00 - loss: 1.2961e-06 - accuracy: 1.000 - ETA: 16:49 - loss: 1.2608e-06 - accuracy: 1.000 - ETA: 16:40 - loss: 1.2622e-06 - accuracy: 1.000 - ETA: 16:32 - loss: 1.2527e-06 - accuracy: 1.000 - ETA: 16:23 - loss: 1.2311e-06 - accuracy: 1.000 - ETA: 16:13 - loss: 1.2650e-06 - accuracy: 1.000 - ETA: 16:03 - loss: 1.3155e-06 - accuracy: 1.000 - ETA: 15:52 - loss: 1.3241e-06 - accuracy: 1.000 - ETA: 15:42 - loss: 1.2987e-06 - accuracy: 1.000 - ETA: 15:32 - loss: 1.2767e-06 - accuracy: 1.000 - ETA: 15:23 - loss: 1.2482e-06 - accuracy: 1.000 - ETA: 15:12 - loss: 1.2458e-06 - accuracy: 1.000 - ETA: 15:02 - loss: 1.2507e-06 - accuracy: 1.000 - ETA: 14:51 - loss: 1.2437e-06 - accuracy: 1.000 - ETA: 14:40 - loss: 1.2255e-06 - accuracy: 1.000 - ETA: 14:31 - loss: 1.2021e-06 - accuracy: 1.000 - ETA: 14:21 - loss: 1.1861e-06 - accuracy: 1.000 - ETA: 14:15 - loss: 1.1948e-06 - accuracy: 1.000 - ETA: 14:07 - loss: 1.1756e-06 - accuracy: 1.000 - ETA: 13:59 - loss: 1.3140e-06 - accuracy: 1.000 - ETA: 13:51 - loss: 1.3074e-06 - accuracy: 1.000 - ETA: 13:43 - loss: 1.2926e-06 - accuracy: 1.000 - ETA: 13:34 - loss: 1.2770e-06 - accuracy: 1.000 - ETA: 13:25 - loss: 1.2714e-06 - accuracy: 1.000 - ETA: 13:14 - loss: 1.2961e-06 - accuracy: 1.000 - ETA: 13:03 - loss: 1.3076e-06 - accuracy: 1.000 - ETA: 12:53 - loss: 1.3100e-06 - accuracy: 1.000 - ETA: 12:42 - loss: 1.3003e-06 - accuracy: 1.000 - ETA: 12:31 - loss: 1.2864e-06 - accuracy: 1.000 - ETA: 12:21 - loss: 1.2989e-06 - accuracy: 1.000 - ETA: 12:09 - loss: 1.3034e-06 - accuracy: 1.000 - ETA: 11:59 - loss: 1.2890e-06 - accuracy: 1.000 - ETA: 11:48 - loss: 1.2799e-06 - accuracy: 1.000 - ETA: 11:38 - loss: 1.2788e-06 - accuracy: 1.000 - ETA: 11:28 - loss: 1.2657e-06 - accuracy: 1.000 - ETA: 11:18 - loss: 1.2674e-06 - accuracy: 1.000 - ETA: 11:07 - loss: 1.2871e-06 - accuracy: 1.000 - ETA: 10:56 - loss: 1.2693e-06 - accuracy: 1.000 - ETA: 10:47 - loss: 1.2549e-06 - accuracy: 1.000 - ETA: 10:37 - loss: 1.2632e-06 - accuracy: 1.000 - ETA: 10:27 - loss: 1.2521e-06 - accuracy: 1.000 - ETA: 10:17 - loss: 1.2455e-06 - accuracy: 1.000 - ETA: 10:06 - loss: 1.2626e-06 - accuracy: 1.000 - ETA: 9:56 - loss: 1.2575e-06 - accuracy: 1.000 - ETA: 9:46 - loss: 1.2541e-06 - accuracy: 1.00 - ETA: 9:36 - loss: 1.2690e-06 - accuracy: 1.00 - ETA: 9:26 - loss: 1.2650e-06 - accuracy: 1.00 - ETA: 9:15 - loss: 1.2644e-06 - accuracy: 1.00 - ETA: 9:04 - loss: 1.2521e-06 - accuracy: 1.00 - ETA: 8:54 - loss: 1.2465e-06 - accuracy: 1.00 - ETA: 8:43 - loss: 1.2499e-06 - accuracy: 1.00 - ETA: 8:33 - loss: 1.2399e-06 - accuracy: 1.00 - ETA: 8:23 - loss: 1.2330e-06 - accuracy: 1.00 - ETA: 8:12 - loss: 1.2278e-06 - accuracy: 1.00 - ETA: 8:02 - loss: 1.2197e-06 - accuracy: 1.00 - ETA: 7:52 - loss: 1.2244e-06 - accuracy: 1.00 - ETA: 7:41 - loss: 1.2210e-06 - accuracy: 1.00 - ETA: 7:31 - loss: 1.2094e-06 - accuracy: 1.00 - ETA: 7:21 - loss: 1.2070e-06 - accuracy: 1.00 - ETA: 7:11 - loss: 1.2093e-06 - accuracy: 1.00 - ETA: 7:00 - loss: 1.2073e-06 - accuracy: 1.00 - ETA: 6:50 - loss: 1.1998e-06 - accuracy: 1.00 - ETA: 6:40 - loss: 1.1981e-06 - accuracy: 1.00 - ETA: 6:30 - loss: 1.1905e-06 - accuracy: 1.00 - ETA: 6:19 - loss: 1.1954e-06 - accuracy: 1.00 - ETA: 6:09 - loss: 1.1900e-06 - accuracy: 1.00 - ETA: 5:59 - loss: 1.1963e-06 - accuracy: 1.00 - ETA: 5:49 - loss: 1.1885e-06 - accuracy: 1.00 - ETA: 5:38 - loss: 1.1832e-06 - accuracy: 1.00 - ETA: 5:28 - loss: 1.1748e-06 - accuracy: 1.00 - ETA: 5:18 - loss: 1.1725e-06 - accuracy: 1.00 - ETA: 5:08 - loss: 1.1677e-06 - accuracy: 1.00 - ETA: 4:57 - loss: 1.1883e-06 - accuracy: 1.00 - ETA: 4:47 - loss: 1.1814e-06 - accuracy: 1.00 - ETA: 4:37 - loss: 1.1802e-06 - accuracy: 1.00 - ETA: 4:27 - loss: 1.1832e-06 - accuracy: 1.00 - ETA: 4:16 - loss: 1.1822e-06 - accuracy: 1.00 - ETA: 4:06 - loss: 1.2059e-06 - accuracy: 1.00 - ETA: 3:56 - loss: 1.1979e-06 - accuracy: 1.00 - ETA: 3:46 - loss: 1.2014e-06 - accuracy: 1.00 - ETA: 3:36 - loss: 1.1974e-06 - accuracy: 1.00 - ETA: 3:26 - loss: 1.2035e-06 - accuracy: 1.00 - ETA: 3:15 - loss: 1.1977e-06 - accuracy: 1.00 - ETA: 3:05 - loss: 1.1914e-06 - accuracy: 1.00 - ETA: 2:55 - loss: 1.1984e-06 - accuracy: 1.00 - ETA: 2:45 - loss: 1.1959e-06 - accuracy: 1.00 - ETA: 2:35 - loss: 1.2066e-06 - accuracy: 1.00 - ETA: 2:24 - loss: 1.2040e-06 - accuracy: 1.00 - ETA: 2:14 - loss: 1.2009e-06 - accuracy: 1.00 - ETA: 2:04 - loss: 1.1934e-06 - accuracy: 1.00 - ETA: 1:54 - loss: 1.1909e-06 - accuracy: 1.00 - ETA: 1:44 - loss: 1.1920e-06 - accuracy: 1.00 - ETA: 1:33 - loss: 1.1901e-06 - accuracy: 1.00 - ETA: 1:23 - loss: 1.1842e-06 - accuracy: 1.00 - ETA: 1:13 - loss: 1.1901e-06 - accuracy: 1.00 - ETA: 1:03 - loss: 1.1830e-06 - accuracy: 1.00 - ETA: 53s - loss: 1.1832e-06 - accuracy: 1.0000 - ETA: 43s - loss: 1.1759e-06 - accuracy: 1.000 - ETA: 33s - loss: 1.1738e-06 - accuracy: 1.000 - ETA: 22s - loss: 1.1710e-06 - accuracy: 1.000 - ETA: 12s - loss: 1.1638e-06 - accuracy: 1.000 - ETA: 2s - loss: 1.1626e-06 - accuracy: 1.000 - 1295s 679ms/step - loss: 1.1607e-06 - accuracy: 1.0000 - val_loss: 0.0169 - val_accuracy: 0.9877\n",
      "Epoch 17/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1908/1908 [==============================] - ETA: 19:31 - loss: 7.3945e-07 - accuracy: 1.000 - ETA: 19:19 - loss: 8.6570e-07 - accuracy: 1.000 - ETA: 19:25 - loss: 9.3509e-07 - accuracy: 1.000 - ETA: 19:19 - loss: 8.4924e-07 - accuracy: 1.000 - ETA: 19:20 - loss: 9.9594e-07 - accuracy: 1.000 - ETA: 19:11 - loss: 1.0051e-06 - accuracy: 1.000 - ETA: 19:02 - loss: 1.1684e-06 - accuracy: 1.000 - ETA: 18:49 - loss: 1.1325e-06 - accuracy: 1.000 - ETA: 18:40 - loss: 1.0644e-06 - accuracy: 1.000 - ETA: 18:30 - loss: 1.0557e-06 - accuracy: 1.000 - ETA: 18:25 - loss: 1.1545e-06 - accuracy: 1.000 - ETA: 18:15 - loss: 1.1579e-06 - accuracy: 1.000 - ETA: 18:07 - loss: 1.1270e-06 - accuracy: 1.000 - ETA: 17:54 - loss: 1.1286e-06 - accuracy: 1.000 - ETA: 17:43 - loss: 1.1184e-06 - accuracy: 1.000 - ETA: 17:31 - loss: 1.0790e-06 - accuracy: 1.000 - ETA: 17:20 - loss: 1.1152e-06 - accuracy: 1.000 - ETA: 17:08 - loss: 1.1144e-06 - accuracy: 1.000 - ETA: 16:59 - loss: 1.1108e-06 - accuracy: 1.000 - ETA: 16:47 - loss: 1.1007e-06 - accuracy: 1.000 - ETA: 16:36 - loss: 1.0901e-06 - accuracy: 1.000 - ETA: 16:24 - loss: 1.0864e-06 - accuracy: 1.000 - ETA: 16:15 - loss: 1.1026e-06 - accuracy: 1.000 - ETA: 16:04 - loss: 1.0698e-06 - accuracy: 1.000 - ETA: 15:54 - loss: 1.0583e-06 - accuracy: 1.000 - ETA: 15:45 - loss: 1.0447e-06 - accuracy: 1.000 - ETA: 15:34 - loss: 1.0423e-06 - accuracy: 1.000 - ETA: 15:25 - loss: 1.0279e-06 - accuracy: 1.000 - ETA: 15:15 - loss: 1.0177e-06 - accuracy: 1.000 - ETA: 15:05 - loss: 1.0379e-06 - accuracy: 1.000 - ETA: 14:55 - loss: 1.0484e-06 - accuracy: 1.000 - ETA: 14:44 - loss: 1.0254e-06 - accuracy: 1.000 - ETA: 14:33 - loss: 1.0124e-06 - accuracy: 1.000 - ETA: 14:22 - loss: 1.0250e-06 - accuracy: 1.000 - ETA: 14:13 - loss: 1.0229e-06 - accuracy: 1.000 - ETA: 14:03 - loss: 1.0153e-06 - accuracy: 1.000 - ETA: 13:52 - loss: 1.0234e-06 - accuracy: 1.000 - ETA: 13:41 - loss: 1.0053e-06 - accuracy: 1.000 - ETA: 13:30 - loss: 9.9556e-07 - accuracy: 1.000 - ETA: 13:19 - loss: 9.9357e-07 - accuracy: 1.000 - ETA: 13:09 - loss: 9.9924e-07 - accuracy: 1.000 - ETA: 12:59 - loss: 9.9098e-07 - accuracy: 1.000 - ETA: 12:50 - loss: 9.8963e-07 - accuracy: 1.000 - ETA: 12:40 - loss: 9.9094e-07 - accuracy: 1.000 - ETA: 12:30 - loss: 9.9009e-07 - accuracy: 1.000 - ETA: 12:20 - loss: 9.8864e-07 - accuracy: 1.000 - ETA: 12:10 - loss: 9.9904e-07 - accuracy: 1.000 - ETA: 12:00 - loss: 9.9622e-07 - accuracy: 1.000 - ETA: 11:50 - loss: 9.9075e-07 - accuracy: 1.000 - ETA: 11:40 - loss: 9.8911e-07 - accuracy: 1.000 - ETA: 11:30 - loss: 9.7963e-07 - accuracy: 1.000 - ETA: 11:20 - loss: 9.7125e-07 - accuracy: 1.000 - ETA: 11:10 - loss: 9.6003e-07 - accuracy: 1.000 - ETA: 11:00 - loss: 9.5778e-07 - accuracy: 1.000 - ETA: 10:49 - loss: 9.8424e-07 - accuracy: 1.000 - ETA: 10:39 - loss: 9.7876e-07 - accuracy: 1.000 - ETA: 10:29 - loss: 9.7391e-07 - accuracy: 1.000 - ETA: 10:18 - loss: 9.6196e-07 - accuracy: 1.000 - ETA: 10:08 - loss: 9.6225e-07 - accuracy: 1.000 - ETA: 9:58 - loss: 9.6714e-07 - accuracy: 1.000 - ETA: 9:48 - loss: 9.7761e-07 - accuracy: 1.00 - ETA: 9:37 - loss: 9.7709e-07 - accuracy: 1.00 - ETA: 9:27 - loss: 9.7352e-07 - accuracy: 1.00 - ETA: 9:17 - loss: 9.6513e-07 - accuracy: 1.00 - ETA: 9:07 - loss: 9.7316e-07 - accuracy: 1.00 - ETA: 8:57 - loss: 9.6395e-07 - accuracy: 1.00 - ETA: 8:47 - loss: 9.6477e-07 - accuracy: 1.00 - ETA: 8:37 - loss: 9.6319e-07 - accuracy: 1.00 - ETA: 8:27 - loss: 9.7340e-07 - accuracy: 1.00 - ETA: 8:16 - loss: 9.6749e-07 - accuracy: 1.00 - ETA: 8:06 - loss: 9.6581e-07 - accuracy: 1.00 - ETA: 7:57 - loss: 9.6192e-07 - accuracy: 1.00 - ETA: 7:47 - loss: 9.5661e-07 - accuracy: 1.00 - ETA: 7:36 - loss: 9.5028e-07 - accuracy: 1.00 - ETA: 7:26 - loss: 9.4619e-07 - accuracy: 1.00 - ETA: 7:16 - loss: 9.4227e-07 - accuracy: 1.00 - ETA: 7:06 - loss: 9.4201e-07 - accuracy: 1.00 - ETA: 6:56 - loss: 9.3723e-07 - accuracy: 1.00 - ETA: 6:46 - loss: 9.4058e-07 - accuracy: 1.00 - ETA: 6:36 - loss: 9.3332e-07 - accuracy: 1.00 - ETA: 6:26 - loss: 9.2611e-07 - accuracy: 1.00 - ETA: 6:16 - loss: 9.1853e-07 - accuracy: 1.00 - ETA: 6:05 - loss: 9.2011e-07 - accuracy: 1.00 - ETA: 5:55 - loss: 9.5632e-07 - accuracy: 1.00 - ETA: 5:45 - loss: 9.5162e-07 - accuracy: 1.00 - ETA: 5:35 - loss: 9.5411e-07 - accuracy: 1.00 - ETA: 5:25 - loss: 9.5488e-07 - accuracy: 1.00 - ETA: 5:15 - loss: 9.5288e-07 - accuracy: 1.00 - ETA: 5:05 - loss: 9.5317e-07 - accuracy: 1.00 - ETA: 4:55 - loss: 9.4861e-07 - accuracy: 1.00 - ETA: 4:45 - loss: 9.4746e-07 - accuracy: 1.00 - ETA: 4:35 - loss: 9.4180e-07 - accuracy: 1.00 - ETA: 4:24 - loss: 9.4104e-07 - accuracy: 1.00 - ETA: 4:14 - loss: 9.3542e-07 - accuracy: 1.00 - ETA: 4:04 - loss: 9.3549e-07 - accuracy: 1.00 - ETA: 3:54 - loss: 9.2990e-07 - accuracy: 1.00 - ETA: 3:44 - loss: 9.4585e-07 - accuracy: 1.00 - ETA: 3:34 - loss: 9.5374e-07 - accuracy: 1.00 - ETA: 3:24 - loss: 9.4944e-07 - accuracy: 1.00 - ETA: 3:14 - loss: 9.4396e-07 - accuracy: 1.00 - ETA: 3:03 - loss: 9.3954e-07 - accuracy: 1.00 - ETA: 2:53 - loss: 9.4386e-07 - accuracy: 1.00 - ETA: 2:43 - loss: 9.4056e-07 - accuracy: 1.00 - ETA: 2:33 - loss: 9.3397e-07 - accuracy: 1.00 - ETA: 2:23 - loss: 9.3173e-07 - accuracy: 1.00 - ETA: 2:13 - loss: 9.2835e-07 - accuracy: 1.00 - ETA: 2:03 - loss: 9.2351e-07 - accuracy: 1.00 - ETA: 1:53 - loss: 9.2256e-07 - accuracy: 1.00 - ETA: 1:43 - loss: 9.1692e-07 - accuracy: 1.00 - ETA: 1:33 - loss: 9.1715e-07 - accuracy: 1.00 - ETA: 1:23 - loss: 9.1618e-07 - accuracy: 1.00 - ETA: 1:13 - loss: 9.1321e-07 - accuracy: 1.00 - ETA: 1:03 - loss: 9.0832e-07 - accuracy: 1.00 - ETA: 52s - loss: 9.0364e-07 - accuracy: 1.0000 - ETA: 42s - loss: 9.0601e-07 - accuracy: 1.000 - ETA: 32s - loss: 9.0388e-07 - accuracy: 1.000 - ETA: 22s - loss: 9.1252e-07 - accuracy: 1.000 - ETA: 12s - loss: 9.3309e-07 - accuracy: 1.000 - ETA: 2s - loss: 9.2908e-07 - accuracy: 1.000 - 1286s 674ms/step - loss: 9.2756e-07 - accuracy: 1.0000 - val_loss: 0.0172 - val_accuracy: 0.9877\n",
      "Epoch 18/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1908/1908 [==============================] - ETA: 19:37 - loss: 5.1850e-07 - accuracy: 1.000 - ETA: 19:33 - loss: 6.4581e-07 - accuracy: 1.000 - ETA: 19:38 - loss: 7.3607e-07 - accuracy: 1.000 - ETA: 19:35 - loss: 7.6555e-07 - accuracy: 1.000 - ETA: 19:24 - loss: 8.9232e-07 - accuracy: 1.000 - ETA: 19:11 - loss: 9.7798e-07 - accuracy: 1.000 - ETA: 18:58 - loss: 9.1144e-07 - accuracy: 1.000 - ETA: 18:46 - loss: 8.5097e-07 - accuracy: 1.000 - ETA: 18:31 - loss: 8.3783e-07 - accuracy: 1.000 - ETA: 18:23 - loss: 8.2951e-07 - accuracy: 1.000 - ETA: 18:11 - loss: 7.9003e-07 - accuracy: 1.000 - ETA: 18:00 - loss: 8.1047e-07 - accuracy: 1.000 - ETA: 17:47 - loss: 7.9574e-07 - accuracy: 1.000 - ETA: 17:37 - loss: 7.8474e-07 - accuracy: 1.000 - ETA: 17:31 - loss: 7.6962e-07 - accuracy: 1.000 - ETA: 17:18 - loss: 8.0577e-07 - accuracy: 1.000 - ETA: 17:08 - loss: 7.9643e-07 - accuracy: 1.000 - ETA: 16:58 - loss: 7.6742e-07 - accuracy: 1.000 - ETA: 16:48 - loss: 7.7571e-07 - accuracy: 1.000 - ETA: 16:40 - loss: 7.6725e-07 - accuracy: 1.000 - ETA: 16:32 - loss: 7.4966e-07 - accuracy: 1.000 - ETA: 16:21 - loss: 7.4701e-07 - accuracy: 1.000 - ETA: 16:11 - loss: 7.2648e-07 - accuracy: 1.000 - ETA: 16:00 - loss: 7.2003e-07 - accuracy: 1.000 - ETA: 15:48 - loss: 7.0178e-07 - accuracy: 1.000 - ETA: 15:37 - loss: 7.0052e-07 - accuracy: 1.000 - ETA: 15:28 - loss: 7.0727e-07 - accuracy: 1.000 - ETA: 15:17 - loss: 7.1659e-07 - accuracy: 1.000 - ETA: 15:06 - loss: 7.1773e-07 - accuracy: 1.000 - ETA: 14:56 - loss: 7.2296e-07 - accuracy: 1.000 - ETA: 14:45 - loss: 7.1980e-07 - accuracy: 1.000 - ETA: 14:35 - loss: 7.1086e-07 - accuracy: 1.000 - ETA: 14:25 - loss: 7.0553e-07 - accuracy: 1.000 - ETA: 14:16 - loss: 6.9555e-07 - accuracy: 1.000 - ETA: 14:06 - loss: 7.0017e-07 - accuracy: 1.000 - ETA: 13:57 - loss: 7.8420e-07 - accuracy: 1.000 - ETA: 13:46 - loss: 7.7658e-07 - accuracy: 1.000 - ETA: 13:36 - loss: 7.9977e-07 - accuracy: 1.000 - ETA: 13:26 - loss: 7.9121e-07 - accuracy: 1.000 - ETA: 13:17 - loss: 7.8714e-07 - accuracy: 1.000 - ETA: 13:07 - loss: 7.7962e-07 - accuracy: 1.000 - ETA: 12:57 - loss: 7.7823e-07 - accuracy: 1.000 - ETA: 12:47 - loss: 7.7571e-07 - accuracy: 1.000 - ETA: 12:38 - loss: 7.8015e-07 - accuracy: 1.000 - ETA: 12:28 - loss: 7.8418e-07 - accuracy: 1.000 - ETA: 12:18 - loss: 7.7494e-07 - accuracy: 1.000 - ETA: 12:08 - loss: 7.6510e-07 - accuracy: 1.000 - ETA: 11:58 - loss: 7.5712e-07 - accuracy: 1.000 - ETA: 11:48 - loss: 7.5355e-07 - accuracy: 1.000 - ETA: 11:37 - loss: 7.4726e-07 - accuracy: 1.000 - ETA: 11:27 - loss: 7.4341e-07 - accuracy: 1.000 - ETA: 11:18 - loss: 7.4280e-07 - accuracy: 1.000 - ETA: 11:08 - loss: 7.3883e-07 - accuracy: 1.000 - ETA: 10:59 - loss: 7.4395e-07 - accuracy: 1.000 - ETA: 10:49 - loss: 7.4504e-07 - accuracy: 1.000 - ETA: 10:39 - loss: 7.4938e-07 - accuracy: 1.000 - ETA: 10:30 - loss: 7.5047e-07 - accuracy: 1.000 - ETA: 10:20 - loss: 7.4590e-07 - accuracy: 1.000 - ETA: 10:10 - loss: 7.4288e-07 - accuracy: 1.000 - ETA: 10:00 - loss: 7.4829e-07 - accuracy: 1.000 - ETA: 9:49 - loss: 7.6627e-07 - accuracy: 1.000 - ETA: 9:40 - loss: 7.6292e-07 - accuracy: 1.00 - ETA: 9:29 - loss: 7.7083e-07 - accuracy: 1.00 - ETA: 9:20 - loss: 7.9167e-07 - accuracy: 1.00 - ETA: 9:09 - loss: 8.0562e-07 - accuracy: 1.00 - ETA: 9:00 - loss: 8.0224e-07 - accuracy: 1.00 - ETA: 8:49 - loss: 7.9797e-07 - accuracy: 1.00 - ETA: 8:39 - loss: 8.0331e-07 - accuracy: 1.00 - ETA: 8:29 - loss: 8.0158e-07 - accuracy: 1.00 - ETA: 8:19 - loss: 8.0911e-07 - accuracy: 1.00 - ETA: 8:09 - loss: 8.1630e-07 - accuracy: 1.00 - ETA: 8:00 - loss: 8.0831e-07 - accuracy: 1.00 - ETA: 7:50 - loss: 8.0047e-07 - accuracy: 1.00 - ETA: 7:40 - loss: 7.9574e-07 - accuracy: 1.00 - ETA: 7:30 - loss: 7.9752e-07 - accuracy: 1.00 - ETA: 7:20 - loss: 7.9596e-07 - accuracy: 1.00 - ETA: 7:10 - loss: 7.9027e-07 - accuracy: 1.00 - ETA: 6:59 - loss: 7.8655e-07 - accuracy: 1.00 - ETA: 6:49 - loss: 7.8568e-07 - accuracy: 1.00 - ETA: 6:39 - loss: 7.8929e-07 - accuracy: 1.00 - ETA: 6:29 - loss: 7.9172e-07 - accuracy: 1.00 - ETA: 6:19 - loss: 7.9298e-07 - accuracy: 1.00 - ETA: 6:09 - loss: 7.9951e-07 - accuracy: 1.00 - ETA: 5:59 - loss: 7.9928e-07 - accuracy: 1.00 - ETA: 5:49 - loss: 8.0359e-07 - accuracy: 1.00 - ETA: 5:38 - loss: 8.0137e-07 - accuracy: 1.00 - ETA: 5:28 - loss: 8.0098e-07 - accuracy: 1.00 - ETA: 5:18 - loss: 8.0036e-07 - accuracy: 1.00 - ETA: 5:08 - loss: 8.1285e-07 - accuracy: 1.00 - ETA: 4:57 - loss: 8.0824e-07 - accuracy: 1.00 - ETA: 4:47 - loss: 8.0724e-07 - accuracy: 1.00 - ETA: 4:37 - loss: 8.0782e-07 - accuracy: 1.00 - ETA: 4:27 - loss: 8.0869e-07 - accuracy: 1.00 - ETA: 4:17 - loss: 8.0874e-07 - accuracy: 1.00 - ETA: 4:07 - loss: 8.1180e-07 - accuracy: 1.00 - ETA: 3:56 - loss: 8.0682e-07 - accuracy: 1.00 - ETA: 3:46 - loss: 8.0503e-07 - accuracy: 1.00 - ETA: 3:36 - loss: 8.0229e-07 - accuracy: 1.00 - ETA: 3:26 - loss: 7.9773e-07 - accuracy: 1.00 - ETA: 3:16 - loss: 8.0273e-07 - accuracy: 1.00 - ETA: 3:06 - loss: 8.0061e-07 - accuracy: 1.00 - ETA: 2:56 - loss: 7.9709e-07 - accuracy: 1.00 - ETA: 2:45 - loss: 7.9436e-07 - accuracy: 1.00 - ETA: 2:35 - loss: 7.9149e-07 - accuracy: 1.00 - ETA: 2:25 - loss: 7.8773e-07 - accuracy: 1.00 - ETA: 2:15 - loss: 7.8374e-07 - accuracy: 1.00 - ETA: 2:04 - loss: 7.8265e-07 - accuracy: 1.00 - ETA: 1:54 - loss: 7.7916e-07 - accuracy: 1.00 - ETA: 1:44 - loss: 7.7770e-07 - accuracy: 1.00 - ETA: 1:34 - loss: 7.7310e-07 - accuracy: 1.00 - ETA: 1:24 - loss: 7.7003e-07 - accuracy: 1.00 - ETA: 1:13 - loss: 7.8002e-07 - accuracy: 1.00 - ETA: 1:03 - loss: 7.8124e-07 - accuracy: 1.00 - ETA: 53s - loss: 7.7980e-07 - accuracy: 1.0000 - ETA: 43s - loss: 7.8001e-07 - accuracy: 1.000 - ETA: 33s - loss: 7.7632e-07 - accuracy: 1.000 - ETA: 22s - loss: 7.7302e-07 - accuracy: 1.000 - ETA: 12s - loss: 7.7302e-07 - accuracy: 1.000 - ETA: 2s - loss: 7.6991e-07 - accuracy: 1.000 - 1303s 683ms/step - loss: 7.6907e-07 - accuracy: 1.0000 - val_loss: 0.0174 - val_accuracy: 0.9878\n",
      "Epoch 19/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1908/1908 [==============================] - ETA: 20:16 - loss: 3.2892e-07 - accuracy: 1.000 - ETA: 19:54 - loss: 4.0404e-07 - accuracy: 1.000 - ETA: 19:46 - loss: 3.7027e-07 - accuracy: 1.000 - ETA: 19:36 - loss: 3.9006e-07 - accuracy: 1.000 - ETA: 19:21 - loss: 3.6647e-07 - accuracy: 1.000 - ETA: 19:16 - loss: 3.7726e-07 - accuracy: 1.000 - ETA: 19:12 - loss: 5.6027e-07 - accuracy: 1.000 - ETA: 19:04 - loss: 5.6259e-07 - accuracy: 1.000 - ETA: 18:55 - loss: 5.3243e-07 - accuracy: 1.000 - ETA: 18:50 - loss: 6.3099e-07 - accuracy: 1.000 - ETA: 18:39 - loss: 6.5289e-07 - accuracy: 1.000 - ETA: 18:30 - loss: 6.9112e-07 - accuracy: 1.000 - ETA: 18:18 - loss: 6.7016e-07 - accuracy: 1.000 - ETA: 18:11 - loss: 6.4136e-07 - accuracy: 1.000 - ETA: 18:09 - loss: 6.4785e-07 - accuracy: 1.000 - ETA: 18:05 - loss: 6.2827e-07 - accuracy: 1.000 - ETA: 18:00 - loss: 6.3349e-07 - accuracy: 1.000 - ETA: 17:54 - loss: 6.1167e-07 - accuracy: 1.000 - ETA: 17:47 - loss: 6.1916e-07 - accuracy: 1.000 - ETA: 17:40 - loss: 6.1699e-07 - accuracy: 1.000 - ETA: 17:31 - loss: 6.2834e-07 - accuracy: 1.000 - ETA: 17:21 - loss: 6.4613e-07 - accuracy: 1.000 - ETA: 17:11 - loss: 6.5523e-07 - accuracy: 1.000 - ETA: 17:01 - loss: 6.4360e-07 - accuracy: 1.000 - ETA: 16:52 - loss: 6.3684e-07 - accuracy: 1.000 - ETA: 16:40 - loss: 6.2509e-07 - accuracy: 1.000 - ETA: 16:30 - loss: 6.3006e-07 - accuracy: 1.000 - ETA: 16:19 - loss: 6.3634e-07 - accuracy: 1.000 - ETA: 16:07 - loss: 6.3910e-07 - accuracy: 1.000 - ETA: 15:58 - loss: 6.5637e-07 - accuracy: 1.000 - ETA: 15:46 - loss: 6.5377e-07 - accuracy: 1.000 - ETA: 15:35 - loss: 6.6158e-07 - accuracy: 1.000 - ETA: 15:23 - loss: 6.4715e-07 - accuracy: 1.000 - ETA: 15:12 - loss: 6.4877e-07 - accuracy: 1.000 - ETA: 15:00 - loss: 6.4075e-07 - accuracy: 1.000 - ETA: 14:50 - loss: 6.4441e-07 - accuracy: 1.000 - ETA: 14:40 - loss: 6.3841e-07 - accuracy: 1.000 - ETA: 14:33 - loss: 6.2932e-07 - accuracy: 1.000 - ETA: 14:22 - loss: 6.4468e-07 - accuracy: 1.000 - ETA: 14:11 - loss: 6.4687e-07 - accuracy: 1.000 - ETA: 14:01 - loss: 6.4853e-07 - accuracy: 1.000 - ETA: 13:49 - loss: 6.4440e-07 - accuracy: 1.000 - ETA: 13:37 - loss: 6.5815e-07 - accuracy: 1.000 - ETA: 13:26 - loss: 6.5529e-07 - accuracy: 1.000 - ETA: 13:14 - loss: 6.4737e-07 - accuracy: 1.000 - ETA: 13:03 - loss: 6.3898e-07 - accuracy: 1.000 - ETA: 12:52 - loss: 6.4783e-07 - accuracy: 1.000 - ETA: 12:40 - loss: 6.4085e-07 - accuracy: 1.000 - ETA: 12:29 - loss: 6.3610e-07 - accuracy: 1.000 - ETA: 12:19 - loss: 6.4683e-07 - accuracy: 1.000 - ETA: 12:09 - loss: 6.4945e-07 - accuracy: 1.000 - ETA: 11:58 - loss: 6.4591e-07 - accuracy: 1.000 - ETA: 11:47 - loss: 6.4126e-07 - accuracy: 1.000 - ETA: 11:37 - loss: 6.4459e-07 - accuracy: 1.000 - ETA: 11:26 - loss: 6.4470e-07 - accuracy: 1.000 - ETA: 11:15 - loss: 6.5103e-07 - accuracy: 1.000 - ETA: 11:04 - loss: 6.5428e-07 - accuracy: 1.000 - ETA: 10:53 - loss: 6.4975e-07 - accuracy: 1.000 - ETA: 10:42 - loss: 6.4581e-07 - accuracy: 1.000 - ETA: 10:31 - loss: 6.4103e-07 - accuracy: 1.000 - ETA: 10:20 - loss: 6.4967e-07 - accuracy: 1.000 - ETA: 10:09 - loss: 6.4626e-07 - accuracy: 1.000 - ETA: 9:58 - loss: 6.6534e-07 - accuracy: 1.000 - ETA: 9:47 - loss: 6.6655e-07 - accuracy: 1.00 - ETA: 9:36 - loss: 6.6403e-07 - accuracy: 1.00 - ETA: 9:26 - loss: 6.5860e-07 - accuracy: 1.00 - ETA: 9:15 - loss: 6.5798e-07 - accuracy: 1.00 - ETA: 9:04 - loss: 6.5272e-07 - accuracy: 1.00 - ETA: 8:54 - loss: 6.4970e-07 - accuracy: 1.00 - ETA: 8:43 - loss: 6.4670e-07 - accuracy: 1.00 - ETA: 8:32 - loss: 6.4249e-07 - accuracy: 1.00 - ETA: 8:21 - loss: 6.4102e-07 - accuracy: 1.00 - ETA: 8:10 - loss: 6.7305e-07 - accuracy: 1.00 - ETA: 7:59 - loss: 6.7541e-07 - accuracy: 1.00 - ETA: 7:48 - loss: 6.8782e-07 - accuracy: 1.00 - ETA: 7:37 - loss: 6.9615e-07 - accuracy: 1.00 - ETA: 7:26 - loss: 6.9463e-07 - accuracy: 1.00 - ETA: 7:15 - loss: 6.9036e-07 - accuracy: 1.00 - ETA: 7:04 - loss: 6.9062e-07 - accuracy: 1.00 - ETA: 6:53 - loss: 6.8661e-07 - accuracy: 1.00 - ETA: 6:43 - loss: 6.8348e-07 - accuracy: 1.00 - ETA: 6:32 - loss: 6.8028e-07 - accuracy: 1.00 - ETA: 6:22 - loss: 6.7666e-07 - accuracy: 1.00 - ETA: 6:11 - loss: 6.7789e-07 - accuracy: 1.00 - ETA: 6:00 - loss: 6.7406e-07 - accuracy: 1.00 - ETA: 5:50 - loss: 6.7341e-07 - accuracy: 1.00 - ETA: 5:39 - loss: 6.7278e-07 - accuracy: 1.00 - ETA: 5:28 - loss: 6.6746e-07 - accuracy: 1.00 - ETA: 5:18 - loss: 6.6326e-07 - accuracy: 1.00 - ETA: 5:07 - loss: 6.6424e-07 - accuracy: 1.00 - ETA: 4:56 - loss: 6.7091e-07 - accuracy: 1.00 - ETA: 4:46 - loss: 6.7083e-07 - accuracy: 1.00 - ETA: 4:35 - loss: 6.7249e-07 - accuracy: 1.00 - ETA: 4:25 - loss: 6.6857e-07 - accuracy: 1.00 - ETA: 4:14 - loss: 6.6770e-07 - accuracy: 1.00 - ETA: 4:03 - loss: 6.6498e-07 - accuracy: 1.00 - ETA: 3:53 - loss: 6.6618e-07 - accuracy: 1.00 - ETA: 3:42 - loss: 6.6447e-07 - accuracy: 1.00 - ETA: 3:32 - loss: 6.6439e-07 - accuracy: 1.00 - ETA: 3:21 - loss: 6.6029e-07 - accuracy: 1.00 - ETA: 3:11 - loss: 6.6423e-07 - accuracy: 1.00 - ETA: 3:00 - loss: 6.6305e-07 - accuracy: 1.00 - ETA: 2:50 - loss: 6.6175e-07 - accuracy: 1.00 - ETA: 2:39 - loss: 6.6376e-07 - accuracy: 1.00 - ETA: 2:29 - loss: 6.6074e-07 - accuracy: 1.00 - ETA: 2:18 - loss: 6.5930e-07 - accuracy: 1.00 - ETA: 2:08 - loss: 6.6080e-07 - accuracy: 1.00 - ETA: 1:57 - loss: 6.5827e-07 - accuracy: 1.00 - ETA: 1:47 - loss: 6.5681e-07 - accuracy: 1.00 - ETA: 1:36 - loss: 6.6330e-07 - accuracy: 1.00 - ETA: 1:26 - loss: 6.6326e-07 - accuracy: 1.00 - ETA: 1:15 - loss: 6.6124e-07 - accuracy: 1.00 - ETA: 1:05 - loss: 6.6033e-07 - accuracy: 1.00 - ETA: 54s - loss: 6.6080e-07 - accuracy: 1.0000 - ETA: 44s - loss: 6.5899e-07 - accuracy: 1.000 - ETA: 33s - loss: 6.5488e-07 - accuracy: 1.000 - ETA: 23s - loss: 6.5436e-07 - accuracy: 1.000 - ETA: 13s - loss: 6.5268e-07 - accuracy: 1.000 - ETA: 2s - loss: 6.5066e-07 - accuracy: 1.000 - 1325s 695ms/step - loss: 6.5237e-07 - accuracy: 1.0000 - val_loss: 0.0176 - val_accuracy: 0.9880\n",
      "Epoch 20/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1908/1908 [==============================] - ETA: 20:30 - loss: 2.4691e-07 - accuracy: 1.000 - ETA: 19:50 - loss: 2.9270e-07 - accuracy: 1.000 - ETA: 19:42 - loss: 3.4153e-07 - accuracy: 1.000 - ETA: 19:43 - loss: 4.3554e-07 - accuracy: 1.000 - ETA: 19:22 - loss: 4.0015e-07 - accuracy: 1.000 - ETA: 19:08 - loss: 4.2710e-07 - accuracy: 1.000 - ETA: 18:53 - loss: 4.7051e-07 - accuracy: 1.000 - ETA: 18:38 - loss: 4.9132e-07 - accuracy: 1.000 - ETA: 18:27 - loss: 5.2911e-07 - accuracy: 1.000 - ETA: 18:23 - loss: 5.2324e-07 - accuracy: 1.000 - ETA: 18:13 - loss: 5.4437e-07 - accuracy: 1.000 - ETA: 18:04 - loss: 5.7995e-07 - accuracy: 1.000 - ETA: 17:56 - loss: 5.5897e-07 - accuracy: 1.000 - ETA: 17:47 - loss: 5.5772e-07 - accuracy: 1.000 - ETA: 17:43 - loss: 5.7589e-07 - accuracy: 1.000 - ETA: 17:43 - loss: 5.7091e-07 - accuracy: 1.000 - ETA: 17:38 - loss: 5.9674e-07 - accuracy: 1.000 - ETA: 17:26 - loss: 6.2251e-07 - accuracy: 1.000 - ETA: 17:14 - loss: 6.3777e-07 - accuracy: 1.000 - ETA: 17:02 - loss: 6.2272e-07 - accuracy: 1.000 - ETA: 16:50 - loss: 6.6496e-07 - accuracy: 1.000 - ETA: 16:42 - loss: 6.5651e-07 - accuracy: 1.000 - ETA: 16:32 - loss: 6.7306e-07 - accuracy: 1.000 - ETA: 16:21 - loss: 6.6730e-07 - accuracy: 1.000 - ETA: 16:12 - loss: 6.7023e-07 - accuracy: 1.000 - ETA: 16:02 - loss: 6.5505e-07 - accuracy: 1.000 - ETA: 15:55 - loss: 6.4219e-07 - accuracy: 1.000 - ETA: 15:45 - loss: 6.3466e-07 - accuracy: 1.000 - ETA: 15:34 - loss: 6.3457e-07 - accuracy: 1.000 - ETA: 15:24 - loss: 6.2347e-07 - accuracy: 1.000 - ETA: 15:13 - loss: 6.1065e-07 - accuracy: 1.000 - ETA: 15:01 - loss: 6.1469e-07 - accuracy: 1.000 - ETA: 14:50 - loss: 6.0425e-07 - accuracy: 1.000 - ETA: 14:40 - loss: 6.1040e-07 - accuracy: 1.000 - ETA: 14:29 - loss: 6.0792e-07 - accuracy: 1.000 - ETA: 14:19 - loss: 5.9769e-07 - accuracy: 1.000 - ETA: 14:08 - loss: 5.8866e-07 - accuracy: 1.000 - ETA: 13:56 - loss: 5.8453e-07 - accuracy: 1.000 - ETA: 13:46 - loss: 6.4360e-07 - accuracy: 1.000 - ETA: 13:35 - loss: 6.3484e-07 - accuracy: 1.000 - ETA: 13:25 - loss: 6.2573e-07 - accuracy: 1.000 - ETA: 13:14 - loss: 6.2220e-07 - accuracy: 1.000 - ETA: 13:04 - loss: 6.2543e-07 - accuracy: 1.000 - ETA: 12:54 - loss: 6.2298e-07 - accuracy: 1.000 - ETA: 12:43 - loss: 6.2059e-07 - accuracy: 1.000 - ETA: 12:34 - loss: 6.1481e-07 - accuracy: 1.000 - ETA: 12:23 - loss: 6.0745e-07 - accuracy: 1.000 - ETA: 12:13 - loss: 5.9913e-07 - accuracy: 1.000 - ETA: 12:02 - loss: 5.9565e-07 - accuracy: 1.000 - ETA: 11:51 - loss: 5.9313e-07 - accuracy: 1.000 - ETA: 11:40 - loss: 5.9371e-07 - accuracy: 1.000 - ETA: 11:30 - loss: 5.9276e-07 - accuracy: 1.000 - ETA: 11:20 - loss: 5.8907e-07 - accuracy: 1.000 - ETA: 11:09 - loss: 5.8663e-07 - accuracy: 1.000 - ETA: 10:59 - loss: 5.9134e-07 - accuracy: 1.000 - ETA: 10:49 - loss: 5.9593e-07 - accuracy: 1.000 - ETA: 10:38 - loss: 5.9250e-07 - accuracy: 1.000 - ETA: 10:28 - loss: 5.9019e-07 - accuracy: 1.000 - ETA: 10:18 - loss: 5.8851e-07 - accuracy: 1.000 - ETA: 10:07 - loss: 5.8375e-07 - accuracy: 1.000 - ETA: 9:57 - loss: 5.8388e-07 - accuracy: 1.000 - ETA: 9:46 - loss: 5.8137e-07 - accuracy: 1.00 - ETA: 9:36 - loss: 5.8679e-07 - accuracy: 1.00 - ETA: 9:25 - loss: 5.8250e-07 - accuracy: 1.00 - ETA: 9:15 - loss: 5.8086e-07 - accuracy: 1.00 - ETA: 9:04 - loss: 5.7960e-07 - accuracy: 1.00 - ETA: 8:54 - loss: 5.7500e-07 - accuracy: 1.00 - ETA: 8:44 - loss: 5.7182e-07 - accuracy: 1.00 - ETA: 8:33 - loss: 5.6956e-07 - accuracy: 1.00 - ETA: 8:23 - loss: 5.7084e-07 - accuracy: 1.00 - ETA: 8:13 - loss: 5.6704e-07 - accuracy: 1.00 - ETA: 8:03 - loss: 5.8868e-07 - accuracy: 1.00 - ETA: 7:53 - loss: 5.8349e-07 - accuracy: 1.00 - ETA: 7:43 - loss: 5.8117e-07 - accuracy: 1.00 - ETA: 7:32 - loss: 5.8168e-07 - accuracy: 1.00 - ETA: 7:22 - loss: 5.8034e-07 - accuracy: 1.00 - ETA: 7:12 - loss: 5.8359e-07 - accuracy: 1.00 - ETA: 7:02 - loss: 5.9392e-07 - accuracy: 1.00 - ETA: 6:52 - loss: 5.9924e-07 - accuracy: 1.00 - ETA: 6:41 - loss: 5.9536e-07 - accuracy: 1.00 - ETA: 6:31 - loss: 5.9153e-07 - accuracy: 1.00 - ETA: 6:21 - loss: 5.8970e-07 - accuracy: 1.00 - ETA: 6:11 - loss: 5.8654e-07 - accuracy: 1.00 - ETA: 6:01 - loss: 5.8540e-07 - accuracy: 1.00 - ETA: 5:51 - loss: 5.8360e-07 - accuracy: 1.00 - ETA: 5:41 - loss: 5.8495e-07 - accuracy: 1.00 - ETA: 5:30 - loss: 5.8226e-07 - accuracy: 1.00 - ETA: 5:20 - loss: 5.7975e-07 - accuracy: 1.00 - ETA: 5:10 - loss: 5.7674e-07 - accuracy: 1.00 - ETA: 4:59 - loss: 5.7417e-07 - accuracy: 1.00 - ETA: 4:49 - loss: 5.7123e-07 - accuracy: 1.00 - ETA: 4:39 - loss: 5.7549e-07 - accuracy: 1.00 - ETA: 4:29 - loss: 5.7274e-07 - accuracy: 1.00 - ETA: 4:18 - loss: 5.7341e-07 - accuracy: 1.00 - ETA: 4:08 - loss: 5.7063e-07 - accuracy: 1.00 - ETA: 3:58 - loss: 5.6966e-07 - accuracy: 1.00 - ETA: 3:47 - loss: 5.6753e-07 - accuracy: 1.00 - ETA: 3:37 - loss: 5.6518e-07 - accuracy: 1.00 - ETA: 3:27 - loss: 5.6319e-07 - accuracy: 1.00 - ETA: 3:17 - loss: 5.5969e-07 - accuracy: 1.00 - ETA: 3:06 - loss: 5.5874e-07 - accuracy: 1.00 - ETA: 2:56 - loss: 5.5793e-07 - accuracy: 1.00 - ETA: 2:46 - loss: 5.6458e-07 - accuracy: 1.00 - ETA: 2:36 - loss: 5.6431e-07 - accuracy: 1.00 - ETA: 2:25 - loss: 5.6307e-07 - accuracy: 1.00 - ETA: 2:15 - loss: 5.6052e-07 - accuracy: 1.00 - ETA: 2:05 - loss: 5.5934e-07 - accuracy: 1.00 - ETA: 1:55 - loss: 5.5846e-07 - accuracy: 1.00 - ETA: 1:44 - loss: 5.5932e-07 - accuracy: 1.00 - ETA: 1:34 - loss: 5.5960e-07 - accuracy: 1.00 - ETA: 1:24 - loss: 5.5902e-07 - accuracy: 1.00 - ETA: 1:14 - loss: 5.6138e-07 - accuracy: 1.00 - ETA: 1:03 - loss: 5.6030e-07 - accuracy: 1.00 - ETA: 53s - loss: 5.5942e-07 - accuracy: 1.0000 - ETA: 43s - loss: 5.5830e-07 - accuracy: 1.000 - ETA: 33s - loss: 5.6377e-07 - accuracy: 1.000 - ETA: 22s - loss: 5.6414e-07 - accuracy: 1.000 - ETA: 12s - loss: 5.6170e-07 - accuracy: 1.000 - ETA: 2s - loss: 5.6245e-07 - accuracy: 1.000 - 1298s 680ms/step - loss: 5.6165e-07 - accuracy: 1.0000 - val_loss: 0.0179 - val_accuracy: 0.9880\n",
      "Epoch 21/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1908/1908 [==============================] - ETA: 19:33 - loss: 2.0298e-07 - accuracy: 1.000 - ETA: 19:57 - loss: 8.4019e-07 - accuracy: 1.000 - ETA: 19:57 - loss: 7.2120e-07 - accuracy: 1.000 - ETA: 19:45 - loss: 6.9738e-07 - accuracy: 1.000 - ETA: 19:30 - loss: 6.3516e-07 - accuracy: 1.000 - ETA: 19:25 - loss: 6.2337e-07 - accuracy: 1.000 - ETA: 19:14 - loss: 6.4605e-07 - accuracy: 1.000 - ETA: 19:02 - loss: 5.8641e-07 - accuracy: 1.000 - ETA: 18:50 - loss: 5.9239e-07 - accuracy: 1.000 - ETA: 18:41 - loss: 5.9351e-07 - accuracy: 1.000 - ETA: 18:28 - loss: 5.6466e-07 - accuracy: 1.000 - ETA: 18:17 - loss: 5.9997e-07 - accuracy: 1.000 - ETA: 18:05 - loss: 5.9560e-07 - accuracy: 1.000 - ETA: 17:58 - loss: 6.1129e-07 - accuracy: 1.000 - ETA: 17:52 - loss: 5.9163e-07 - accuracy: 1.000 - ETA: 17:44 - loss: 5.9689e-07 - accuracy: 1.000 - ETA: 17:34 - loss: 5.9614e-07 - accuracy: 1.000 - ETA: 17:24 - loss: 5.6979e-07 - accuracy: 1.000 - ETA: 17:12 - loss: 5.7532e-07 - accuracy: 1.000 - ETA: 17:02 - loss: 5.6672e-07 - accuracy: 1.000 - ETA: 16:50 - loss: 5.6575e-07 - accuracy: 1.000 - ETA: 16:39 - loss: 5.7189e-07 - accuracy: 1.000 - ETA: 16:29 - loss: 5.5581e-07 - accuracy: 1.000 - ETA: 16:19 - loss: 5.5462e-07 - accuracy: 1.000 - ETA: 16:09 - loss: 5.5998e-07 - accuracy: 1.000 - ETA: 15:58 - loss: 5.5424e-07 - accuracy: 1.000 - ETA: 15:47 - loss: 5.4483e-07 - accuracy: 1.000 - ETA: 15:36 - loss: 5.3938e-07 - accuracy: 1.000 - ETA: 15:25 - loss: 5.5538e-07 - accuracy: 1.000 - ETA: 15:15 - loss: 5.5909e-07 - accuracy: 1.000 - ETA: 15:06 - loss: 5.4881e-07 - accuracy: 1.000 - ETA: 14:54 - loss: 5.4355e-07 - accuracy: 1.000 - ETA: 14:44 - loss: 5.4028e-07 - accuracy: 1.000 - ETA: 14:33 - loss: 5.3990e-07 - accuracy: 1.000 - ETA: 14:23 - loss: 5.3911e-07 - accuracy: 1.000 - ETA: 14:12 - loss: 5.3185e-07 - accuracy: 1.000 - ETA: 14:02 - loss: 5.2713e-07 - accuracy: 1.000 - ETA: 13:52 - loss: 5.2301e-07 - accuracy: 1.000 - ETA: 13:42 - loss: 5.2119e-07 - accuracy: 1.000 - ETA: 13:33 - loss: 5.2447e-07 - accuracy: 1.000 - ETA: 13:23 - loss: 5.2543e-07 - accuracy: 1.000 - ETA: 13:12 - loss: 5.1638e-07 - accuracy: 1.000 - ETA: 13:02 - loss: 5.1012e-07 - accuracy: 1.000 - ETA: 12:52 - loss: 5.0846e-07 - accuracy: 1.000 - ETA: 12:41 - loss: 5.2176e-07 - accuracy: 1.000 - ETA: 12:31 - loss: 5.1760e-07 - accuracy: 1.000 - ETA: 12:20 - loss: 5.0984e-07 - accuracy: 1.000 - ETA: 12:10 - loss: 5.0334e-07 - accuracy: 1.000 - ETA: 12:00 - loss: 4.9910e-07 - accuracy: 1.000 - ETA: 11:49 - loss: 4.9768e-07 - accuracy: 1.000 - ETA: 11:39 - loss: 5.0087e-07 - accuracy: 1.000 - ETA: 11:28 - loss: 4.9701e-07 - accuracy: 1.000 - ETA: 11:17 - loss: 5.0732e-07 - accuracy: 1.000 - ETA: 11:06 - loss: 5.1177e-07 - accuracy: 1.000 - ETA: 10:56 - loss: 5.0750e-07 - accuracy: 1.000 - ETA: 10:46 - loss: 5.4288e-07 - accuracy: 1.000 - ETA: 10:35 - loss: 5.3752e-07 - accuracy: 1.000 - ETA: 10:25 - loss: 5.3227e-07 - accuracy: 1.000 - ETA: 10:15 - loss: 5.2900e-07 - accuracy: 1.000 - ETA: 10:04 - loss: 5.2891e-07 - accuracy: 1.000 - ETA: 9:54 - loss: 5.3468e-07 - accuracy: 1.000 - ETA: 9:44 - loss: 5.3171e-07 - accuracy: 1.00 - ETA: 9:34 - loss: 5.2898e-07 - accuracy: 1.00 - ETA: 9:24 - loss: 5.2632e-07 - accuracy: 1.00 - ETA: 9:14 - loss: 5.2723e-07 - accuracy: 1.00 - ETA: 9:04 - loss: 5.2216e-07 - accuracy: 1.00 - ETA: 8:54 - loss: 5.2526e-07 - accuracy: 1.00 - ETA: 8:44 - loss: 5.2406e-07 - accuracy: 1.00 - ETA: 8:34 - loss: 5.2358e-07 - accuracy: 1.00 - ETA: 8:23 - loss: 5.2085e-07 - accuracy: 1.00 - ETA: 8:13 - loss: 5.2497e-07 - accuracy: 1.00 - ETA: 8:02 - loss: 5.2217e-07 - accuracy: 1.00 - ETA: 7:52 - loss: 5.1955e-07 - accuracy: 1.00 - ETA: 7:42 - loss: 5.1497e-07 - accuracy: 1.00 - ETA: 7:31 - loss: 5.1529e-07 - accuracy: 1.00 - ETA: 7:21 - loss: 5.1192e-07 - accuracy: 1.00 - ETA: 7:11 - loss: 5.1552e-07 - accuracy: 1.00 - ETA: 7:00 - loss: 5.1607e-07 - accuracy: 1.00 - ETA: 6:50 - loss: 5.1427e-07 - accuracy: 1.00 - ETA: 6:40 - loss: 5.1554e-07 - accuracy: 1.00 - ETA: 6:29 - loss: 5.1346e-07 - accuracy: 1.00 - ETA: 6:19 - loss: 5.0920e-07 - accuracy: 1.00 - ETA: 6:09 - loss: 5.0696e-07 - accuracy: 1.00 - ETA: 5:59 - loss: 5.0513e-07 - accuracy: 1.00 - ETA: 5:49 - loss: 5.0280e-07 - accuracy: 1.00 - ETA: 5:38 - loss: 5.0075e-07 - accuracy: 1.00 - ETA: 5:28 - loss: 4.9870e-07 - accuracy: 1.00 - ETA: 5:18 - loss: 5.0118e-07 - accuracy: 1.00 - ETA: 5:08 - loss: 5.0063e-07 - accuracy: 1.00 - ETA: 4:58 - loss: 4.9739e-07 - accuracy: 1.00 - ETA: 4:47 - loss: 4.9509e-07 - accuracy: 1.00 - ETA: 4:37 - loss: 4.9546e-07 - accuracy: 1.00 - ETA: 4:27 - loss: 4.9198e-07 - accuracy: 1.00 - ETA: 4:17 - loss: 4.9151e-07 - accuracy: 1.00 - ETA: 4:07 - loss: 4.8962e-07 - accuracy: 1.00 - ETA: 3:56 - loss: 4.8737e-07 - accuracy: 1.00 - ETA: 3:46 - loss: 4.8508e-07 - accuracy: 1.00 - ETA: 3:36 - loss: 4.8413e-07 - accuracy: 1.00 - ETA: 3:26 - loss: 4.9017e-07 - accuracy: 1.00 - ETA: 3:16 - loss: 4.8836e-07 - accuracy: 1.00 - ETA: 3:05 - loss: 4.8718e-07 - accuracy: 1.00 - ETA: 2:55 - loss: 4.9393e-07 - accuracy: 1.00 - ETA: 2:45 - loss: 4.9456e-07 - accuracy: 1.00 - ETA: 2:35 - loss: 4.9404e-07 - accuracy: 1.00 - ETA: 2:25 - loss: 4.9365e-07 - accuracy: 1.00 - ETA: 2:14 - loss: 4.9222e-07 - accuracy: 1.00 - ETA: 2:04 - loss: 4.9012e-07 - accuracy: 1.00 - ETA: 1:54 - loss: 4.9271e-07 - accuracy: 1.00 - ETA: 1:44 - loss: 4.9711e-07 - accuracy: 1.00 - ETA: 1:34 - loss: 4.9566e-07 - accuracy: 1.00 - ETA: 1:24 - loss: 4.9261e-07 - accuracy: 1.00 - ETA: 1:13 - loss: 4.9328e-07 - accuracy: 1.00 - ETA: 1:03 - loss: 4.9285e-07 - accuracy: 1.00 - ETA: 53s - loss: 4.9194e-07 - accuracy: 1.0000 - ETA: 43s - loss: 4.9265e-07 - accuracy: 1.000 - ETA: 33s - loss: 4.9168e-07 - accuracy: 1.000 - ETA: 22s - loss: 4.9296e-07 - accuracy: 1.000 - ETA: 12s - loss: 4.9325e-07 - accuracy: 1.000 - ETA: 2s - loss: 4.9092e-07 - accuracy: 1.000 - 1303s 683ms/step - loss: 4.9110e-07 - accuracy: 1.0000 - val_loss: 0.0181 - val_accuracy: 0.9880\n",
      "Epoch 22/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1908/1908 [==============================] - ETA: 18:56 - loss: 3.9525e-07 - accuracy: 1.000 - ETA: 18:55 - loss: 3.4358e-07 - accuracy: 1.000 - ETA: 18:51 - loss: 3.8275e-07 - accuracy: 1.000 - ETA: 18:49 - loss: 3.6978e-07 - accuracy: 1.000 - ETA: 18:47 - loss: 3.9955e-07 - accuracy: 1.000 - ETA: 18:38 - loss: 3.7564e-07 - accuracy: 1.000 - ETA: 18:38 - loss: 3.5534e-07 - accuracy: 1.000 - ETA: 18:34 - loss: 3.6137e-07 - accuracy: 1.000 - ETA: 18:24 - loss: 3.5574e-07 - accuracy: 1.000 - ETA: 18:14 - loss: 3.4703e-07 - accuracy: 1.000 - ETA: 18:04 - loss: 3.4926e-07 - accuracy: 1.000 - ETA: 17:54 - loss: 4.2309e-07 - accuracy: 1.000 - ETA: 17:45 - loss: 4.2813e-07 - accuracy: 1.000 - ETA: 17:38 - loss: 4.2933e-07 - accuracy: 1.000 - ETA: 17:29 - loss: 4.1500e-07 - accuracy: 1.000 - ETA: 17:19 - loss: 4.1740e-07 - accuracy: 1.000 - ETA: 17:10 - loss: 4.2292e-07 - accuracy: 1.000 - ETA: 17:00 - loss: 4.1720e-07 - accuracy: 1.000 - ETA: 16:50 - loss: 4.1686e-07 - accuracy: 1.000 - ETA: 16:42 - loss: 4.0268e-07 - accuracy: 1.000 - ETA: 16:33 - loss: 3.9171e-07 - accuracy: 1.000 - ETA: 16:22 - loss: 3.8222e-07 - accuracy: 1.000 - ETA: 16:12 - loss: 3.8966e-07 - accuracy: 1.000 - ETA: 16:01 - loss: 3.9315e-07 - accuracy: 1.000 - ETA: 15:50 - loss: 3.8985e-07 - accuracy: 1.000 - ETA: 15:40 - loss: 3.8813e-07 - accuracy: 1.000 - ETA: 15:31 - loss: 3.8154e-07 - accuracy: 1.000 - ETA: 15:21 - loss: 3.8424e-07 - accuracy: 1.000 - ETA: 15:11 - loss: 3.8422e-07 - accuracy: 1.000 - ETA: 15:00 - loss: 3.7970e-07 - accuracy: 1.000 - ETA: 14:48 - loss: 3.9368e-07 - accuracy: 1.000 - ETA: 14:40 - loss: 3.9705e-07 - accuracy: 1.000 - ETA: 14:30 - loss: 4.0715e-07 - accuracy: 1.000 - ETA: 14:20 - loss: 4.0968e-07 - accuracy: 1.000 - ETA: 14:10 - loss: 4.1109e-07 - accuracy: 1.000 - ETA: 14:00 - loss: 4.1286e-07 - accuracy: 1.000 - ETA: 13:50 - loss: 4.1096e-07 - accuracy: 1.000 - ETA: 13:40 - loss: 4.1426e-07 - accuracy: 1.000 - ETA: 13:30 - loss: 4.1587e-07 - accuracy: 1.000 - ETA: 13:20 - loss: 4.1148e-07 - accuracy: 1.000 - ETA: 13:10 - loss: 4.1097e-07 - accuracy: 1.000 - ETA: 12:59 - loss: 4.0941e-07 - accuracy: 1.000 - ETA: 12:49 - loss: 4.1780e-07 - accuracy: 1.000 - ETA: 12:39 - loss: 4.1384e-07 - accuracy: 1.000 - ETA: 12:29 - loss: 4.1268e-07 - accuracy: 1.000 - ETA: 12:18 - loss: 4.1263e-07 - accuracy: 1.000 - ETA: 12:08 - loss: 4.1343e-07 - accuracy: 1.000 - ETA: 11:57 - loss: 4.1434e-07 - accuracy: 1.000 - ETA: 11:47 - loss: 4.1150e-07 - accuracy: 1.000 - ETA: 11:37 - loss: 4.1035e-07 - accuracy: 1.000 - ETA: 11:27 - loss: 4.1229e-07 - accuracy: 1.000 - ETA: 11:18 - loss: 4.1106e-07 - accuracy: 1.000 - ETA: 11:08 - loss: 4.0802e-07 - accuracy: 1.000 - ETA: 10:58 - loss: 4.0614e-07 - accuracy: 1.000 - ETA: 10:48 - loss: 4.1758e-07 - accuracy: 1.000 - ETA: 10:37 - loss: 4.1751e-07 - accuracy: 1.000 - ETA: 10:28 - loss: 4.1472e-07 - accuracy: 1.000 - ETA: 10:18 - loss: 4.1707e-07 - accuracy: 1.000 - ETA: 10:08 - loss: 4.2331e-07 - accuracy: 1.000 - ETA: 9:58 - loss: 4.1973e-07 - accuracy: 1.000 - ETA: 9:48 - loss: 4.2425e-07 - accuracy: 1.00 - ETA: 9:37 - loss: 4.2295e-07 - accuracy: 1.00 - ETA: 9:27 - loss: 4.2223e-07 - accuracy: 1.00 - ETA: 9:17 - loss: 4.2360e-07 - accuracy: 1.00 - ETA: 9:07 - loss: 4.1915e-07 - accuracy: 1.00 - ETA: 8:57 - loss: 4.1717e-07 - accuracy: 1.00 - ETA: 8:47 - loss: 4.1631e-07 - accuracy: 1.00 - ETA: 8:37 - loss: 4.1362e-07 - accuracy: 1.00 - ETA: 8:27 - loss: 4.2347e-07 - accuracy: 1.00 - ETA: 8:17 - loss: 4.2418e-07 - accuracy: 1.00 - ETA: 8:07 - loss: 4.2328e-07 - accuracy: 1.00 - ETA: 7:57 - loss: 4.2249e-07 - accuracy: 1.00 - ETA: 7:47 - loss: 4.1984e-07 - accuracy: 1.00 - ETA: 7:37 - loss: 4.2699e-07 - accuracy: 1.00 - ETA: 7:27 - loss: 4.2513e-07 - accuracy: 1.00 - ETA: 7:17 - loss: 4.2151e-07 - accuracy: 1.00 - ETA: 7:06 - loss: 4.1969e-07 - accuracy: 1.00 - ETA: 6:56 - loss: 4.1906e-07 - accuracy: 1.00 - ETA: 6:46 - loss: 4.1723e-07 - accuracy: 1.00 - ETA: 6:36 - loss: 4.1452e-07 - accuracy: 1.00 - ETA: 6:26 - loss: 4.1205e-07 - accuracy: 1.00 - ETA: 6:16 - loss: 4.0946e-07 - accuracy: 1.00 - ETA: 6:06 - loss: 4.0792e-07 - accuracy: 1.00 - ETA: 5:56 - loss: 4.0718e-07 - accuracy: 1.00 - ETA: 5:45 - loss: 4.0696e-07 - accuracy: 1.00 - ETA: 5:35 - loss: 4.0553e-07 - accuracy: 1.00 - ETA: 5:25 - loss: 4.0571e-07 - accuracy: 1.00 - ETA: 5:15 - loss: 4.0612e-07 - accuracy: 1.00 - ETA: 5:05 - loss: 4.0548e-07 - accuracy: 1.00 - ETA: 4:55 - loss: 4.0656e-07 - accuracy: 1.00 - ETA: 4:45 - loss: 4.1407e-07 - accuracy: 1.00 - ETA: 4:34 - loss: 4.1535e-07 - accuracy: 1.00 - ETA: 4:24 - loss: 4.1528e-07 - accuracy: 1.00 - ETA: 4:14 - loss: 4.1424e-07 - accuracy: 1.00 - ETA: 4:04 - loss: 4.1324e-07 - accuracy: 1.00 - ETA: 3:54 - loss: 4.1785e-07 - accuracy: 1.00 - ETA: 3:44 - loss: 4.1718e-07 - accuracy: 1.00 - ETA: 3:34 - loss: 4.1556e-07 - accuracy: 1.00 - ETA: 3:24 - loss: 4.2156e-07 - accuracy: 1.00 - ETA: 3:14 - loss: 4.2052e-07 - accuracy: 1.00 - ETA: 3:04 - loss: 4.1926e-07 - accuracy: 1.00 - ETA: 2:54 - loss: 4.1971e-07 - accuracy: 1.00 - ETA: 2:44 - loss: 4.1814e-07 - accuracy: 1.00 - ETA: 2:33 - loss: 4.2270e-07 - accuracy: 1.00 - ETA: 2:23 - loss: 4.2067e-07 - accuracy: 1.00 - ETA: 2:13 - loss: 4.2257e-07 - accuracy: 1.00 - ETA: 2:03 - loss: 4.2500e-07 - accuracy: 1.00 - ETA: 1:53 - loss: 4.2312e-07 - accuracy: 1.00 - ETA: 1:43 - loss: 4.2430e-07 - accuracy: 1.00 - ETA: 1:33 - loss: 4.2276e-07 - accuracy: 1.00 - ETA: 1:23 - loss: 4.2213e-07 - accuracy: 1.00 - ETA: 1:13 - loss: 4.3696e-07 - accuracy: 1.00 - ETA: 1:03 - loss: 4.3614e-07 - accuracy: 1.00 - ETA: 52s - loss: 4.3624e-07 - accuracy: 1.0000 - ETA: 42s - loss: 4.3617e-07 - accuracy: 1.000 - ETA: 32s - loss: 4.3361e-07 - accuracy: 1.000 - ETA: 22s - loss: 4.3465e-07 - accuracy: 1.000 - ETA: 12s - loss: 4.3292e-07 - accuracy: 1.000 - ETA: 2s - loss: 4.3355e-07 - accuracy: 1.000 - 1285s 674ms/step - loss: 4.3315e-07 - accuracy: 1.0000 - val_loss: 0.0182 - val_accuracy: 0.9880\n",
      "Epoch 23/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1908/1908 [==============================] - ETA: 19:26 - loss: 5.9296e-07 - accuracy: 1.000 - ETA: 19:10 - loss: 4.4966e-07 - accuracy: 1.000 - ETA: 19:17 - loss: 5.0824e-07 - accuracy: 1.000 - ETA: 19:18 - loss: 5.1872e-07 - accuracy: 1.000 - ETA: 19:06 - loss: 5.3836e-07 - accuracy: 1.000 - ETA: 19:07 - loss: 5.7495e-07 - accuracy: 1.000 - ETA: 18:58 - loss: 5.1519e-07 - accuracy: 1.000 - ETA: 18:45 - loss: 4.9866e-07 - accuracy: 1.000 - ETA: 18:35 - loss: 4.9222e-07 - accuracy: 1.000 - ETA: 18:22 - loss: 5.4461e-07 - accuracy: 1.000 - ETA: 18:15 - loss: 5.4254e-07 - accuracy: 1.000 - ETA: 18:04 - loss: 5.2217e-07 - accuracy: 1.000 - ETA: 17:56 - loss: 5.2459e-07 - accuracy: 1.000 - ETA: 17:45 - loss: 5.1328e-07 - accuracy: 1.000 - ETA: 17:34 - loss: 4.8621e-07 - accuracy: 1.000 - ETA: 17:24 - loss: 4.7373e-07 - accuracy: 1.000 - ETA: 17:15 - loss: 4.5425e-07 - accuracy: 1.000 - ETA: 17:03 - loss: 4.3994e-07 - accuracy: 1.000 - ETA: 16:55 - loss: 4.2978e-07 - accuracy: 1.000 - ETA: 16:45 - loss: 4.3539e-07 - accuracy: 1.000 - ETA: 16:34 - loss: 4.2454e-07 - accuracy: 1.000 - ETA: 16:22 - loss: 4.5085e-07 - accuracy: 1.000 - ETA: 16:13 - loss: 4.3905e-07 - accuracy: 1.000 - ETA: 16:02 - loss: 4.3379e-07 - accuracy: 1.000 - ETA: 15:51 - loss: 4.3166e-07 - accuracy: 1.000 - ETA: 15:41 - loss: 4.5383e-07 - accuracy: 1.000 - ETA: 15:31 - loss: 4.4709e-07 - accuracy: 1.000 - ETA: 15:22 - loss: 4.4523e-07 - accuracy: 1.000 - ETA: 15:12 - loss: 4.3974e-07 - accuracy: 1.000 - ETA: 15:01 - loss: 4.3861e-07 - accuracy: 1.000 - ETA: 14:52 - loss: 4.3282e-07 - accuracy: 1.000 - ETA: 14:41 - loss: 4.3370e-07 - accuracy: 1.000 - ETA: 14:30 - loss: 4.3549e-07 - accuracy: 1.000 - ETA: 14:19 - loss: 4.3410e-07 - accuracy: 1.000 - ETA: 14:09 - loss: 4.3233e-07 - accuracy: 1.000 - ETA: 13:59 - loss: 4.2499e-07 - accuracy: 1.000 - ETA: 13:48 - loss: 4.3032e-07 - accuracy: 1.000 - ETA: 13:38 - loss: 4.3181e-07 - accuracy: 1.000 - ETA: 13:27 - loss: 4.3493e-07 - accuracy: 1.000 - ETA: 13:16 - loss: 4.2813e-07 - accuracy: 1.000 - ETA: 13:06 - loss: 4.2392e-07 - accuracy: 1.000 - ETA: 12:57 - loss: 4.2261e-07 - accuracy: 1.000 - ETA: 12:48 - loss: 4.1894e-07 - accuracy: 1.000 - ETA: 12:38 - loss: 4.1590e-07 - accuracy: 1.000 - ETA: 12:28 - loss: 4.1174e-07 - accuracy: 1.000 - ETA: 12:18 - loss: 4.0731e-07 - accuracy: 1.000 - ETA: 12:09 - loss: 4.1380e-07 - accuracy: 1.000 - ETA: 11:59 - loss: 4.1358e-07 - accuracy: 1.000 - ETA: 11:49 - loss: 4.1126e-07 - accuracy: 1.000 - ETA: 11:39 - loss: 4.0944e-07 - accuracy: 1.000 - ETA: 11:30 - loss: 4.1122e-07 - accuracy: 1.000 - ETA: 11:19 - loss: 4.1208e-07 - accuracy: 1.000 - ETA: 11:10 - loss: 4.1237e-07 - accuracy: 1.000 - ETA: 11:00 - loss: 4.1088e-07 - accuracy: 1.000 - ETA: 10:50 - loss: 4.1299e-07 - accuracy: 1.000 - ETA: 10:40 - loss: 4.1099e-07 - accuracy: 1.000 - ETA: 10:30 - loss: 4.0840e-07 - accuracy: 1.000 - ETA: 10:20 - loss: 4.0764e-07 - accuracy: 1.000 - ETA: 10:10 - loss: 4.1017e-07 - accuracy: 1.000 - ETA: 10:00 - loss: 4.1252e-07 - accuracy: 1.000 - ETA: 9:50 - loss: 4.0973e-07 - accuracy: 1.000 - ETA: 9:41 - loss: 4.0996e-07 - accuracy: 1.00 - ETA: 9:31 - loss: 4.0560e-07 - accuracy: 1.00 - ETA: 9:22 - loss: 4.0517e-07 - accuracy: 1.00 - ETA: 9:12 - loss: 4.0209e-07 - accuracy: 1.00 - ETA: 9:03 - loss: 4.0216e-07 - accuracy: 1.00 - ETA: 8:53 - loss: 3.9960e-07 - accuracy: 1.00 - ETA: 8:43 - loss: 4.0049e-07 - accuracy: 1.00 - ETA: 8:33 - loss: 3.9749e-07 - accuracy: 1.00 - ETA: 8:23 - loss: 3.9824e-07 - accuracy: 1.00 - ETA: 8:13 - loss: 3.9848e-07 - accuracy: 1.00 - ETA: 8:03 - loss: 3.9671e-07 - accuracy: 1.00 - ETA: 7:53 - loss: 3.9439e-07 - accuracy: 1.00 - ETA: 7:43 - loss: 3.9144e-07 - accuracy: 1.00 - ETA: 7:33 - loss: 4.1199e-07 - accuracy: 1.00 - ETA: 7:23 - loss: 4.1065e-07 - accuracy: 1.00 - ETA: 7:13 - loss: 4.0749e-07 - accuracy: 1.00 - ETA: 7:03 - loss: 4.0604e-07 - accuracy: 1.00 - ETA: 6:53 - loss: 4.0399e-07 - accuracy: 1.00 - ETA: 6:42 - loss: 4.0460e-07 - accuracy: 1.00 - ETA: 6:32 - loss: 4.0828e-07 - accuracy: 1.00 - ETA: 6:22 - loss: 4.0754e-07 - accuracy: 1.00 - ETA: 6:12 - loss: 4.0605e-07 - accuracy: 1.00 - ETA: 6:02 - loss: 4.0362e-07 - accuracy: 1.00 - ETA: 5:52 - loss: 4.0303e-07 - accuracy: 1.00 - ETA: 5:42 - loss: 4.0397e-07 - accuracy: 1.00 - ETA: 5:32 - loss: 4.0235e-07 - accuracy: 1.00 - ETA: 5:21 - loss: 3.9994e-07 - accuracy: 1.00 - ETA: 5:11 - loss: 4.0029e-07 - accuracy: 1.00 - ETA: 5:01 - loss: 3.9867e-07 - accuracy: 1.00 - ETA: 4:51 - loss: 3.9758e-07 - accuracy: 1.00 - ETA: 4:40 - loss: 3.9554e-07 - accuracy: 1.00 - ETA: 4:30 - loss: 3.9491e-07 - accuracy: 1.00 - ETA: 4:20 - loss: 3.9614e-07 - accuracy: 1.00 - ETA: 4:09 - loss: 3.9746e-07 - accuracy: 1.00 - ETA: 3:59 - loss: 3.9678e-07 - accuracy: 1.00 - ETA: 3:49 - loss: 3.9704e-07 - accuracy: 1.00 - ETA: 3:39 - loss: 3.9587e-07 - accuracy: 1.00 - ETA: 3:28 - loss: 3.9613e-07 - accuracy: 1.00 - ETA: 3:18 - loss: 3.9892e-07 - accuracy: 1.00 - ETA: 3:08 - loss: 3.9985e-07 - accuracy: 1.00 - ETA: 2:58 - loss: 3.9951e-07 - accuracy: 1.00 - ETA: 2:47 - loss: 3.9724e-07 - accuracy: 1.00 - ETA: 2:37 - loss: 3.9642e-07 - accuracy: 1.00 - ETA: 2:27 - loss: 3.9433e-07 - accuracy: 1.00 - ETA: 2:16 - loss: 3.9181e-07 - accuracy: 1.00 - ETA: 2:06 - loss: 3.8958e-07 - accuracy: 1.00 - ETA: 1:56 - loss: 3.9093e-07 - accuracy: 1.00 - ETA: 1:45 - loss: 3.8917e-07 - accuracy: 1.00 - ETA: 1:35 - loss: 3.9142e-07 - accuracy: 1.00 - ETA: 1:25 - loss: 3.9047e-07 - accuracy: 1.00 - ETA: 1:14 - loss: 3.9053e-07 - accuracy: 1.00 - ETA: 1:04 - loss: 3.8970e-07 - accuracy: 1.00 - ETA: 54s - loss: 3.8801e-07 - accuracy: 1.0000 - ETA: 43s - loss: 3.8619e-07 - accuracy: 1.000 - ETA: 33s - loss: 3.8459e-07 - accuracy: 1.000 - ETA: 23s - loss: 3.8820e-07 - accuracy: 1.000 - ETA: 12s - loss: 3.8607e-07 - accuracy: 1.000 - ETA: 2s - loss: 3.8466e-07 - accuracy: 1.000 - 1312s 688ms/step - loss: 3.8563e-07 - accuracy: 1.0000 - val_loss: 0.0184 - val_accuracy: 0.9880\n",
      "Epoch 24/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1908/1908 [==============================] - ETA: 19:08 - loss: 3.5060e-07 - accuracy: 1.000 - ETA: 19:28 - loss: 3.3826e-07 - accuracy: 1.000 - ETA: 19:18 - loss: 3.2428e-07 - accuracy: 1.000 - ETA: 19:10 - loss: 2.8206e-07 - accuracy: 1.000 - ETA: 18:59 - loss: 2.9256e-07 - accuracy: 1.000 - ETA: 18:48 - loss: 2.6718e-07 - accuracy: 1.000 - ETA: 18:38 - loss: 3.2126e-07 - accuracy: 1.000 - ETA: 18:32 - loss: 3.1919e-07 - accuracy: 1.000 - ETA: 18:22 - loss: 3.0605e-07 - accuracy: 1.000 - ETA: 18:18 - loss: 2.9786e-07 - accuracy: 1.000 - ETA: 18:14 - loss: 3.2391e-07 - accuracy: 1.000 - ETA: 18:06 - loss: 3.3254e-07 - accuracy: 1.000 - ETA: 17:55 - loss: 3.4317e-07 - accuracy: 1.000 - ETA: 17:44 - loss: 3.6820e-07 - accuracy: 1.000 - ETA: 17:33 - loss: 3.7920e-07 - accuracy: 1.000 - ETA: 17:22 - loss: 3.8190e-07 - accuracy: 1.000 - ETA: 17:14 - loss: 3.7313e-07 - accuracy: 1.000 - ETA: 17:06 - loss: 3.6744e-07 - accuracy: 1.000 - ETA: 16:56 - loss: 3.6615e-07 - accuracy: 1.000 - ETA: 16:46 - loss: 3.6833e-07 - accuracy: 1.000 - ETA: 16:35 - loss: 3.6487e-07 - accuracy: 1.000 - ETA: 16:24 - loss: 3.5542e-07 - accuracy: 1.000 - ETA: 16:13 - loss: 3.8004e-07 - accuracy: 1.000 - ETA: 16:03 - loss: 3.7513e-07 - accuracy: 1.000 - ETA: 15:52 - loss: 3.6917e-07 - accuracy: 1.000 - ETA: 15:41 - loss: 3.6100e-07 - accuracy: 1.000 - ETA: 15:30 - loss: 4.0680e-07 - accuracy: 1.000 - ETA: 15:19 - loss: 4.0590e-07 - accuracy: 1.000 - ETA: 15:08 - loss: 4.1343e-07 - accuracy: 1.000 - ETA: 14:58 - loss: 4.0674e-07 - accuracy: 1.000 - ETA: 14:48 - loss: 3.9813e-07 - accuracy: 1.000 - ETA: 14:39 - loss: 3.9367e-07 - accuracy: 1.000 - ETA: 14:30 - loss: 3.9101e-07 - accuracy: 1.000 - ETA: 14:20 - loss: 3.9015e-07 - accuracy: 1.000 - ETA: 14:10 - loss: 3.9176e-07 - accuracy: 1.000 - ETA: 14:01 - loss: 3.8661e-07 - accuracy: 1.000 - ETA: 13:51 - loss: 3.8327e-07 - accuracy: 1.000 - ETA: 13:42 - loss: 3.8384e-07 - accuracy: 1.000 - ETA: 13:32 - loss: 3.8828e-07 - accuracy: 1.000 - ETA: 13:22 - loss: 3.8699e-07 - accuracy: 1.000 - ETA: 13:12 - loss: 3.8427e-07 - accuracy: 1.000 - ETA: 13:01 - loss: 3.8139e-07 - accuracy: 1.000 - ETA: 12:51 - loss: 3.7909e-07 - accuracy: 1.000 - ETA: 12:41 - loss: 3.7455e-07 - accuracy: 1.000 - ETA: 12:30 - loss: 3.7457e-07 - accuracy: 1.000 - ETA: 12:20 - loss: 3.7149e-07 - accuracy: 1.000 - ETA: 12:09 - loss: 3.6820e-07 - accuracy: 1.000 - ETA: 11:59 - loss: 3.6413e-07 - accuracy: 1.000 - ETA: 11:49 - loss: 3.6222e-07 - accuracy: 1.000 - ETA: 11:38 - loss: 3.6636e-07 - accuracy: 1.000 - ETA: 11:28 - loss: 3.6510e-07 - accuracy: 1.000 - ETA: 11:18 - loss: 3.6380e-07 - accuracy: 1.000 - ETA: 11:08 - loss: 3.6116e-07 - accuracy: 1.000 - ETA: 10:58 - loss: 3.6424e-07 - accuracy: 1.000 - ETA: 10:48 - loss: 3.6342e-07 - accuracy: 1.000 - ETA: 10:38 - loss: 3.5970e-07 - accuracy: 1.000 - ETA: 10:28 - loss: 3.7323e-07 - accuracy: 1.000 - ETA: 10:18 - loss: 3.6991e-07 - accuracy: 1.000 - ETA: 10:08 - loss: 3.6758e-07 - accuracy: 1.000 - ETA: 9:58 - loss: 3.6617e-07 - accuracy: 1.000 - ETA: 9:48 - loss: 3.6450e-07 - accuracy: 1.00 - ETA: 9:38 - loss: 3.6308e-07 - accuracy: 1.00 - ETA: 9:28 - loss: 3.6122e-07 - accuracy: 1.00 - ETA: 9:18 - loss: 3.6117e-07 - accuracy: 1.00 - ETA: 9:08 - loss: 3.5996e-07 - accuracy: 1.00 - ETA: 8:57 - loss: 3.5967e-07 - accuracy: 1.00 - ETA: 8:47 - loss: 3.6727e-07 - accuracy: 1.00 - ETA: 8:37 - loss: 3.6674e-07 - accuracy: 1.00 - ETA: 8:27 - loss: 3.6365e-07 - accuracy: 1.00 - ETA: 8:17 - loss: 3.6368e-07 - accuracy: 1.00 - ETA: 8:07 - loss: 3.6100e-07 - accuracy: 1.00 - ETA: 7:56 - loss: 3.6279e-07 - accuracy: 1.00 - ETA: 7:46 - loss: 3.6429e-07 - accuracy: 1.00 - ETA: 7:36 - loss: 3.6534e-07 - accuracy: 1.00 - ETA: 7:27 - loss: 3.6193e-07 - accuracy: 1.00 - ETA: 7:17 - loss: 3.6166e-07 - accuracy: 1.00 - ETA: 7:07 - loss: 3.6103e-07 - accuracy: 1.00 - ETA: 6:57 - loss: 3.6193e-07 - accuracy: 1.00 - ETA: 6:47 - loss: 3.6017e-07 - accuracy: 1.00 - ETA: 6:37 - loss: 3.6525e-07 - accuracy: 1.00 - ETA: 6:27 - loss: 3.6938e-07 - accuracy: 1.00 - ETA: 6:17 - loss: 3.7021e-07 - accuracy: 1.00 - ETA: 6:07 - loss: 3.6840e-07 - accuracy: 1.00 - ETA: 5:57 - loss: 3.6796e-07 - accuracy: 1.00 - ETA: 5:46 - loss: 3.6686e-07 - accuracy: 1.00 - ETA: 5:36 - loss: 3.7144e-07 - accuracy: 1.00 - ETA: 5:26 - loss: 3.6870e-07 - accuracy: 1.00 - ETA: 5:16 - loss: 3.6700e-07 - accuracy: 1.00 - ETA: 5:06 - loss: 3.6761e-07 - accuracy: 1.00 - ETA: 4:56 - loss: 3.6565e-07 - accuracy: 1.00 - ETA: 4:46 - loss: 3.6345e-07 - accuracy: 1.00 - ETA: 4:36 - loss: 3.6342e-07 - accuracy: 1.00 - ETA: 4:25 - loss: 3.6310e-07 - accuracy: 1.00 - ETA: 4:15 - loss: 3.6251e-07 - accuracy: 1.00 - ETA: 4:05 - loss: 3.6135e-07 - accuracy: 1.00 - ETA: 3:55 - loss: 3.5909e-07 - accuracy: 1.00 - ETA: 3:45 - loss: 3.5766e-07 - accuracy: 1.00 - ETA: 3:35 - loss: 3.5548e-07 - accuracy: 1.00 - ETA: 3:25 - loss: 3.5308e-07 - accuracy: 1.00 - ETA: 3:15 - loss: 3.5120e-07 - accuracy: 1.00 - ETA: 3:05 - loss: 3.5317e-07 - accuracy: 1.00 - ETA: 2:55 - loss: 3.5318e-07 - accuracy: 1.00 - ETA: 2:45 - loss: 3.5144e-07 - accuracy: 1.00 - ETA: 2:35 - loss: 3.5210e-07 - accuracy: 1.00 - ETA: 2:25 - loss: 3.5211e-07 - accuracy: 1.00 - ETA: 2:15 - loss: 3.5261e-07 - accuracy: 1.00 - ETA: 2:04 - loss: 3.5165e-07 - accuracy: 1.00 - ETA: 1:54 - loss: 3.5247e-07 - accuracy: 1.00 - ETA: 1:44 - loss: 3.5340e-07 - accuracy: 1.00 - ETA: 1:34 - loss: 3.5372e-07 - accuracy: 1.00 - ETA: 1:24 - loss: 3.5514e-07 - accuracy: 1.00 - ETA: 1:13 - loss: 3.5326e-07 - accuracy: 1.00 - ETA: 1:03 - loss: 3.5210e-07 - accuracy: 1.00 - ETA: 53s - loss: 3.5127e-07 - accuracy: 1.0000 - ETA: 43s - loss: 3.4924e-07 - accuracy: 1.000 - ETA: 33s - loss: 3.4726e-07 - accuracy: 1.000 - ETA: 22s - loss: 3.4694e-07 - accuracy: 1.000 - ETA: 12s - loss: 3.4540e-07 - accuracy: 1.000 - ETA: 2s - loss: 3.4507e-07 - accuracy: 1.000 - 1296s 679ms/step - loss: 3.4519e-07 - accuracy: 1.0000 - val_loss: 0.0186 - val_accuracy: 0.9880\n",
      "Epoch 25/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1908/1908 [==============================] - ETA: 19:40 - loss: 3.7813e-07 - accuracy: 1.000 - ETA: 19:36 - loss: 4.2206e-07 - accuracy: 1.000 - ETA: 19:27 - loss: 3.8642e-07 - accuracy: 1.000 - ETA: 19:07 - loss: 3.3970e-07 - accuracy: 1.000 - ETA: 19:03 - loss: 3.1946e-07 - accuracy: 1.000 - ETA: 18:53 - loss: 3.2263e-07 - accuracy: 1.000 - ETA: 18:50 - loss: 4.0617e-07 - accuracy: 1.000 - ETA: 18:47 - loss: 3.8036e-07 - accuracy: 1.000 - ETA: 18:40 - loss: 3.5828e-07 - accuracy: 1.000 - ETA: 18:30 - loss: 3.3664e-07 - accuracy: 1.000 - ETA: 18:19 - loss: 3.5100e-07 - accuracy: 1.000 - ETA: 18:06 - loss: 3.3966e-07 - accuracy: 1.000 - ETA: 17:53 - loss: 3.3353e-07 - accuracy: 1.000 - ETA: 17:41 - loss: 3.1953e-07 - accuracy: 1.000 - ETA: 17:31 - loss: 3.3009e-07 - accuracy: 1.000 - ETA: 17:20 - loss: 3.2519e-07 - accuracy: 1.000 - ETA: 17:09 - loss: 3.1667e-07 - accuracy: 1.000 - ETA: 16:57 - loss: 3.0725e-07 - accuracy: 1.000 - ETA: 16:47 - loss: 3.0186e-07 - accuracy: 1.000 - ETA: 16:36 - loss: 3.1380e-07 - accuracy: 1.000 - ETA: 16:26 - loss: 3.1020e-07 - accuracy: 1.000 - ETA: 16:19 - loss: 3.0925e-07 - accuracy: 1.000 - ETA: 16:10 - loss: 3.0533e-07 - accuracy: 1.000 - ETA: 16:00 - loss: 3.0328e-07 - accuracy: 1.000 - ETA: 15:49 - loss: 3.0086e-07 - accuracy: 1.000 - ETA: 15:41 - loss: 2.9523e-07 - accuracy: 1.000 - ETA: 15:31 - loss: 3.0563e-07 - accuracy: 1.000 - ETA: 15:22 - loss: 3.0086e-07 - accuracy: 1.000 - ETA: 15:12 - loss: 2.9440e-07 - accuracy: 1.000 - ETA: 15:03 - loss: 2.9439e-07 - accuracy: 1.000 - ETA: 14:53 - loss: 2.9742e-07 - accuracy: 1.000 - ETA: 14:43 - loss: 3.0366e-07 - accuracy: 1.000 - ETA: 14:32 - loss: 3.0624e-07 - accuracy: 1.000 - ETA: 14:23 - loss: 3.1840e-07 - accuracy: 1.000 - ETA: 14:12 - loss: 3.1700e-07 - accuracy: 1.000 - ETA: 14:02 - loss: 3.1438e-07 - accuracy: 1.000 - ETA: 13:52 - loss: 3.1024e-07 - accuracy: 1.000 - ETA: 13:42 - loss: 3.3394e-07 - accuracy: 1.000 - ETA: 13:32 - loss: 3.3099e-07 - accuracy: 1.000 - ETA: 13:21 - loss: 3.3137e-07 - accuracy: 1.000 - ETA: 13:11 - loss: 3.3045e-07 - accuracy: 1.000 - ETA: 13:01 - loss: 3.2992e-07 - accuracy: 1.000 - ETA: 12:51 - loss: 3.2447e-07 - accuracy: 1.000 - ETA: 12:41 - loss: 3.2382e-07 - accuracy: 1.000 - ETA: 12:31 - loss: 3.2651e-07 - accuracy: 1.000 - ETA: 12:21 - loss: 3.3122e-07 - accuracy: 1.000 - ETA: 12:11 - loss: 3.2964e-07 - accuracy: 1.000 - ETA: 12:01 - loss: 3.3231e-07 - accuracy: 1.000 - ETA: 11:50 - loss: 3.3626e-07 - accuracy: 1.000 - ETA: 11:40 - loss: 3.3333e-07 - accuracy: 1.000 - ETA: 11:30 - loss: 3.2850e-07 - accuracy: 1.000 - ETA: 11:21 - loss: 3.2450e-07 - accuracy: 1.000 - ETA: 11:11 - loss: 3.2477e-07 - accuracy: 1.000 - ETA: 11:00 - loss: 3.2714e-07 - accuracy: 1.000 - ETA: 10:50 - loss: 3.2587e-07 - accuracy: 1.000 - ETA: 10:39 - loss: 3.2622e-07 - accuracy: 1.000 - ETA: 10:29 - loss: 3.2698e-07 - accuracy: 1.000 - ETA: 10:19 - loss: 3.2920e-07 - accuracy: 1.000 - ETA: 10:09 - loss: 3.2640e-07 - accuracy: 1.000 - ETA: 9:58 - loss: 3.3017e-07 - accuracy: 1.000 - ETA: 9:48 - loss: 3.2909e-07 - accuracy: 1.00 - ETA: 9:38 - loss: 3.2997e-07 - accuracy: 1.00 - ETA: 9:28 - loss: 3.2955e-07 - accuracy: 1.00 - ETA: 9:17 - loss: 3.3111e-07 - accuracy: 1.00 - ETA: 9:09 - loss: 3.2967e-07 - accuracy: 1.00 - ETA: 9:02 - loss: 3.2655e-07 - accuracy: 1.00 - ETA: 8:53 - loss: 3.2514e-07 - accuracy: 1.00 - ETA: 8:44 - loss: 3.2517e-07 - accuracy: 1.00 - ETA: 8:35 - loss: 3.2429e-07 - accuracy: 1.00 - ETA: 8:25 - loss: 3.2493e-07 - accuracy: 1.00 - ETA: 8:15 - loss: 3.2401e-07 - accuracy: 1.00 - ETA: 8:05 - loss: 3.2120e-07 - accuracy: 1.00 - ETA: 7:56 - loss: 3.1927e-07 - accuracy: 1.00 - ETA: 7:46 - loss: 3.1734e-07 - accuracy: 1.00 - ETA: 7:36 - loss: 3.1772e-07 - accuracy: 1.00 - ETA: 7:28 - loss: 3.1570e-07 - accuracy: 1.00 - ETA: 7:18 - loss: 3.1537e-07 - accuracy: 1.00 - ETA: 7:07 - loss: 3.1528e-07 - accuracy: 1.00 - ETA: 6:57 - loss: 3.1367e-07 - accuracy: 1.00 - ETA: 6:47 - loss: 3.1287e-07 - accuracy: 1.00 - ETA: 6:37 - loss: 3.1183e-07 - accuracy: 1.00 - ETA: 6:27 - loss: 3.1316e-07 - accuracy: 1.00 - ETA: 6:17 - loss: 3.2871e-07 - accuracy: 1.00 - ETA: 6:06 - loss: 3.2705e-07 - accuracy: 1.00 - ETA: 5:56 - loss: 3.2570e-07 - accuracy: 1.00 - ETA: 5:46 - loss: 3.2464e-07 - accuracy: 1.00 - ETA: 5:36 - loss: 3.2297e-07 - accuracy: 1.00 - ETA: 5:26 - loss: 3.2363e-07 - accuracy: 1.00 - ETA: 5:15 - loss: 3.2381e-07 - accuracy: 1.00 - ETA: 5:05 - loss: 3.2350e-07 - accuracy: 1.00 - ETA: 4:55 - loss: 3.2329e-07 - accuracy: 1.00 - ETA: 4:45 - loss: 3.2158e-07 - accuracy: 1.00 - ETA: 4:34 - loss: 3.2070e-07 - accuracy: 1.00 - ETA: 4:24 - loss: 3.2023e-07 - accuracy: 1.00 - ETA: 4:14 - loss: 3.1851e-07 - accuracy: 1.00 - ETA: 4:03 - loss: 3.1688e-07 - accuracy: 1.00 - ETA: 3:53 - loss: 3.1564e-07 - accuracy: 1.00 - ETA: 3:42 - loss: 3.1593e-07 - accuracy: 1.00 - ETA: 3:32 - loss: 3.1584e-07 - accuracy: 1.00 - ETA: 3:21 - loss: 3.1528e-07 - accuracy: 1.00 - ETA: 3:11 - loss: 3.1609e-07 - accuracy: 1.00 - ETA: 3:00 - loss: 3.1614e-07 - accuracy: 1.00 - ETA: 2:50 - loss: 3.1523e-07 - accuracy: 1.00 - ETA: 2:39 - loss: 3.1424e-07 - accuracy: 1.00 - ETA: 2:29 - loss: 3.1302e-07 - accuracy: 1.00 - ETA: 2:19 - loss: 3.1216e-07 - accuracy: 1.00 - ETA: 2:08 - loss: 3.1426e-07 - accuracy: 1.00 - ETA: 1:58 - loss: 3.1472e-07 - accuracy: 1.00 - ETA: 1:47 - loss: 3.1465e-07 - accuracy: 1.00 - ETA: 1:37 - loss: 3.1579e-07 - accuracy: 1.00 - ETA: 1:26 - loss: 3.1434e-07 - accuracy: 1.00 - ETA: 1:16 - loss: 3.1353e-07 - accuracy: 1.00 - ETA: 1:05 - loss: 3.1290e-07 - accuracy: 1.00 - ETA: 55s - loss: 3.1250e-07 - accuracy: 1.0000 - ETA: 44s - loss: 3.1209e-07 - accuracy: 1.000 - ETA: 34s - loss: 3.1246e-07 - accuracy: 1.000 - ETA: 23s - loss: 3.1298e-07 - accuracy: 1.000 - ETA: 13s - loss: 3.1262e-07 - accuracy: 1.000 - ETA: 2s - loss: 3.1135e-07 - accuracy: 1.000 - 1331s 698ms/step - loss: 3.1114e-07 - accuracy: 1.0000 - val_loss: 0.0187 - val_accuracy: 0.9880\n",
      "Epoch 26/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1908/1908 [==============================] - ETA: 19:07 - loss: 3.9666e-07 - accuracy: 1.000 - ETA: 19:19 - loss: 3.0755e-07 - accuracy: 1.000 - ETA: 19:24 - loss: 2.9408e-07 - accuracy: 1.000 - ETA: 19:14 - loss: 3.4148e-07 - accuracy: 1.000 - ETA: 19:05 - loss: 3.1981e-07 - accuracy: 1.000 - ETA: 18:55 - loss: 3.0413e-07 - accuracy: 1.000 - ETA: 18:54 - loss: 3.6420e-07 - accuracy: 1.000 - ETA: 18:47 - loss: 3.4953e-07 - accuracy: 1.000 - ETA: 18:42 - loss: 3.2749e-07 - accuracy: 1.000 - ETA: 18:37 - loss: 3.1547e-07 - accuracy: 1.000 - ETA: 18:28 - loss: 3.2439e-07 - accuracy: 1.000 - ETA: 18:17 - loss: 3.3855e-07 - accuracy: 1.000 - ETA: 18:10 - loss: 3.3620e-07 - accuracy: 1.000 - ETA: 18:06 - loss: 3.3921e-07 - accuracy: 1.000 - ETA: 17:57 - loss: 3.2735e-07 - accuracy: 1.000 - ETA: 17:46 - loss: 3.1817e-07 - accuracy: 1.000 - ETA: 17:38 - loss: 3.1332e-07 - accuracy: 1.000 - ETA: 17:28 - loss: 3.0577e-07 - accuracy: 1.000 - ETA: 17:17 - loss: 3.0460e-07 - accuracy: 1.000 - ETA: 17:08 - loss: 3.0697e-07 - accuracy: 1.000 - ETA: 16:57 - loss: 3.0590e-07 - accuracy: 1.000 - ETA: 16:45 - loss: 2.9648e-07 - accuracy: 1.000 - ETA: 16:35 - loss: 2.8691e-07 - accuracy: 1.000 - ETA: 16:26 - loss: 2.8314e-07 - accuracy: 1.000 - ETA: 16:18 - loss: 2.7776e-07 - accuracy: 1.000 - ETA: 16:06 - loss: 2.7514e-07 - accuracy: 1.000 - ETA: 15:54 - loss: 2.7493e-07 - accuracy: 1.000 - ETA: 15:43 - loss: 2.6925e-07 - accuracy: 1.000 - ETA: 15:31 - loss: 2.6394e-07 - accuracy: 1.000 - ETA: 15:20 - loss: 2.6228e-07 - accuracy: 1.000 - ETA: 15:09 - loss: 2.6105e-07 - accuracy: 1.000 - ETA: 14:58 - loss: 2.7634e-07 - accuracy: 1.000 - ETA: 14:47 - loss: 2.7254e-07 - accuracy: 1.000 - ETA: 14:37 - loss: 2.7070e-07 - accuracy: 1.000 - ETA: 14:25 - loss: 2.6782e-07 - accuracy: 1.000 - ETA: 14:14 - loss: 2.7332e-07 - accuracy: 1.000 - ETA: 14:04 - loss: 2.7156e-07 - accuracy: 1.000 - ETA: 13:54 - loss: 2.7680e-07 - accuracy: 1.000 - ETA: 13:43 - loss: 2.7256e-07 - accuracy: 1.000 - ETA: 13:33 - loss: 2.7640e-07 - accuracy: 1.000 - ETA: 13:22 - loss: 2.7753e-07 - accuracy: 1.000 - ETA: 13:11 - loss: 2.7805e-07 - accuracy: 1.000 - ETA: 13:00 - loss: 2.7610e-07 - accuracy: 1.000 - ETA: 12:49 - loss: 2.7616e-07 - accuracy: 1.000 - ETA: 12:39 - loss: 2.8857e-07 - accuracy: 1.000 - ETA: 12:28 - loss: 2.8789e-07 - accuracy: 1.000 - ETA: 12:17 - loss: 2.8531e-07 - accuracy: 1.000 - ETA: 12:07 - loss: 2.8334e-07 - accuracy: 1.000 - ETA: 11:56 - loss: 2.8348e-07 - accuracy: 1.000 - ETA: 11:45 - loss: 2.8187e-07 - accuracy: 1.000 - ETA: 11:35 - loss: 2.8072e-07 - accuracy: 1.000 - ETA: 11:24 - loss: 2.8040e-07 - accuracy: 1.000 - ETA: 11:14 - loss: 2.7768e-07 - accuracy: 1.000 - ETA: 11:04 - loss: 2.7776e-07 - accuracy: 1.000 - ETA: 10:54 - loss: 2.7992e-07 - accuracy: 1.000 - ETA: 10:43 - loss: 2.7673e-07 - accuracy: 1.000 - ETA: 10:33 - loss: 2.7586e-07 - accuracy: 1.000 - ETA: 10:23 - loss: 2.7545e-07 - accuracy: 1.000 - ETA: 10:12 - loss: 2.7701e-07 - accuracy: 1.000 - ETA: 10:01 - loss: 2.7639e-07 - accuracy: 1.000 - ETA: 9:51 - loss: 2.7524e-07 - accuracy: 1.000 - ETA: 9:40 - loss: 2.7264e-07 - accuracy: 1.00 - ETA: 9:30 - loss: 2.7477e-07 - accuracy: 1.00 - ETA: 9:19 - loss: 2.7215e-07 - accuracy: 1.00 - ETA: 9:09 - loss: 2.7125e-07 - accuracy: 1.00 - ETA: 8:59 - loss: 2.6913e-07 - accuracy: 1.00 - ETA: 8:48 - loss: 2.7293e-07 - accuracy: 1.00 - ETA: 8:38 - loss: 2.7432e-07 - accuracy: 1.00 - ETA: 8:28 - loss: 2.7593e-07 - accuracy: 1.00 - ETA: 8:18 - loss: 2.7697e-07 - accuracy: 1.00 - ETA: 8:08 - loss: 2.7501e-07 - accuracy: 1.00 - ETA: 7:58 - loss: 2.7340e-07 - accuracy: 1.00 - ETA: 7:47 - loss: 2.7252e-07 - accuracy: 1.00 - ETA: 7:38 - loss: 2.7330e-07 - accuracy: 1.00 - ETA: 7:27 - loss: 2.7338e-07 - accuracy: 1.00 - ETA: 7:17 - loss: 2.7359e-07 - accuracy: 1.00 - ETA: 7:07 - loss: 2.7280e-07 - accuracy: 1.00 - ETA: 6:57 - loss: 2.7375e-07 - accuracy: 1.00 - ETA: 6:47 - loss: 2.7452e-07 - accuracy: 1.00 - ETA: 6:37 - loss: 2.7247e-07 - accuracy: 1.00 - ETA: 6:26 - loss: 2.7216e-07 - accuracy: 1.00 - ETA: 6:16 - loss: 2.7497e-07 - accuracy: 1.00 - ETA: 6:06 - loss: 2.7461e-07 - accuracy: 1.00 - ETA: 5:56 - loss: 2.7324e-07 - accuracy: 1.00 - ETA: 5:45 - loss: 2.7195e-07 - accuracy: 1.00 - ETA: 5:35 - loss: 2.7253e-07 - accuracy: 1.00 - ETA: 5:25 - loss: 2.7111e-07 - accuracy: 1.00 - ETA: 5:15 - loss: 2.6999e-07 - accuracy: 1.00 - ETA: 5:05 - loss: 2.6939e-07 - accuracy: 1.00 - ETA: 4:55 - loss: 2.7012e-07 - accuracy: 1.00 - ETA: 4:45 - loss: 2.7114e-07 - accuracy: 1.00 - ETA: 4:34 - loss: 2.7034e-07 - accuracy: 1.00 - ETA: 4:24 - loss: 2.7034e-07 - accuracy: 1.00 - ETA: 4:14 - loss: 2.7012e-07 - accuracy: 1.00 - ETA: 4:04 - loss: 2.6974e-07 - accuracy: 1.00 - ETA: 3:54 - loss: 2.7005e-07 - accuracy: 1.00 - ETA: 3:44 - loss: 2.6905e-07 - accuracy: 1.00 - ETA: 3:34 - loss: 2.7038e-07 - accuracy: 1.00 - ETA: 3:24 - loss: 2.7029e-07 - accuracy: 1.00 - ETA: 3:14 - loss: 2.6973e-07 - accuracy: 1.00 - ETA: 3:04 - loss: 2.6961e-07 - accuracy: 1.00 - ETA: 2:53 - loss: 2.7012e-07 - accuracy: 1.00 - ETA: 2:43 - loss: 2.7231e-07 - accuracy: 1.00 - ETA: 2:33 - loss: 2.7120e-07 - accuracy: 1.00 - ETA: 2:23 - loss: 2.7383e-07 - accuracy: 1.00 - ETA: 2:13 - loss: 2.7292e-07 - accuracy: 1.00 - ETA: 2:03 - loss: 2.7311e-07 - accuracy: 1.00 - ETA: 1:53 - loss: 2.7279e-07 - accuracy: 1.00 - ETA: 1:43 - loss: 2.8430e-07 - accuracy: 1.00 - ETA: 1:33 - loss: 2.8331e-07 - accuracy: 1.00 - ETA: 1:23 - loss: 2.8340e-07 - accuracy: 1.00 - ETA: 1:13 - loss: 2.8279e-07 - accuracy: 1.00 - ETA: 1:03 - loss: 2.8160e-07 - accuracy: 1.00 - ETA: 53s - loss: 2.8085e-07 - accuracy: 1.0000 - ETA: 43s - loss: 2.8225e-07 - accuracy: 1.000 - ETA: 32s - loss: 2.8134e-07 - accuracy: 1.000 - ETA: 22s - loss: 2.8048e-07 - accuracy: 1.000 - ETA: 12s - loss: 2.8220e-07 - accuracy: 1.000 - ETA: 2s - loss: 2.8232e-07 - accuracy: 1.000 - 1296s 679ms/step - loss: 2.8205e-07 - accuracy: 1.0000 - val_loss: 0.0189 - val_accuracy: 0.9878\n",
      "Epoch 27/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1908/1908 [==============================] - ETA: 21:57 - loss: 1.1874e-07 - accuracy: 1.000 - ETA: 21:20 - loss: 2.0994e-07 - accuracy: 1.000 - ETA: 21:05 - loss: 2.0041e-07 - accuracy: 1.000 - ETA: 20:57 - loss: 1.9954e-07 - accuracy: 1.000 - ETA: 20:51 - loss: 2.0877e-07 - accuracy: 1.000 - ETA: 20:47 - loss: 1.9317e-07 - accuracy: 1.000 - ETA: 20:49 - loss: 1.8752e-07 - accuracy: 1.000 - ETA: 20:33 - loss: 1.9537e-07 - accuracy: 1.000 - ETA: 20:14 - loss: 1.8227e-07 - accuracy: 1.000 - ETA: 19:53 - loss: 1.7828e-07 - accuracy: 1.000 - ETA: 19:36 - loss: 1.7565e-07 - accuracy: 1.000 - ETA: 19:18 - loss: 1.8070e-07 - accuracy: 1.000 - ETA: 19:01 - loss: 1.8415e-07 - accuracy: 1.000 - ETA: 18:44 - loss: 2.1388e-07 - accuracy: 1.000 - ETA: 18:32 - loss: 2.2184e-07 - accuracy: 1.000 - ETA: 18:18 - loss: 2.2027e-07 - accuracy: 1.000 - ETA: 18:05 - loss: 2.2128e-07 - accuracy: 1.000 - ETA: 17:49 - loss: 2.2185e-07 - accuracy: 1.000 - ETA: 17:40 - loss: 2.2198e-07 - accuracy: 1.000 - ETA: 17:26 - loss: 2.7164e-07 - accuracy: 1.000 - ETA: 17:14 - loss: 2.6558e-07 - accuracy: 1.000 - ETA: 16:59 - loss: 2.6655e-07 - accuracy: 1.000 - ETA: 16:47 - loss: 2.9080e-07 - accuracy: 1.000 - ETA: 16:36 - loss: 2.8332e-07 - accuracy: 1.000 - ETA: 16:23 - loss: 2.8779e-07 - accuracy: 1.000 - ETA: 16:10 - loss: 2.8977e-07 - accuracy: 1.000 - ETA: 16:00 - loss: 2.9671e-07 - accuracy: 1.000 - ETA: 15:48 - loss: 2.8973e-07 - accuracy: 1.000 - ETA: 15:37 - loss: 2.9220e-07 - accuracy: 1.000 - ETA: 15:25 - loss: 2.8772e-07 - accuracy: 1.000 - ETA: 15:14 - loss: 2.8405e-07 - accuracy: 1.000 - ETA: 15:02 - loss: 2.8073e-07 - accuracy: 1.000 - ETA: 14:50 - loss: 2.7891e-07 - accuracy: 1.000 - ETA: 14:39 - loss: 2.9360e-07 - accuracy: 1.000 - ETA: 14:27 - loss: 2.8943e-07 - accuracy: 1.000 - ETA: 14:16 - loss: 2.9980e-07 - accuracy: 1.000 - ETA: 14:05 - loss: 2.9879e-07 - accuracy: 1.000 - ETA: 13:53 - loss: 2.9304e-07 - accuracy: 1.000 - ETA: 13:42 - loss: 2.9285e-07 - accuracy: 1.000 - ETA: 13:32 - loss: 2.9571e-07 - accuracy: 1.000 - ETA: 13:23 - loss: 2.9308e-07 - accuracy: 1.000 - ETA: 13:13 - loss: 2.9148e-07 - accuracy: 1.000 - ETA: 13:02 - loss: 2.9054e-07 - accuracy: 1.000 - ETA: 12:52 - loss: 2.9244e-07 - accuracy: 1.000 - ETA: 12:42 - loss: 2.9116e-07 - accuracy: 1.000 - ETA: 12:31 - loss: 2.9074e-07 - accuracy: 1.000 - ETA: 12:20 - loss: 2.8961e-07 - accuracy: 1.000 - ETA: 12:10 - loss: 2.8884e-07 - accuracy: 1.000 - ETA: 11:59 - loss: 2.8895e-07 - accuracy: 1.000 - ETA: 11:48 - loss: 2.8572e-07 - accuracy: 1.000 - ETA: 11:37 - loss: 2.8374e-07 - accuracy: 1.000 - ETA: 11:27 - loss: 2.8277e-07 - accuracy: 1.000 - ETA: 11:15 - loss: 2.8176e-07 - accuracy: 1.000 - ETA: 11:04 - loss: 2.7927e-07 - accuracy: 1.000 - ETA: 10:54 - loss: 2.7971e-07 - accuracy: 1.000 - ETA: 10:43 - loss: 2.7746e-07 - accuracy: 1.000 - ETA: 10:33 - loss: 2.7751e-07 - accuracy: 1.000 - ETA: 10:23 - loss: 2.7509e-07 - accuracy: 1.000 - ETA: 10:13 - loss: 2.7401e-07 - accuracy: 1.000 - ETA: 10:03 - loss: 2.7637e-07 - accuracy: 1.000 - ETA: 9:52 - loss: 2.7492e-07 - accuracy: 1.000 - ETA: 9:42 - loss: 2.7453e-07 - accuracy: 1.00 - ETA: 9:32 - loss: 2.7455e-07 - accuracy: 1.00 - ETA: 9:21 - loss: 2.7386e-07 - accuracy: 1.00 - ETA: 9:11 - loss: 2.7211e-07 - accuracy: 1.00 - ETA: 9:01 - loss: 2.7127e-07 - accuracy: 1.00 - ETA: 8:51 - loss: 2.6913e-07 - accuracy: 1.00 - ETA: 8:41 - loss: 2.6855e-07 - accuracy: 1.00 - ETA: 8:31 - loss: 2.6843e-07 - accuracy: 1.00 - ETA: 8:20 - loss: 2.6815e-07 - accuracy: 1.00 - ETA: 8:10 - loss: 2.6881e-07 - accuracy: 1.00 - ETA: 8:01 - loss: 2.6876e-07 - accuracy: 1.00 - ETA: 7:51 - loss: 2.6964e-07 - accuracy: 1.00 - ETA: 7:41 - loss: 2.7026e-07 - accuracy: 1.00 - ETA: 7:31 - loss: 2.7049e-07 - accuracy: 1.00 - ETA: 7:21 - loss: 2.7102e-07 - accuracy: 1.00 - ETA: 7:11 - loss: 2.7042e-07 - accuracy: 1.00 - ETA: 7:01 - loss: 2.6899e-07 - accuracy: 1.00 - ETA: 6:51 - loss: 2.6986e-07 - accuracy: 1.00 - ETA: 6:41 - loss: 2.6957e-07 - accuracy: 1.00 - ETA: 6:30 - loss: 2.6733e-07 - accuracy: 1.00 - ETA: 6:20 - loss: 2.6662e-07 - accuracy: 1.00 - ETA: 6:10 - loss: 2.6625e-07 - accuracy: 1.00 - ETA: 5:59 - loss: 2.6594e-07 - accuracy: 1.00 - ETA: 5:49 - loss: 2.6454e-07 - accuracy: 1.00 - ETA: 5:39 - loss: 2.6346e-07 - accuracy: 1.00 - ETA: 5:29 - loss: 2.6556e-07 - accuracy: 1.00 - ETA: 5:18 - loss: 2.6418e-07 - accuracy: 1.00 - ETA: 5:08 - loss: 2.6436e-07 - accuracy: 1.00 - ETA: 4:58 - loss: 2.6284e-07 - accuracy: 1.00 - ETA: 4:48 - loss: 2.6272e-07 - accuracy: 1.00 - ETA: 4:37 - loss: 2.6118e-07 - accuracy: 1.00 - ETA: 4:27 - loss: 2.6098e-07 - accuracy: 1.00 - ETA: 4:17 - loss: 2.6058e-07 - accuracy: 1.00 - ETA: 4:06 - loss: 2.6065e-07 - accuracy: 1.00 - ETA: 3:56 - loss: 2.6057e-07 - accuracy: 1.00 - ETA: 3:46 - loss: 2.6174e-07 - accuracy: 1.00 - ETA: 3:36 - loss: 2.6097e-07 - accuracy: 1.00 - ETA: 3:25 - loss: 2.6255e-07 - accuracy: 1.00 - ETA: 3:15 - loss: 2.6214e-07 - accuracy: 1.00 - ETA: 3:05 - loss: 2.6153e-07 - accuracy: 1.00 - ETA: 2:55 - loss: 2.6067e-07 - accuracy: 1.00 - ETA: 2:45 - loss: 2.5908e-07 - accuracy: 1.00 - ETA: 2:34 - loss: 2.5947e-07 - accuracy: 1.00 - ETA: 2:24 - loss: 2.5868e-07 - accuracy: 1.00 - ETA: 2:14 - loss: 2.5808e-07 - accuracy: 1.00 - ETA: 2:04 - loss: 2.5922e-07 - accuracy: 1.00 - ETA: 1:54 - loss: 2.5883e-07 - accuracy: 1.00 - ETA: 1:44 - loss: 2.5898e-07 - accuracy: 1.00 - ETA: 1:33 - loss: 2.5755e-07 - accuracy: 1.00 - ETA: 1:23 - loss: 2.5705e-07 - accuracy: 1.00 - ETA: 1:13 - loss: 2.5656e-07 - accuracy: 1.00 - ETA: 1:03 - loss: 2.5631e-07 - accuracy: 1.00 - ETA: 53s - loss: 2.5775e-07 - accuracy: 1.0000 - ETA: 43s - loss: 2.5792e-07 - accuracy: 1.000 - ETA: 32s - loss: 2.5718e-07 - accuracy: 1.000 - ETA: 22s - loss: 2.5602e-07 - accuracy: 1.000 - ETA: 12s - loss: 2.5495e-07 - accuracy: 1.000 - ETA: 2s - loss: 2.5584e-07 - accuracy: 1.000 - 1288s 675ms/step - loss: 2.5676e-07 - accuracy: 1.0000 - val_loss: 0.0190 - val_accuracy: 0.9877\n",
      "Epoch 28/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1908/1908 [==============================] - ETA: 20:21 - loss: 1.7840e-07 - accuracy: 1.000 - ETA: 20:16 - loss: 2.9161e-07 - accuracy: 1.000 - ETA: 20:03 - loss: 2.4691e-07 - accuracy: 1.000 - ETA: 19:43 - loss: 2.4207e-07 - accuracy: 1.000 - ETA: 19:24 - loss: 2.5325e-07 - accuracy: 1.000 - ETA: 19:07 - loss: 2.4598e-07 - accuracy: 1.000 - ETA: 18:56 - loss: 2.4489e-07 - accuracy: 1.000 - ETA: 18:49 - loss: 2.4145e-07 - accuracy: 1.000 - ETA: 18:32 - loss: 2.4975e-07 - accuracy: 1.000 - ETA: 18:23 - loss: 2.4237e-07 - accuracy: 1.000 - ETA: 18:11 - loss: 2.3253e-07 - accuracy: 1.000 - ETA: 18:00 - loss: 2.3855e-07 - accuracy: 1.000 - ETA: 17:48 - loss: 2.4446e-07 - accuracy: 1.000 - ETA: 17:39 - loss: 2.3583e-07 - accuracy: 1.000 - ETA: 17:30 - loss: 2.2562e-07 - accuracy: 1.000 - ETA: 17:19 - loss: 2.3140e-07 - accuracy: 1.000 - ETA: 17:08 - loss: 2.2727e-07 - accuracy: 1.000 - ETA: 16:56 - loss: 2.2578e-07 - accuracy: 1.000 - ETA: 16:47 - loss: 2.2728e-07 - accuracy: 1.000 - ETA: 16:38 - loss: 2.2792e-07 - accuracy: 1.000 - ETA: 16:28 - loss: 2.2483e-07 - accuracy: 1.000 - ETA: 16:18 - loss: 2.2533e-07 - accuracy: 1.000 - ETA: 16:07 - loss: 2.3098e-07 - accuracy: 1.000 - ETA: 15:56 - loss: 2.2653e-07 - accuracy: 1.000 - ETA: 15:45 - loss: 2.2496e-07 - accuracy: 1.000 - ETA: 15:33 - loss: 2.2201e-07 - accuracy: 1.000 - ETA: 15:23 - loss: 2.2171e-07 - accuracy: 1.000 - ETA: 15:12 - loss: 2.1825e-07 - accuracy: 1.000 - ETA: 15:02 - loss: 2.1511e-07 - accuracy: 1.000 - ETA: 14:51 - loss: 2.1256e-07 - accuracy: 1.000 - ETA: 14:41 - loss: 2.1518e-07 - accuracy: 1.000 - ETA: 14:30 - loss: 2.1671e-07 - accuracy: 1.000 - ETA: 14:20 - loss: 2.1532e-07 - accuracy: 1.000 - ETA: 14:12 - loss: 2.1343e-07 - accuracy: 1.000 - ETA: 14:05 - loss: 2.1399e-07 - accuracy: 1.000 - ETA: 13:57 - loss: 2.1271e-07 - accuracy: 1.000 - ETA: 13:49 - loss: 2.1318e-07 - accuracy: 1.000 - ETA: 13:41 - loss: 2.1559e-07 - accuracy: 1.000 - ETA: 13:33 - loss: 2.1588e-07 - accuracy: 1.000 - ETA: 13:24 - loss: 2.1540e-07 - accuracy: 1.000 - ETA: 13:17 - loss: 2.2576e-07 - accuracy: 1.000 - ETA: 13:08 - loss: 2.2612e-07 - accuracy: 1.000 - ETA: 12:58 - loss: 2.2500e-07 - accuracy: 1.000 - ETA: 12:48 - loss: 2.2459e-07 - accuracy: 1.000 - ETA: 12:39 - loss: 2.2255e-07 - accuracy: 1.000 - ETA: 12:29 - loss: 2.2541e-07 - accuracy: 1.000 - ETA: 12:18 - loss: 2.2388e-07 - accuracy: 1.000 - ETA: 12:09 - loss: 2.2458e-07 - accuracy: 1.000 - ETA: 11:59 - loss: 2.2269e-07 - accuracy: 1.000 - ETA: 11:49 - loss: 2.2179e-07 - accuracy: 1.000 - ETA: 11:42 - loss: 2.2464e-07 - accuracy: 1.000 - ETA: 11:32 - loss: 2.2213e-07 - accuracy: 1.000 - ETA: 11:22 - loss: 2.2929e-07 - accuracy: 1.000 - ETA: 11:12 - loss: 2.2677e-07 - accuracy: 1.000 - ETA: 11:02 - loss: 2.2513e-07 - accuracy: 1.000 - ETA: 10:53 - loss: 2.3359e-07 - accuracy: 1.000 - ETA: 10:43 - loss: 2.3373e-07 - accuracy: 1.000 - ETA: 10:33 - loss: 2.3320e-07 - accuracy: 1.000 - ETA: 10:23 - loss: 2.3401e-07 - accuracy: 1.000 - ETA: 10:13 - loss: 2.3417e-07 - accuracy: 1.000 - ETA: 10:02 - loss: 2.3384e-07 - accuracy: 1.000 - ETA: 9:52 - loss: 2.3318e-07 - accuracy: 1.000 - ETA: 9:42 - loss: 2.3289e-07 - accuracy: 1.00 - ETA: 9:33 - loss: 2.3214e-07 - accuracy: 1.00 - ETA: 9:24 - loss: 2.3203e-07 - accuracy: 1.00 - ETA: 9:14 - loss: 2.3331e-07 - accuracy: 1.00 - ETA: 9:03 - loss: 2.3551e-07 - accuracy: 1.00 - ETA: 8:53 - loss: 2.3383e-07 - accuracy: 1.00 - ETA: 8:43 - loss: 2.3199e-07 - accuracy: 1.00 - ETA: 8:33 - loss: 2.3127e-07 - accuracy: 1.00 - ETA: 8:23 - loss: 2.3625e-07 - accuracy: 1.00 - ETA: 8:13 - loss: 2.3663e-07 - accuracy: 1.00 - ETA: 8:02 - loss: 2.3534e-07 - accuracy: 1.00 - ETA: 7:52 - loss: 2.3361e-07 - accuracy: 1.00 - ETA: 7:42 - loss: 2.3250e-07 - accuracy: 1.00 - ETA: 7:32 - loss: 2.3143e-07 - accuracy: 1.00 - ETA: 7:22 - loss: 2.3254e-07 - accuracy: 1.00 - ETA: 7:11 - loss: 2.3130e-07 - accuracy: 1.00 - ETA: 7:00 - loss: 2.3526e-07 - accuracy: 1.00 - ETA: 6:50 - loss: 2.3543e-07 - accuracy: 1.00 - ETA: 6:39 - loss: 2.3621e-07 - accuracy: 1.00 - ETA: 6:29 - loss: 2.3561e-07 - accuracy: 1.00 - ETA: 6:18 - loss: 2.3440e-07 - accuracy: 1.00 - ETA: 6:08 - loss: 2.3278e-07 - accuracy: 1.00 - ETA: 5:57 - loss: 2.3383e-07 - accuracy: 1.00 - ETA: 5:47 - loss: 2.3197e-07 - accuracy: 1.00 - ETA: 5:36 - loss: 2.3275e-07 - accuracy: 1.00 - ETA: 5:25 - loss: 2.3133e-07 - accuracy: 1.00 - ETA: 5:15 - loss: 2.3266e-07 - accuracy: 1.00 - ETA: 5:04 - loss: 2.3354e-07 - accuracy: 1.00 - ETA: 4:54 - loss: 2.3267e-07 - accuracy: 1.00 - ETA: 4:43 - loss: 2.3321e-07 - accuracy: 1.00 - ETA: 4:33 - loss: 2.3238e-07 - accuracy: 1.00 - ETA: 4:22 - loss: 2.3126e-07 - accuracy: 1.00 - ETA: 4:12 - loss: 2.3190e-07 - accuracy: 1.00 - ETA: 4:01 - loss: 2.3351e-07 - accuracy: 1.00 - ETA: 3:51 - loss: 2.3227e-07 - accuracy: 1.00 - ETA: 3:40 - loss: 2.3291e-07 - accuracy: 1.00 - ETA: 3:30 - loss: 2.3135e-07 - accuracy: 1.00 - ETA: 3:19 - loss: 2.3083e-07 - accuracy: 1.00 - ETA: 3:09 - loss: 2.3124e-07 - accuracy: 1.00 - ETA: 2:58 - loss: 2.3015e-07 - accuracy: 1.00 - ETA: 2:48 - loss: 2.2872e-07 - accuracy: 1.00 - ETA: 2:37 - loss: 2.3745e-07 - accuracy: 1.00 - ETA: 2:27 - loss: 2.3650e-07 - accuracy: 1.00 - ETA: 2:17 - loss: 2.3587e-07 - accuracy: 1.00 - ETA: 2:06 - loss: 2.3503e-07 - accuracy: 1.00 - ETA: 1:56 - loss: 2.3589e-07 - accuracy: 1.00 - ETA: 1:45 - loss: 2.3720e-07 - accuracy: 1.00 - ETA: 1:35 - loss: 2.3608e-07 - accuracy: 1.00 - ETA: 1:25 - loss: 2.3662e-07 - accuracy: 1.00 - ETA: 1:14 - loss: 2.3537e-07 - accuracy: 1.00 - ETA: 1:04 - loss: 2.3473e-07 - accuracy: 1.00 - ETA: 54s - loss: 2.3523e-07 - accuracy: 1.0000 - ETA: 43s - loss: 2.3575e-07 - accuracy: 1.000 - ETA: 33s - loss: 2.3541e-07 - accuracy: 1.000 - ETA: 23s - loss: 2.3454e-07 - accuracy: 1.000 - ETA: 12s - loss: 2.3474e-07 - accuracy: 1.000 - ETA: 2s - loss: 2.3424e-07 - accuracy: 1.000 - 1308s 686ms/step - loss: 2.3439e-07 - accuracy: 1.0000 - val_loss: 0.0191 - val_accuracy: 0.9877\n",
      "Epoch 29/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1908/1908 [==============================] - ETA: 19:04 - loss: 2.0981e-07 - accuracy: 1.000 - ETA: 19:30 - loss: 1.5072e-07 - accuracy: 1.000 - ETA: 19:25 - loss: 1.4679e-07 - accuracy: 1.000 - ETA: 19:23 - loss: 1.7651e-07 - accuracy: 1.000 - ETA: 19:11 - loss: 1.7216e-07 - accuracy: 1.000 - ETA: 19:02 - loss: 1.8602e-07 - accuracy: 1.000 - ETA: 18:51 - loss: 1.9303e-07 - accuracy: 1.000 - ETA: 18:42 - loss: 1.9763e-07 - accuracy: 1.000 - ETA: 18:30 - loss: 2.2123e-07 - accuracy: 1.000 - ETA: 18:22 - loss: 2.1420e-07 - accuracy: 1.000 - ETA: 18:12 - loss: 2.4166e-07 - accuracy: 1.000 - ETA: 18:00 - loss: 2.3971e-07 - accuracy: 1.000 - ETA: 17:49 - loss: 2.3841e-07 - accuracy: 1.000 - ETA: 17:39 - loss: 2.3002e-07 - accuracy: 1.000 - ETA: 17:26 - loss: 2.2700e-07 - accuracy: 1.000 - ETA: 17:15 - loss: 2.2266e-07 - accuracy: 1.000 - ETA: 17:04 - loss: 2.1772e-07 - accuracy: 1.000 - ETA: 16:54 - loss: 2.1586e-07 - accuracy: 1.000 - ETA: 16:43 - loss: 2.1070e-07 - accuracy: 1.000 - ETA: 16:33 - loss: 2.0591e-07 - accuracy: 1.000 - ETA: 16:22 - loss: 1.9890e-07 - accuracy: 1.000 - ETA: 16:12 - loss: 2.0163e-07 - accuracy: 1.000 - ETA: 16:02 - loss: 1.9776e-07 - accuracy: 1.000 - ETA: 15:51 - loss: 2.0211e-07 - accuracy: 1.000 - ETA: 15:42 - loss: 2.0059e-07 - accuracy: 1.000 - ETA: 15:32 - loss: 2.0147e-07 - accuracy: 1.000 - ETA: 15:21 - loss: 2.0059e-07 - accuracy: 1.000 - ETA: 15:12 - loss: 2.0050e-07 - accuracy: 1.000 - ETA: 15:02 - loss: 1.9822e-07 - accuracy: 1.000 - ETA: 14:51 - loss: 1.9663e-07 - accuracy: 1.000 - ETA: 14:41 - loss: 2.0397e-07 - accuracy: 1.000 - ETA: 14:31 - loss: 2.0084e-07 - accuracy: 1.000 - ETA: 14:20 - loss: 2.0277e-07 - accuracy: 1.000 - ETA: 14:10 - loss: 2.0233e-07 - accuracy: 1.000 - ETA: 14:00 - loss: 2.0235e-07 - accuracy: 1.000 - ETA: 13:50 - loss: 2.0064e-07 - accuracy: 1.000 - ETA: 13:40 - loss: 2.0015e-07 - accuracy: 1.000 - ETA: 13:29 - loss: 1.9899e-07 - accuracy: 1.000 - ETA: 13:19 - loss: 1.9735e-07 - accuracy: 1.000 - ETA: 13:09 - loss: 1.9928e-07 - accuracy: 1.000 - ETA: 13:00 - loss: 1.9629e-07 - accuracy: 1.000 - ETA: 12:50 - loss: 1.9401e-07 - accuracy: 1.000 - ETA: 12:40 - loss: 1.9168e-07 - accuracy: 1.000 - ETA: 12:31 - loss: 1.9510e-07 - accuracy: 1.000 - ETA: 12:20 - loss: 1.9479e-07 - accuracy: 1.000 - ETA: 12:11 - loss: 1.9511e-07 - accuracy: 1.000 - ETA: 12:01 - loss: 1.9429e-07 - accuracy: 1.000 - ETA: 11:51 - loss: 1.9435e-07 - accuracy: 1.000 - ETA: 11:41 - loss: 1.9773e-07 - accuracy: 1.000 - ETA: 11:31 - loss: 1.9760e-07 - accuracy: 1.000 - ETA: 11:21 - loss: 1.9994e-07 - accuracy: 1.000 - ETA: 11:11 - loss: 2.0284e-07 - accuracy: 1.000 - ETA: 11:01 - loss: 2.0233e-07 - accuracy: 1.000 - ETA: 10:51 - loss: 2.0894e-07 - accuracy: 1.000 - ETA: 10:41 - loss: 2.0898e-07 - accuracy: 1.000 - ETA: 10:31 - loss: 2.0799e-07 - accuracy: 1.000 - ETA: 10:21 - loss: 2.0982e-07 - accuracy: 1.000 - ETA: 10:11 - loss: 2.1275e-07 - accuracy: 1.000 - ETA: 10:01 - loss: 2.1161e-07 - accuracy: 1.000 - ETA: 9:51 - loss: 2.1080e-07 - accuracy: 1.000 - ETA: 9:41 - loss: 2.0918e-07 - accuracy: 1.00 - ETA: 9:31 - loss: 2.0892e-07 - accuracy: 1.00 - ETA: 9:21 - loss: 2.0773e-07 - accuracy: 1.00 - ETA: 9:11 - loss: 2.0712e-07 - accuracy: 1.00 - ETA: 9:01 - loss: 2.0978e-07 - accuracy: 1.00 - ETA: 8:51 - loss: 2.0960e-07 - accuracy: 1.00 - ETA: 8:41 - loss: 2.1076e-07 - accuracy: 1.00 - ETA: 8:32 - loss: 2.1026e-07 - accuracy: 1.00 - ETA: 8:22 - loss: 2.0897e-07 - accuracy: 1.00 - ETA: 8:12 - loss: 2.1385e-07 - accuracy: 1.00 - ETA: 8:01 - loss: 2.1211e-07 - accuracy: 1.00 - ETA: 7:51 - loss: 2.1037e-07 - accuracy: 1.00 - ETA: 7:41 - loss: 2.0975e-07 - accuracy: 1.00 - ETA: 7:31 - loss: 2.0987e-07 - accuracy: 1.00 - ETA: 7:21 - loss: 2.1060e-07 - accuracy: 1.00 - ETA: 7:11 - loss: 2.0871e-07 - accuracy: 1.00 - ETA: 7:01 - loss: 2.0954e-07 - accuracy: 1.00 - ETA: 6:51 - loss: 2.2037e-07 - accuracy: 1.00 - ETA: 6:41 - loss: 2.1941e-07 - accuracy: 1.00 - ETA: 6:32 - loss: 2.1853e-07 - accuracy: 1.00 - ETA: 6:22 - loss: 2.1705e-07 - accuracy: 1.00 - ETA: 6:12 - loss: 2.1845e-07 - accuracy: 1.00 - ETA: 6:02 - loss: 2.2054e-07 - accuracy: 1.00 - ETA: 5:52 - loss: 2.1936e-07 - accuracy: 1.00 - ETA: 5:42 - loss: 2.1837e-07 - accuracy: 1.00 - ETA: 5:32 - loss: 2.1856e-07 - accuracy: 1.00 - ETA: 5:22 - loss: 2.1884e-07 - accuracy: 1.00 - ETA: 5:12 - loss: 2.2293e-07 - accuracy: 1.00 - ETA: 5:02 - loss: 2.2361e-07 - accuracy: 1.00 - ETA: 4:52 - loss: 2.2279e-07 - accuracy: 1.00 - ETA: 4:42 - loss: 2.2170e-07 - accuracy: 1.00 - ETA: 4:32 - loss: 2.2029e-07 - accuracy: 1.00 - ETA: 4:22 - loss: 2.2019e-07 - accuracy: 1.00 - ETA: 4:12 - loss: 2.1926e-07 - accuracy: 1.00 - ETA: 4:02 - loss: 2.1814e-07 - accuracy: 1.00 - ETA: 3:52 - loss: 2.1728e-07 - accuracy: 1.00 - ETA: 3:41 - loss: 2.1845e-07 - accuracy: 1.00 - ETA: 3:32 - loss: 2.1698e-07 - accuracy: 1.00 - ETA: 3:21 - loss: 2.1657e-07 - accuracy: 1.00 - ETA: 3:11 - loss: 2.1640e-07 - accuracy: 1.00 - ETA: 3:02 - loss: 2.1633e-07 - accuracy: 1.00 - ETA: 2:52 - loss: 2.1544e-07 - accuracy: 1.00 - ETA: 2:42 - loss: 2.1493e-07 - accuracy: 1.00 - ETA: 2:32 - loss: 2.1469e-07 - accuracy: 1.00 - ETA: 2:22 - loss: 2.1524e-07 - accuracy: 1.00 - ETA: 2:12 - loss: 2.1425e-07 - accuracy: 1.00 - ETA: 2:02 - loss: 2.1427e-07 - accuracy: 1.00 - ETA: 1:52 - loss: 2.1458e-07 - accuracy: 1.00 - ETA: 1:42 - loss: 2.1400e-07 - accuracy: 1.00 - ETA: 1:32 - loss: 2.1296e-07 - accuracy: 1.00 - ETA: 1:22 - loss: 2.1387e-07 - accuracy: 1.00 - ETA: 1:12 - loss: 2.1323e-07 - accuracy: 1.00 - ETA: 1:02 - loss: 2.1332e-07 - accuracy: 1.00 - ETA: 52s - loss: 2.1634e-07 - accuracy: 1.0000 - ETA: 42s - loss: 2.1606e-07 - accuracy: 1.000 - ETA: 32s - loss: 2.1622e-07 - accuracy: 1.000 - ETA: 22s - loss: 2.1573e-07 - accuracy: 1.000 - ETA: 12s - loss: 2.1491e-07 - accuracy: 1.000 - ETA: 2s - loss: 2.1518e-07 - accuracy: 1.000 - 1268s 665ms/step - loss: 2.1490e-07 - accuracy: 1.0000 - val_loss: 0.0193 - val_accuracy: 0.9878\n",
      "Epoch 30/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1908/1908 [==============================] - ETA: 20:13 - loss: 1.8296e-07 - accuracy: 1.000 - ETA: 19:58 - loss: 1.4436e-07 - accuracy: 1.000 - ETA: 20:00 - loss: 1.1905e-07 - accuracy: 1.000 - ETA: 19:38 - loss: 1.5478e-07 - accuracy: 1.000 - ETA: 19:25 - loss: 1.6866e-07 - accuracy: 1.000 - ETA: 19:11 - loss: 1.7170e-07 - accuracy: 1.000 - ETA: 19:00 - loss: 1.5697e-07 - accuracy: 1.000 - ETA: 18:46 - loss: 1.4688e-07 - accuracy: 1.000 - ETA: 18:33 - loss: 1.5147e-07 - accuracy: 1.000 - ETA: 18:20 - loss: 1.5397e-07 - accuracy: 1.000 - ETA: 18:09 - loss: 1.5380e-07 - accuracy: 1.000 - ETA: 17:57 - loss: 1.4911e-07 - accuracy: 1.000 - ETA: 17:47 - loss: 1.6013e-07 - accuracy: 1.000 - ETA: 17:35 - loss: 1.6737e-07 - accuracy: 1.000 - ETA: 17:25 - loss: 1.6164e-07 - accuracy: 1.000 - ETA: 17:14 - loss: 1.5721e-07 - accuracy: 1.000 - ETA: 17:05 - loss: 1.5915e-07 - accuracy: 1.000 - ETA: 16:56 - loss: 1.5735e-07 - accuracy: 1.000 - ETA: 16:48 - loss: 1.5470e-07 - accuracy: 1.000 - ETA: 16:39 - loss: 1.5460e-07 - accuracy: 1.000 - ETA: 16:28 - loss: 1.5681e-07 - accuracy: 1.000 - ETA: 16:19 - loss: 1.6214e-07 - accuracy: 1.000 - ETA: 16:08 - loss: 1.6054e-07 - accuracy: 1.000 - ETA: 15:55 - loss: 1.5854e-07 - accuracy: 1.000 - ETA: 15:45 - loss: 1.5948e-07 - accuracy: 1.000 - ETA: 15:34 - loss: 1.5616e-07 - accuracy: 1.000 - ETA: 15:23 - loss: 1.5984e-07 - accuracy: 1.000 - ETA: 15:13 - loss: 1.5924e-07 - accuracy: 1.000 - ETA: 15:02 - loss: 1.5671e-07 - accuracy: 1.000 - ETA: 14:51 - loss: 1.6034e-07 - accuracy: 1.000 - ETA: 14:39 - loss: 1.6067e-07 - accuracy: 1.000 - ETA: 14:29 - loss: 1.5807e-07 - accuracy: 1.000 - ETA: 14:20 - loss: 1.5616e-07 - accuracy: 1.000 - ETA: 14:11 - loss: 1.5656e-07 - accuracy: 1.000 - ETA: 14:02 - loss: 1.5611e-07 - accuracy: 1.000 - ETA: 13:52 - loss: 1.5699e-07 - accuracy: 1.000 - ETA: 13:42 - loss: 1.5649e-07 - accuracy: 1.000 - ETA: 13:31 - loss: 1.5804e-07 - accuracy: 1.000 - ETA: 13:22 - loss: 1.5723e-07 - accuracy: 1.000 - ETA: 13:12 - loss: 1.5741e-07 - accuracy: 1.000 - ETA: 13:02 - loss: 1.5908e-07 - accuracy: 1.000 - ETA: 12:52 - loss: 1.5879e-07 - accuracy: 1.000 - ETA: 12:41 - loss: 1.5891e-07 - accuracy: 1.000 - ETA: 12:31 - loss: 1.5717e-07 - accuracy: 1.000 - ETA: 12:21 - loss: 1.5563e-07 - accuracy: 1.000 - ETA: 12:12 - loss: 1.5686e-07 - accuracy: 1.000 - ETA: 12:02 - loss: 1.5823e-07 - accuracy: 1.000 - ETA: 11:52 - loss: 1.5915e-07 - accuracy: 1.000 - ETA: 11:42 - loss: 1.6572e-07 - accuracy: 1.000 - ETA: 11:32 - loss: 1.7210e-07 - accuracy: 1.000 - ETA: 11:22 - loss: 1.7220e-07 - accuracy: 1.000 - ETA: 11:12 - loss: 1.7119e-07 - accuracy: 1.000 - ETA: 11:02 - loss: 1.7877e-07 - accuracy: 1.000 - ETA: 10:53 - loss: 1.7860e-07 - accuracy: 1.000 - ETA: 10:44 - loss: 1.7851e-07 - accuracy: 1.000 - ETA: 10:35 - loss: 1.7793e-07 - accuracy: 1.000 - ETA: 10:25 - loss: 1.7985e-07 - accuracy: 1.000 - ETA: 10:16 - loss: 1.9437e-07 - accuracy: 1.000 - ETA: 10:07 - loss: 1.9345e-07 - accuracy: 1.000 - ETA: 9:57 - loss: 1.9325e-07 - accuracy: 1.000 - ETA: 9:47 - loss: 1.9299e-07 - accuracy: 1.00 - ETA: 9:37 - loss: 1.9198e-07 - accuracy: 1.00 - ETA: 9:27 - loss: 1.8982e-07 - accuracy: 1.00 - ETA: 9:16 - loss: 1.8960e-07 - accuracy: 1.00 - ETA: 9:07 - loss: 1.8895e-07 - accuracy: 1.00 - ETA: 8:56 - loss: 1.8846e-07 - accuracy: 1.00 - ETA: 8:46 - loss: 1.8757e-07 - accuracy: 1.00 - ETA: 8:36 - loss: 1.8816e-07 - accuracy: 1.00 - ETA: 8:26 - loss: 1.8697e-07 - accuracy: 1.00 - ETA: 8:16 - loss: 1.8874e-07 - accuracy: 1.00 - ETA: 8:06 - loss: 1.8741e-07 - accuracy: 1.00 - ETA: 7:56 - loss: 1.8818e-07 - accuracy: 1.00 - ETA: 7:45 - loss: 1.8787e-07 - accuracy: 1.00 - ETA: 7:35 - loss: 1.8706e-07 - accuracy: 1.00 - ETA: 7:25 - loss: 1.8642e-07 - accuracy: 1.00 - ETA: 7:14 - loss: 1.8588e-07 - accuracy: 1.00 - ETA: 7:04 - loss: 1.8505e-07 - accuracy: 1.00 - ETA: 6:55 - loss: 1.8884e-07 - accuracy: 1.00 - ETA: 6:45 - loss: 1.8936e-07 - accuracy: 1.00 - ETA: 6:35 - loss: 1.8930e-07 - accuracy: 1.00 - ETA: 6:25 - loss: 1.8893e-07 - accuracy: 1.00 - ETA: 6:14 - loss: 1.8785e-07 - accuracy: 1.00 - ETA: 6:04 - loss: 1.8885e-07 - accuracy: 1.00 - ETA: 5:54 - loss: 1.8856e-07 - accuracy: 1.00 - ETA: 5:44 - loss: 1.8905e-07 - accuracy: 1.00 - ETA: 5:34 - loss: 1.8904e-07 - accuracy: 1.00 - ETA: 5:24 - loss: 1.8958e-07 - accuracy: 1.00 - ETA: 5:13 - loss: 1.8975e-07 - accuracy: 1.00 - ETA: 5:03 - loss: 1.8975e-07 - accuracy: 1.00 - ETA: 4:53 - loss: 1.9112e-07 - accuracy: 1.00 - ETA: 4:43 - loss: 1.8988e-07 - accuracy: 1.00 - ETA: 4:33 - loss: 1.8919e-07 - accuracy: 1.00 - ETA: 4:23 - loss: 1.8954e-07 - accuracy: 1.00 - ETA: 4:13 - loss: 1.9056e-07 - accuracy: 1.00 - ETA: 4:03 - loss: 1.9300e-07 - accuracy: 1.00 - ETA: 3:53 - loss: 1.9340e-07 - accuracy: 1.00 - ETA: 3:43 - loss: 1.9247e-07 - accuracy: 1.00 - ETA: 3:33 - loss: 1.9258e-07 - accuracy: 1.00 - ETA: 3:23 - loss: 1.9209e-07 - accuracy: 1.00 - ETA: 3:13 - loss: 1.9206e-07 - accuracy: 1.00 - ETA: 3:03 - loss: 1.9154e-07 - accuracy: 1.00 - ETA: 2:53 - loss: 1.9079e-07 - accuracy: 1.00 - ETA: 2:43 - loss: 1.9056e-07 - accuracy: 1.00 - ETA: 2:32 - loss: 1.9103e-07 - accuracy: 1.00 - ETA: 2:22 - loss: 1.9114e-07 - accuracy: 1.00 - ETA: 2:12 - loss: 1.9147e-07 - accuracy: 1.00 - ETA: 2:02 - loss: 1.9122e-07 - accuracy: 1.00 - ETA: 1:52 - loss: 1.9177e-07 - accuracy: 1.00 - ETA: 1:42 - loss: 1.9145e-07 - accuracy: 1.00 - ETA: 1:32 - loss: 1.9190e-07 - accuracy: 1.00 - ETA: 1:22 - loss: 1.9490e-07 - accuracy: 1.00 - ETA: 1:12 - loss: 1.9446e-07 - accuracy: 1.00 - ETA: 1:02 - loss: 1.9360e-07 - accuracy: 1.00 - ETA: 52s - loss: 1.9284e-07 - accuracy: 1.0000 - ETA: 42s - loss: 1.9327e-07 - accuracy: 1.000 - ETA: 32s - loss: 1.9263e-07 - accuracy: 1.000 - ETA: 22s - loss: 1.9247e-07 - accuracy: 1.000 - ETA: 12s - loss: 1.9830e-07 - accuracy: 1.000 - ETA: 2s - loss: 1.9754e-07 - accuracy: 1.000 - 1274s 668ms/step - loss: 1.9778e-07 - accuracy: 1.0000 - val_loss: 0.0194 - val_accuracy: 0.9878\n",
      "Training completed in 41408.33544 seconds\n"
     ]
    }
   ],
   "source": [
    "t_ini = datetime.datetime.now()\n",
    "\n",
    "cp = ModelCheckpoint(filepath=\"bert_tagger.h5\",\n",
    "                     monitor='val_acc',\n",
    "                     save_best_only=True,\n",
    "                     save_weights_only=True,\n",
    "                     verbose=1)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor = 'val_acc', patience = 5)\n",
    "\n",
    "history = model.fit([train_input_ids, train_input_masks, train_segment_ids], \n",
    "                    train_labels,\n",
    "                    validation_data=([test_input_ids, test_input_masks, test_segment_ids], test_labels),\n",
    "                    #validation_split=0.3,\n",
    "                    epochs=EPOCHS,\n",
    "                    batch_size=16,\n",
    "                    shuffle=True,\n",
    "                    verbose=1,\n",
    "                    callbacks=[cp, early_stopping]\n",
    "                   ) \n",
    "\n",
    "t_fin = datetime.datetime.now()\n",
    "print('Training completed in {} seconds'.format((t_fin - t_ini).total_seconds()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('bert_tagger2.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "nbpresent": {
     "id": "ce12f232-8d4a-4093-9977-88402c6a80f4"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable weights: 197\n",
      "Model: \"model_3\"\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_ids (InputLayer)           (None, 72)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_masks (InputLayer)         (None, 72)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)         (None, 72)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "bert_layer_3 (BertLayer)         (None, None, 768)     178565115   input_ids[0][0]                  \n",
      "                                                                   input_masks[0][0]                \n",
      "                                                                   segment_ids[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, None, 3)       2307        bert_layer_3[0][0]               \n",
      "====================================================================================================\n",
      "Total params: 178,567,422\n",
      "Trainable params: 177,265,155\n",
      "Non-trainable params: 1,302,267\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(MAX_SEQUENCE_LENGTH+2) \n",
    "model.load_weights('bert_tagger2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "nbpresent": {
     "id": "615aa234-1351-4d3e-84ff-379d1fc4f5cf"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0412 10:45:04.443657 126028 module_wrapper.py:139] From c:\\users\\pnedelev\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:438: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict([test_input_ids, test_input_masks, test_segment_ids]).argmax(-1)\n",
    "y_true = test_labels.argmax(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "nbpresent": {
     "id": "4d68934e-3464-4fa5-bdfe-33f1bee74efc"
    }
   },
   "outputs": [],
   "source": [
    "def y2label(zipped, mask=0):\n",
    "    out_true = []\n",
    "    out_pred = []\n",
    "    for zip_i in zipped:\n",
    "        a, b = tuple(zip_i)\n",
    "        if a != mask :\n",
    "            out_true.append(int2tag[a])\n",
    "            out_pred.append(int2tag[b])\n",
    "    return out_true, out_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "nbpresent": {
     "id": "6412a274-6f2e-4c27-9fb2-0fd0916ab42a"
    }
   },
   "outputs": [],
   "source": [
    "y_zipped = zip(y_true.flat, y_pred.flat)\n",
    "y_true, y_pred = y2label(y_zipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6198, 6198)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_true), len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "nbpresent": {
     "id": "0e51f9f4-b16f-4e14-a204-8167e2897848"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------ Result of Bert fine-tuned model ----------\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       -PAD-     0.0000    0.0000    0.0000         0\n",
      "           N     0.9921    0.9923    0.9922      5587\n",
      "           Y     0.9310    0.9280    0.9295       611\n",
      "\n",
      "    accuracy                         0.9860      6198\n",
      "   macro avg     0.6411    0.6401    0.6406      6198\n",
      "weighted avg     0.9861    0.9860    0.9860      6198\n",
      "\n"
     ]
    }
   ],
   "source": [
    "name='Bert fine-tuned model'\n",
    "print('\\n------------ Result of {} ----------\\n'.format(name))\n",
    "print(classification_report(y_true, y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "nbpresent": {
     "id": "0e51f9f4-b16f-4e14-a204-8167e2897848"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9860\n",
      "f1-macro score: 0.6406\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: {0:.4f}\".format(accuracy_score(y_true, y_pred)))\n",
    "print('f1-macro score: {0:.4f}'.format(f1_score(y_true, y_pred, average='macro')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tags = sorted(set(y_pred+y_true))\n",
    "cnf_matrix = confusion_matrix(y_true, y_pred)\n",
    "plot_confusion_matrix(f1_score(y_true, y_pred, average='macro'), cnf_matrix, target_names=tags, title=name, normalize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a prediction for a test sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict([test_input_ids, test_input_masks, test_segment_ids], batch_size=16).argmax(-1)\n",
    "y_true = test_labels_ids\n",
    "\n",
    "def make_prediction(i=16):\n",
    "    note = ''\n",
    "    sent = []\n",
    "    print(\"{:10} {:5} : {:5}\".format(\"Word\", \"True\", \"Predicted\"))\n",
    "    print(35*'-')\n",
    "    for w, true, pred in zip(test_input_ids[i], y_true[i], y_pred[i]):\n",
    "        if tokenizer.convert_ids_to_tokens([w])[0]!='[PAD]' and \\\n",
    "            tokenizer.convert_ids_to_tokens([w])[0]!='[CLS]' and \\\n",
    "            tokenizer.convert_ids_to_tokens([w])[0]!='[SEP]':\n",
    "            if int2tag[true] != int2tag[pred]: note='<<--- Error!'\n",
    "            print(\"{:10} {:5} : {:5} {:5}\".format(tokenizer.convert_ids_to_tokens([w])[0], int2tag[true], int2tag[pred], note))\n",
    "            note=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word       True  : Predicted\n",
      "-----------------------------------\n",
      "##ки       N     : N          \n",
      "##рски     N     : N          \n",
      "##ци       N     : N          \n",
      "се         N     : N          \n",
      "##ват      Y     : Y          \n",
      "от         N     : N          \n",
      "##то       N     : N          \n",
      "място      N     : N          \n",
      "и          N     : N          \n",
      "##ят       Y     : N     <<--- Error!\n",
      "на         N     : N          \n",
      "##яг       N     : N          \n",
      ".          N     : N          \n"
     ]
    }
   ],
   "source": [
    "make_prediction(i=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tag an unknown sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_raw = 'Статиите в Уикипедия често са цитирани както от медиите, така и в академичните среди: големите ѝ предимства са нейното свободно разпространение и списване, както и широкият ѝ обхват на теми. Редакторите са насърчавани да се придържат към политиката на „Неутрална гледна точка“, съгласно която основните възгледи по даден проблем се обобщават без опит да се изведе обективна истина. Така или иначе, славата на Уикипедия като достоверен справочник е оспорвана. Отворената ѝ природа позволява вандализъм, неточности, непридържане към приетите норми, ниско качество и пристрастия. Уикипедия – в сравнение с традиционните енциклопедии – е окачествявана като смесица от „истина, полуистина и някои лъжи“[3] и е критикувана за предпочитането на консенсуса пред личните авторитети и за предполагаемата липса на лична отговорност за съдържанието. Упреквана е също, че предоставя средство за безогледна манипулация и пропаганда, особено по спорни теми.[4]. От друга страна, към 2020 г. Уикипедия се оценявана като последното останало добро място в интернет[5], носител на културни иновации, който е несравнимо по-лесно достъпен и четен от традиционните хартиени енциклопедии. Същият автор високо оценява окуражаването на всякакви дискусии и възприемането на шеговит стил на общуване, напомнящ за ранните дни на Просвещението и първата Енциклопедия.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc9d04f221174975a526bbba0c3765a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Converting examples to features', max=1, style=ProgressStyle(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Word in BERT layer  | Initial word   : Predicted POS-tag\n",
      "-------------------------------------------------------------\n",
      "##ите               | Статиите       : N              \n",
      "в                   | в              : N              \n",
      "##педия             | Уикипедия      : N              \n",
      "често               | често          : N              \n",
      "са                  | са             : Y              \n",
      "##рани              | цитирани       : N              \n",
      "както               | както          : N              \n",
      "от                  | от             : N              \n",
      "##ите               | медиите        : N              \n",
      ",                   | ,              : N              \n",
      "така                | така           : N              \n",
      "и                   | и              : N              \n",
      "в                   | в              : N              \n",
      "##чните             | академичните   : N              \n",
      "среди               | среди          : N              \n",
      ":                   | :              : N              \n",
      "големите            | големите       : N              \n",
      "ѝ                   | ѝ              : N              \n",
      "##тва               | предимства     : N              \n",
      "са                  | са             : Y              \n",
      "##ното              | нейното        : N              \n",
      "##ободно            | свободно       : N              \n",
      "##ие                | разпространение: N              \n",
      "и                   | и              : N              \n",
      "##не                | списване       : N              \n",
      ",                   | ,              : N              \n",
      "както               | както          : N              \n",
      "и                   | и              : N              \n",
      "##ят                | широкият       : N              \n",
      "ѝ                   | ѝ              : N              \n",
      "##ват               | обхват         : Y              \n",
      "на                  | на             : N              \n",
      "теми                | теми           : N              \n",
      ".                   | .              : N              \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b34371f953846cbb8ddc8d281d06c7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Converting examples to features', max=1, style=ProgressStyle(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Word in BERT layer  | Initial word   : Predicted POS-tag\n",
      "-------------------------------------------------------------\n",
      "##ите               | Редакторите    : N              \n",
      "са                  | са             : Y              \n",
      "##ни                | насърчавани    : N              \n",
      "да                  | да             : N              \n",
      "се                  | се             : N              \n",
      "##т                 | придържат      : Y              \n",
      "към                 | към            : N              \n",
      "##та                | политиката     : N              \n",
      "на                  | на             : N              \n",
      "„                   | „              : N              \n",
      "##на                | Неутрална      : N              \n",
      "##една              | гледна         : N              \n",
      "точка               | точка          : N              \n",
      "[UNK]               | “              : N              \n",
      ",                   | ,              : N              \n",
      "##сно               | съгласно       : N              \n",
      "която               | която          : N              \n",
      "##те                | основните      : N              \n",
      "##и                 | възгледи       : N              \n",
      "по                  | по             : N              \n",
      "##ден               | даден          : N              \n",
      "проблем             | проблем        : N              \n",
      "се                  | се             : N              \n",
      "##т                 | обобщават      : Y              \n",
      "без                 | без            : N              \n",
      "##пит               | опит           : N              \n",
      "да                  | да             : N              \n",
      "се                  | се             : N              \n",
      "##веде              | изведе         : Y              \n",
      "##вна               | обективна      : N              \n",
      "##на                | истина         : N              \n",
      ".                   | .              : N              \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "437bafe8206640148a3a5d148b805e64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Converting examples to features', max=1, style=ProgressStyle(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Word in BERT layer  | Initial word   : Predicted POS-tag\n",
      "-------------------------------------------------------------\n",
      "Така                | Така           : N              \n",
      "или                 | или            : N              \n",
      "иначе               | иначе          : N              \n",
      ",                   | ,              : N              \n",
      "##та                | славата        : N              \n",
      "на                  | на             : N              \n",
      "##педия             | Уикипедия      : N              \n",
      "като                | като           : N              \n",
      "##ерен              | достоверен     : N              \n",
      "справочник          | справочник     : N              \n",
      "е                   | е              : N              \n",
      "##вана              | оспорвана      : N              \n",
      ".                   | .              : N              \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e04ca6c9a1e84e138e881c0e9266bfc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Converting examples to features', max=1, style=ProgressStyle(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Word in BERT layer  | Initial word   : Predicted POS-tag\n",
      "-------------------------------------------------------------\n",
      "##ената             | Отворената     : N              \n",
      "ѝ                   | ѝ              : N              \n",
      "##рода              | природа        : N              \n",
      "##ва                | позволява      : Y              \n",
      "##ъм                | вандализъм     : N              \n",
      ",                   | ,              : N              \n",
      "##сти               | неточности     : N              \n",
      ",                   | ,              : N              \n",
      "##не                | непридържане   : N              \n",
      "към                 | към            : N              \n",
      "##те                | приетите       : N              \n",
      "##и                 | норми          : N              \n",
      ",                   | ,              : N              \n",
      "##ско               | ниско          : N              \n",
      "##чество            | качество       : N              \n",
      "и                   | и              : N              \n",
      "##я                 | пристрастия    : N              \n",
      ".                   | .              : N              \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c060a673da32488fb1e6d22f1016496a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Converting examples to features', max=1, style=ProgressStyle(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Word in BERT layer  | Initial word   : Predicted POS-tag\n",
      "-------------------------------------------------------------\n",
      "##педия             | Уикипедия      : N              \n",
      "[UNK]               | –              : N              \n",
      "в                   | в              : N              \n",
      "##нение             | сравнение      : N              \n",
      "с                   | с              : N              \n",
      "##те                | традиционните  : N              \n",
      "##дии               | енциклопедии   : N              \n",
      "[UNK]               | –              : N              \n",
      "е                   | е              : N              \n",
      "##вана              | окачествявана  : N              \n",
      "като                | като           : N              \n",
      "##ца                | смесица        : N              \n",
      "от                  | от             : N              \n",
      "„                   | „              : N              \n",
      "##на                | истина         : N              \n",
      ",                   | ,              : N              \n",
      "##на                | полуистина     : N              \n",
      "и                   | и              : N              \n",
      "някои               | някои          : N              \n",
      "##жи                | лъжи           : N              \n",
      "[UNK]               | “              : N              \n",
      "[                   | [              : N              \n",
      "3                   | 3              : N              \n",
      "]                   | ]              : N              \n",
      "и                   | и              : N              \n",
      "е                   | е              : N              \n",
      "##на                | критикувана    : N              \n",
      "за                  | за             : N              \n",
      "##нето              | предпочитането : N              \n",
      "на                  | на             : N              \n",
      "##уса               | консенсуса     : N              \n",
      "пред                | пред           : N              \n",
      "##чните             | личните        : N              \n",
      "##и                 | авторитети     : N              \n",
      "и                   | и              : N              \n",
      "за                  | за             : N              \n",
      "##та                | предполагаемата: N              \n",
      "##са                | липса          : N              \n",
      "на                  | на             : N              \n",
      "##чна               | лична          : N              \n",
      "##ност              | отговорност    : N              \n",
      "за                  | за             : N              \n",
      "##нието             | съдържанието   : N              \n",
      ".                   | .              : N              \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbb02306e6ad4707b44cf3091785e4f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Converting examples to features', max=1, style=ProgressStyle(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Word in BERT layer  | Initial word   : Predicted POS-tag\n",
      "-------------------------------------------------------------\n",
      "##вана              | Упреквана      : N              \n",
      "е                   | е              : N              \n",
      "също                | също           : N              \n",
      ",                   | ,              : N              \n",
      "че                  | че             : N              \n",
      "##я                 | предоставя     : Y              \n",
      "средство            | средство       : N              \n",
      "за                  | за             : N              \n",
      "##дна               | безогледна     : N              \n",
      "##ия                | манипулация    : N              \n",
      "и                   | и              : N              \n",
      "##ганда             | пропаганда     : N              \n",
      ",                   | ,              : N              \n",
      "особено             | особено        : N              \n",
      "по                  | по             : N              \n",
      "##ни                | спорни         : N              \n",
      "теми                | теми           : N              \n",
      ".                   | .              : N              \n",
      "[                   | [              : N              \n",
      "4                   | 4              : N              \n",
      "]                   | ]              : N              \n",
      ".                   | .              : N              \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67bf57400de14edc9dc4845037bb962a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Converting examples to features', max=1, style=ProgressStyle(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Word in BERT layer  | Initial word   : Predicted POS-tag\n",
      "-------------------------------------------------------------\n",
      "От                  | От             : N              \n",
      "друга               | друга          : N              \n",
      "страна              | страна         : N              \n",
      ",                   | ,              : N              \n",
      "към                 | към            : N              \n",
      "2020                | 2020           : N              \n",
      ".                   | г.             : N              \n",
      "##педия             | Уикипедия      : N              \n",
      "се                  | се             : N              \n",
      "##на                | оценявана      : Y              \n",
      "като                | като           : N              \n",
      "##то                | последното     : N              \n",
      "##ло                | останало       : N              \n",
      "добро               | добро          : N              \n",
      "място               | място          : N              \n",
      "в                   | в              : N              \n",
      "интернет            | интернет       : N              \n",
      "[                   | [              : N              \n",
      "5                   | 5              : N              \n",
      "]                   | ]              : N              \n",
      ",                   | ,              : N              \n",
      "##ел                | носител        : N              \n",
      "на                  | на             : N              \n",
      "##турни             | културни       : N              \n",
      "##ции               | иновации       : N              \n",
      ",                   | ,              : N              \n",
      "който               | който          : N              \n",
      "е                   | е              : N              \n",
      "##о                 | несравнимо     : N              \n",
      "##сно               | по-лесно       : N              \n",
      "##пен               | достъпен       : N              \n",
      "и                   | и              : N              \n",
      "##тен               | четен          : N              \n",
      "от                  | от             : N              \n",
      "##те                | традиционните  : N              \n",
      "##ни                | хартиени       : N              \n",
      "##дии               | енциклопедии   : N              \n",
      ".                   | .              : N              \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8e1fe047d5f4e828271ff340c5f5874",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Converting examples to features', max=1, style=ProgressStyle(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Word in BERT layer  | Initial word   : Predicted POS-tag\n",
      "-------------------------------------------------------------\n",
      "##т                 | Същият         : N              \n",
      "автор               | автор          : N              \n",
      "високо              | високо         : N              \n",
      "##ява               | оценява        : Y              \n",
      "##нето              | окуражаването  : N              \n",
      "на                  | на             : N              \n",
      "##кви               | всякакви       : N              \n",
      "##ии                | дискусии       : N              \n",
      "и                   | и              : N              \n",
      "##то                | възприемането  : N              \n",
      "на                  | на             : N              \n",
      "##т                 | шеговит        : N              \n",
      "стил                | стил           : N              \n",
      "на                  | на             : N              \n",
      "##ване              | общуване       : N              \n",
      ",                   | ,              : N              \n",
      "##щ                 | напомнящ       : N              \n",
      "за                  | за             : N              \n",
      "##ните              | ранните        : N              \n",
      "дни                 | дни            : N              \n",
      "на                  | на             : N              \n",
      "##то                | Просвещението  : N              \n",
      "и                   | и              : N              \n",
      "първата             | първата        : N              \n",
      "##педия             | Енциклопедия   : N              \n",
      ".                   | .              : N              \n"
     ]
    }
   ],
   "source": [
    "# sentences = [word_tokenize(s) for s in sent_tokenize(raw_file)]\n",
    "for sentence_raw in sent_tokenize(sentences_raw):\n",
    "    \n",
    "\n",
    "    sentence_ini = nltk.word_tokenize(sentence_raw)\n",
    "\n",
    "    sentence_bert = tokenizer.tokenize(sentence_raw)\n",
    "\n",
    "    tokens_a = sentence_ini\n",
    "\n",
    "    orig_to_tok_map = []              \n",
    "    tokens = []\n",
    "    segment_ids = []\n",
    "    tokens.append(\"[CLS]\")\n",
    "    segment_ids.append(0)\n",
    "    orig_to_tok_map.append(len(tokens)-1)\n",
    "    for token in tokens_a:\n",
    "        #orig_to_tok_map.append(len(tokens)) # keep first piece of tokenized term\n",
    "        tokens.extend(tokenizer.tokenize(token))\n",
    "        orig_to_tok_map.append(len(tokens)-1) # # keep last piece of tokenized term -->> gives better results!\n",
    "        segment_ids.append(0)\n",
    "    tokens.append(\"[SEP]\")\n",
    "    segment_ids.append(0)\n",
    "    orig_to_tok_map.append(len(tokens)-1)\n",
    "    input_ids = tokenizer.convert_tokens_to_ids([tokens[i] for i in orig_to_tok_map])\n",
    "\n",
    "    # print('Original tokens:',tokens_a)\n",
    "\n",
    "    # print('BERT tokens:',tokens)\n",
    "\n",
    "    # orig_to_tok_map\n",
    "\n",
    "    [tokens[i] for i in orig_to_tok_map]\n",
    "\n",
    "    # Convert data to InputExample format\n",
    "    test_example = convert_text_to_examples([sentence_ini], [['-PAD-']*len(sentence_ini)])\n",
    "\n",
    "    # Convert to features\n",
    "    (input_ids, input_masks, segment_ids, _\n",
    "    ) = convert_examples_to_features(tokenizer, test_example, max_seq_length=MAX_SEQUENCE_LENGTH+2)\n",
    "\n",
    "    predictions = model.predict([input_ids, input_masks, segment_ids], batch_size=1).argmax(-1)[0]\n",
    "    print(\"\\n{:20}| {:15}: {:15}\".format(\"Word in BERT layer\", 'Initial word', \"Predicted POS-tag\"))\n",
    "    print(61*'-')\n",
    "    k = 0\n",
    "    for i, pred in enumerate(predictions):\n",
    "        try:\n",
    "            if pred!=0:\n",
    "                print(\"{:20}| {:15}: {:15}\".format([tokens[i] for i in orig_to_tok_map][i], sentence_ini[i-1], int2tag[pred]))            \n",
    "                k+=1            \n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequent types of mistakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_errors(X,y):\n",
    "    error_counter = collections.Counter()\n",
    "    support = 0\n",
    "    for i in range(test_input_ids.shape[0]):\n",
    "        for w, true, pred in zip(test_input_ids[i], y_true[i], y_pred[i]):\n",
    "            if int2tag[true]!='-PAD-':\n",
    "                if true != pred:\n",
    "                    word = tokenizer.convert_ids_to_tokens([w])[0]\n",
    "                    error_counter[word] += 1\n",
    "                support += 1\n",
    "    return error_counter, support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors, support = find_errors([test_input_ids],y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total errors/Total words: 93/3408 | Accuracy: 0.9727\n",
      "\n",
      "Most common errors: [('as', 8), ('appointed', 5), ('to', 4), ('part', 3), ('justice', 3), ('general', 3), ('than', 2), ('contracting', 2), ('base', 2), ('around', 2), ('##s', 1), ('##r', 1), ('is', 1), ('that', 1), ('time', 1), ('set', 1), ('interested', 1), ('traveling', 1), ('japanese', 1), ('by', 1)]\n"
     ]
    }
   ],
   "source": [
    "print('Total errors/Total words: {}/{} | Accuracy: {:.4}\\n'.format(sum(errors.values()), support, 1-sum(errors.values())/support))\n",
    "print('Most common errors:', errors.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_acc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
